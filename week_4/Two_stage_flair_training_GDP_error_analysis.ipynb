{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Two_stage_flair_training_GDP_error_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybjXa4UJ3g7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVY0ZiGk38i_",
        "colab_type": "code",
        "outputId": "1ede56ea-64a6-4418-df0b-8bd2831f203e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/flairNLP/flair.git\n",
            "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-nv97x6km\n",
            "  Running command git clone -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-nv97x6km\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (5.4.2)\n",
            "Requirement already satisfied, skipping upgrade: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (1.2.10)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (0.8.7)\n",
            "Requirement already satisfied, skipping upgrade: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (1.5.10)\n",
            "Requirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: transformers>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (1.5.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: langdetect in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.5) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.5) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.5) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.5) (0.1.9)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.5) (1.8.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.5) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.5) (0.13.1)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.5) (8.3.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.5) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.5) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.5) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.5) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.5) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.5) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.5) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.5) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.5) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.5) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.5) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.10.0->flair==0.5) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.10.0->flair==0.5) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.10.0->flair==0.5) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.10.0->flair==0.5) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.10.0->flair==0.5) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.10.0->flair==0.5) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair==0.5) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.5) (1.13.13)\n",
            "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.5) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.5) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.10.0->flair==0.5) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.10.0->flair==0.5) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.10.0->flair==0.5) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.10.0->flair==0.5) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.10.0->flair==0.5) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.5) (1.16.13)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.5) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.5) (0.15.2)\n",
            "Building wheels for collected packages: flair\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.5-cp36-none-any.whl size=148939 sha256=15768787db02dffc4326eb9dc6d5856574d59a9f5d5f38dc96fdf1ce60db0c87\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dcn_soem/wheels/84/82/73/d2b3b59b7be74ea05f2c6d64132efe27df52daffb47d1dc7bb\n",
            "Successfully built flair\n",
            "Installing collected packages: flair\n",
            "  Found existing installation: flair 0.5\n",
            "    Uninstalling flair-0.5:\n",
            "      Successfully uninstalled flair-0.5\n",
            "Successfully installed flair-0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flair"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP7gl75D39dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings, DocumentRNNEmbeddings, BertEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from pathlib import Path\n",
        "from flair.datasets import CSVClassificationCorpus\n",
        "from flair.data import Corpus\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfpsjw8I4Si-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VBdZ1L34XwO",
        "colab_type": "code",
        "outputId": "6ed9f32e-b745-4533-95be-3cda1329ecd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U90p69Xd5KIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_folder = \"./drive/My Drive/capstone/data/\"\n",
        "data_folder = \"/content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkUisuXz7tuK",
        "colab_type": "text"
      },
      "source": [
        "### First Stage (Train on benchmark dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-_Bmpa6gn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark = pd.read_csv(data_folder + \"combined_benchmark.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVaSsyAn72AU",
        "colab_type": "code",
        "outputId": "30f9bfa8-feed-463b-cee5-fd159bd64f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "benchmark = benchmark[['label', 'text']]\n",
        "benchmark.head()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Why not subscribe to the magazine ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>Tornio Works employs 2,300 of whom more than 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>The move is aimed at boosting sales , cost-eff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>As a result of the merger , the largest profes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>18 March 2010 A leakage in the gypsum pond was...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0                Why not subscribe to the magazine ?\n",
              "1     -1  Tornio Works employs 2,300 of whom more than 1...\n",
              "2      1  The move is aimed at boosting sales , cost-eff...\n",
              "3      0  As a result of the merger , the largest profes...\n",
              "4     -1  18 March 2010 A leakage in the gypsum pond was..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t_MVMXq8pvf",
        "colab_type": "text"
      },
      "source": [
        "#### Create train, dev and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUbbfcSC8ekr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark = benchmark.sample(frac=1)\n",
        "benchmark.iloc[0:int(len(benchmark)*0.8)].to_csv(data_folder + 'train.csv', sep='\\t', index = False, header = False)\n",
        "benchmark.iloc[int(len(benchmark)*0.8):int(len(benchmark)*0.9)].to_csv(data_folder + 'test.csv', sep='\\t', index = False, header = False)\n",
        "benchmark.iloc[int(len(benchmark)*0.9):].to_csv(data_folder + 'dev.csv', sep='\\t', index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjNxI9oDHFlN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "435d88cd-b1d3-410c-a53d-3ab92627aff5"
      },
      "source": [
        "benchmark.head()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>851</th>\n",
              "      <td>0</td>\n",
              "      <td>Construction is expected to be completed in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1045</th>\n",
              "      <td>-1</td>\n",
              "      <td>U.S. fines American Airlines, Delta for long t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>1</td>\n",
              "      <td>Sales for both the Department Store Division a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>0</td>\n",
              "      <td>Kauko-Telko 's centralized administration will...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>0</td>\n",
              "      <td>As part of the agreement , Aspocomp will also ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text\n",
              "851       0  Construction is expected to be completed in th...\n",
              "1045     -1  U.S. fines American Airlines, Delta for long t...\n",
              "153       1  Sales for both the Department Store Division a...\n",
              "532       0  Kauko-Telko 's centralized administration will...\n",
              "292       0  As part of the agreement , Aspocomp will also ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U8LTIJ-H2fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# benchmark.iloc[0:int(len(benchmark)*0.8)].to_csv(data_folder + 'train.csv', index = False, header = False)\n",
        "# benchmark.iloc[int(len(benchmark)*0.8):int(len(benchmark)*0.9)].to_csv(data_folder + 'test.csv', index = False, header = False)\n",
        "# benchmark.iloc[int(len(benchmark)*0.9):].to_csv(data_folder + 'dev.csv', index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a91JSI9nI1Z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training/dev.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7QE3IS69txr",
        "colab_type": "text"
      },
      "source": [
        "#### Build corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz-2gNy58wDI",
        "colab_type": "code",
        "outputId": "2c505340-809b-458e-a194-a7fb13e345f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#corpus = NLPTaskDataFetcher.load_classification_corpus(Path(data_folder), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
        "column_name_map = {1: \"text\", 0: \"label_topic\"}\n",
        "\n",
        "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
        "                                         column_name_map,\n",
        "                                         skip_header=False, #no header in kaggle data\n",
        "                                         delimiter='\\t',    # comma separated rows\n",
        "                                         train_file='train.csv', ## passing in file names manuelly when it can't auto detect\n",
        "                                         dev_file='dev.csv',\n",
        "                                         test_file='test.csv'\n",
        ")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 01:50:15,892 Reading data from /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training\n",
            "2020-05-26 01:50:15,894 Train: /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training/train.csv\n",
            "2020-05-26 01:50:15,895 Dev: /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training/dev.csv\n",
            "2020-05-26 01:50:15,897 Test: /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCgaVpvzHSB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb1lynSQ98-z",
        "colab_type": "text"
      },
      "source": [
        "#### Create word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u9PFUze9_5y",
        "colab_type": "code",
        "outputId": "278dcdbc-341b-4c5e-e139-f5d9b10ba7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#word_embeddings = [BertEmbeddings(), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n",
        "\n",
        "word_embeddings = [BertEmbeddings()]\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HovCqFfSMjmJ",
        "colab_type": "text"
      },
      "source": [
        "#### First Stage Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wik8DDxm-DKy",
        "colab_type": "code",
        "outputId": "10a1da4b-9efc-4471-df1f-61ac42f1e8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "trainer.train(data_folder, max_epochs=10)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 01:50:33,806 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1314/1314 [00:01<00:00, 878.04it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 01:50:35,610 [b'0', b'-1', b'1']\n",
            "2020-05-26 01:50:35,632 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:50:35,637 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentLSTMEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): BertEmbeddings(\n",
            "        (model): BertModel(\n",
            "          (embeddings): BertEmbeddings(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoder(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=3072, out_features=256, bias=True)\n",
            "    (rnn): GRU(256, 512)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-05-26 01:50:35,643 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:50:35,647 Corpus: \"Corpus: 1168 train + 147 dev + 146 test sentences\"\n",
            "2020-05-26 01:50:35,649 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:50:35,651 Parameters:\n",
            "2020-05-26 01:50:35,654  - learning_rate: \"0.1\"\n",
            "2020-05-26 01:50:35,656  - mini_batch_size: \"32\"\n",
            "2020-05-26 01:50:35,658  - patience: \"3\"\n",
            "2020-05-26 01:50:35,660  - anneal_factor: \"0.5\"\n",
            "2020-05-26 01:50:35,663  - max_epochs: \"10\"\n",
            "2020-05-26 01:50:35,665  - shuffle: \"True\"\n",
            "2020-05-26 01:50:35,668  - train_with_dev: \"False\"\n",
            "2020-05-26 01:50:35,671  - batch_growth_annealing: \"False\"\n",
            "2020-05-26 01:50:35,674 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:50:35,677 Model training base path: \"/content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training\"\n",
            "2020-05-26 01:50:35,680 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:50:35,682 Device: cuda:0\n",
            "2020-05-26 01:50:35,685 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:50:35,688 Embeddings storage mode: cpu\n",
            "2020-05-26 01:50:35,703 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 01:50:37,470 epoch 1 - iter 3/37 - loss 1.28199701 - samples/sec: 81.94\n",
            "2020-05-26 01:50:50,166 epoch 1 - iter 6/37 - loss 1.41365312 - samples/sec: 85.52\n",
            "2020-05-26 01:51:02,677 epoch 1 - iter 9/37 - loss 1.39422252 - samples/sec: 96.34\n",
            "2020-05-26 01:51:16,442 epoch 1 - iter 12/37 - loss 1.29789497 - samples/sec: 89.96\n",
            "2020-05-26 01:51:28,251 epoch 1 - iter 15/37 - loss 1.29454457 - samples/sec: 90.61\n",
            "2020-05-26 01:51:42,470 epoch 1 - iter 18/37 - loss 1.28033601 - samples/sec: 85.29\n",
            "2020-05-26 01:51:55,443 epoch 1 - iter 21/37 - loss 1.23133240 - samples/sec: 90.79\n",
            "2020-05-26 01:52:08,022 epoch 1 - iter 24/37 - loss 1.23432668 - samples/sec: 78.13\n",
            "2020-05-26 01:52:20,212 epoch 1 - iter 27/37 - loss 1.19750861 - samples/sec: 87.12\n",
            "2020-05-26 01:52:32,796 epoch 1 - iter 30/37 - loss 1.17403806 - samples/sec: 93.63\n",
            "2020-05-26 01:52:44,973 epoch 1 - iter 33/37 - loss 1.16789715 - samples/sec: 98.47\n",
            "2020-05-26 01:52:57,717 epoch 1 - iter 36/37 - loss 1.19602260 - samples/sec: 92.02\n",
            "2020-05-26 01:53:08,895 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:53:08,900 EPOCH 1 done: loss 1.1884 - lr 0.1000000\n",
            "2020-05-26 01:53:10,833 DEV : loss 0.9351709485054016 - score 0.7052\n",
            "2020-05-26 01:53:10,954 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 01:53:12,857 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:53:14,426 epoch 2 - iter 3/37 - loss 0.82855821 - samples/sec: 78.27\n",
            "2020-05-26 01:53:27,563 epoch 2 - iter 6/37 - loss 0.83640920 - samples/sec: 87.32\n",
            "2020-05-26 01:53:40,325 epoch 2 - iter 9/37 - loss 0.87916591 - samples/sec: 91.06\n",
            "2020-05-26 01:53:52,830 epoch 2 - iter 12/37 - loss 0.88344017 - samples/sec: 85.79\n",
            "2020-05-26 01:54:05,112 epoch 2 - iter 15/37 - loss 0.94352367 - samples/sec: 94.92\n",
            "2020-05-26 01:54:17,094 epoch 2 - iter 18/37 - loss 0.90623282 - samples/sec: 88.90\n",
            "2020-05-26 01:54:28,994 epoch 2 - iter 21/37 - loss 0.93742461 - samples/sec: 85.87\n",
            "2020-05-26 01:54:41,049 epoch 2 - iter 24/37 - loss 0.92627698 - samples/sec: 87.23\n",
            "2020-05-26 01:54:53,110 epoch 2 - iter 27/37 - loss 0.93216389 - samples/sec: 97.94\n",
            "2020-05-26 01:55:05,005 epoch 2 - iter 30/37 - loss 0.91899706 - samples/sec: 91.98\n",
            "2020-05-26 01:55:17,760 epoch 2 - iter 33/37 - loss 0.90614792 - samples/sec: 100.08\n",
            "2020-05-26 01:55:30,069 epoch 2 - iter 36/37 - loss 0.91211008 - samples/sec: 93.36\n",
            "2020-05-26 01:55:41,544 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:55:41,546 EPOCH 2 done: loss 0.9111 - lr 0.1000000\n",
            "2020-05-26 01:55:43,409 DEV : loss 0.7792381644248962 - score 0.737\n",
            "2020-05-26 01:55:43,531 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 01:55:45,297 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:55:46,935 epoch 3 - iter 3/37 - loss 0.68903756 - samples/sec: 75.25\n",
            "2020-05-26 01:55:59,687 epoch 3 - iter 6/37 - loss 0.72896804 - samples/sec: 93.70\n",
            "2020-05-26 01:56:11,767 epoch 3 - iter 9/37 - loss 0.74534210 - samples/sec: 93.59\n",
            "2020-05-26 01:56:23,297 epoch 3 - iter 12/37 - loss 0.80254420 - samples/sec: 90.18\n",
            "2020-05-26 01:56:35,416 epoch 3 - iter 15/37 - loss 0.81358139 - samples/sec: 94.06\n",
            "2020-05-26 01:56:48,380 epoch 3 - iter 18/37 - loss 0.80112129 - samples/sec: 83.07\n",
            "2020-05-26 01:57:00,377 epoch 3 - iter 21/37 - loss 0.79018997 - samples/sec: 94.89\n",
            "2020-05-26 01:57:13,546 epoch 3 - iter 24/37 - loss 0.77064053 - samples/sec: 89.83\n",
            "2020-05-26 01:57:25,280 epoch 3 - iter 27/37 - loss 0.77717844 - samples/sec: 91.98\n",
            "2020-05-26 01:57:37,605 epoch 3 - iter 30/37 - loss 0.77369340 - samples/sec: 88.56\n",
            "2020-05-26 01:57:49,284 epoch 3 - iter 33/37 - loss 0.77520941 - samples/sec: 98.86\n",
            "2020-05-26 01:58:01,559 epoch 3 - iter 36/37 - loss 0.79292606 - samples/sec: 97.61\n",
            "2020-05-26 01:58:12,473 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:58:12,478 EPOCH 3 done: loss 0.8131 - lr 0.1000000\n",
            "2020-05-26 01:58:14,413 DEV : loss 0.8116015791893005 - score 0.7415\n",
            "2020-05-26 01:58:14,534 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 01:58:16,149 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 01:58:17,667 epoch 4 - iter 3/37 - loss 0.58050281 - samples/sec: 83.45\n",
            "2020-05-26 01:58:30,546 epoch 4 - iter 6/37 - loss 0.65156815 - samples/sec: 82.88\n",
            "2020-05-26 01:58:42,677 epoch 4 - iter 9/37 - loss 0.69160875 - samples/sec: 99.44\n",
            "2020-05-26 01:58:54,493 epoch 4 - iter 12/37 - loss 0.69079146 - samples/sec: 99.56\n",
            "2020-05-26 01:59:06,430 epoch 4 - iter 15/37 - loss 0.70955301 - samples/sec: 84.22\n",
            "2020-05-26 01:59:18,197 epoch 4 - iter 18/37 - loss 0.77189673 - samples/sec: 89.63\n",
            "2020-05-26 01:59:30,368 epoch 4 - iter 21/37 - loss 0.76275112 - samples/sec: 88.31\n",
            "2020-05-26 01:59:42,450 epoch 4 - iter 24/37 - loss 0.74707035 - samples/sec: 96.88\n",
            "2020-05-26 01:59:54,639 epoch 4 - iter 27/37 - loss 0.74299491 - samples/sec: 98.05\n",
            "2020-05-26 02:00:06,146 epoch 4 - iter 30/37 - loss 0.78000999 - samples/sec: 92.39\n",
            "2020-05-26 02:00:18,732 epoch 4 - iter 33/37 - loss 0.79254437 - samples/sec: 88.09\n",
            "2020-05-26 02:00:30,756 epoch 4 - iter 36/37 - loss 0.78969791 - samples/sec: 97.89\n",
            "2020-05-26 02:00:41,683 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:00:41,688 EPOCH 4 done: loss 0.7874 - lr 0.1000000\n",
            "2020-05-26 02:00:43,540 DEV : loss 0.7754347324371338 - score 0.746\n",
            "2020-05-26 02:00:43,664 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 02:00:45,344 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:00:46,948 epoch 5 - iter 3/37 - loss 0.61136788 - samples/sec: 82.78\n",
            "2020-05-26 02:00:59,835 epoch 5 - iter 6/37 - loss 0.79407686 - samples/sec: 89.75\n",
            "2020-05-26 02:01:12,048 epoch 5 - iter 9/37 - loss 0.72696088 - samples/sec: 92.14\n",
            "2020-05-26 02:01:23,918 epoch 5 - iter 12/37 - loss 0.69322541 - samples/sec: 95.48\n",
            "2020-05-26 02:01:36,014 epoch 5 - iter 15/37 - loss 0.72787457 - samples/sec: 87.68\n",
            "2020-05-26 02:01:47,865 epoch 5 - iter 18/37 - loss 0.72937183 - samples/sec: 101.27\n",
            "2020-05-26 02:02:01,894 epoch 5 - iter 21/37 - loss 0.71023825 - samples/sec: 93.91\n",
            "2020-05-26 02:02:13,979 epoch 5 - iter 24/37 - loss 0.70534987 - samples/sec: 80.38\n",
            "2020-05-26 02:02:26,050 epoch 5 - iter 27/37 - loss 0.67862597 - samples/sec: 94.39\n",
            "2020-05-26 02:02:37,937 epoch 5 - iter 30/37 - loss 0.68136793 - samples/sec: 95.46\n",
            "2020-05-26 02:02:49,859 epoch 5 - iter 33/37 - loss 0.68665289 - samples/sec: 94.66\n",
            "2020-05-26 02:03:01,625 epoch 5 - iter 36/37 - loss 0.69793470 - samples/sec: 94.29\n",
            "2020-05-26 02:03:13,431 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:03:13,441 EPOCH 5 done: loss 0.6987 - lr 0.1000000\n",
            "2020-05-26 02:03:15,387 DEV : loss 0.977726936340332 - score 0.7188\n",
            "2020-05-26 02:03:15,510 BAD EPOCHS (no improvement): 1\n",
            "2020-05-26 02:03:15,517 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:03:17,034 epoch 6 - iter 3/37 - loss 0.68432456 - samples/sec: 81.46\n",
            "2020-05-26 02:03:28,597 epoch 6 - iter 6/37 - loss 0.82014858 - samples/sec: 93.17\n",
            "2020-05-26 02:03:40,445 epoch 6 - iter 9/37 - loss 0.75381055 - samples/sec: 87.07\n",
            "2020-05-26 02:03:52,473 epoch 6 - iter 12/37 - loss 0.70113593 - samples/sec: 84.13\n",
            "2020-05-26 02:04:04,216 epoch 6 - iter 15/37 - loss 0.67659403 - samples/sec: 91.57\n",
            "2020-05-26 02:04:16,068 epoch 6 - iter 18/37 - loss 0.66622996 - samples/sec: 98.10\n",
            "2020-05-26 02:04:28,417 epoch 6 - iter 21/37 - loss 0.69634743 - samples/sec: 92.02\n",
            "2020-05-26 02:04:41,272 epoch 6 - iter 24/37 - loss 0.72308678 - samples/sec: 90.29\n",
            "2020-05-26 02:04:53,071 epoch 6 - iter 27/37 - loss 0.70906500 - samples/sec: 92.42\n",
            "2020-05-26 02:05:05,544 epoch 6 - iter 30/37 - loss 0.71683806 - samples/sec: 89.37\n",
            "2020-05-26 02:05:17,402 epoch 6 - iter 33/37 - loss 0.74351676 - samples/sec: 97.92\n",
            "2020-05-26 02:05:29,356 epoch 6 - iter 36/37 - loss 0.73110583 - samples/sec: 95.84\n",
            "2020-05-26 02:05:40,065 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:05:40,070 EPOCH 6 done: loss 0.7283 - lr 0.1000000\n",
            "2020-05-26 02:05:41,973 DEV : loss 1.3314703702926636 - score 0.6372\n",
            "2020-05-26 02:05:42,094 BAD EPOCHS (no improvement): 2\n",
            "2020-05-26 02:05:42,098 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:05:43,518 epoch 7 - iter 3/37 - loss 0.54721795 - samples/sec: 93.68\n",
            "2020-05-26 02:05:55,447 epoch 7 - iter 6/37 - loss 0.55013723 - samples/sec: 93.16\n",
            "2020-05-26 02:06:07,960 epoch 7 - iter 9/37 - loss 0.65962101 - samples/sec: 87.16\n",
            "2020-05-26 02:06:19,449 epoch 7 - iter 12/37 - loss 0.63536543 - samples/sec: 95.34\n",
            "2020-05-26 02:06:31,442 epoch 7 - iter 15/37 - loss 0.61172364 - samples/sec: 89.92\n",
            "2020-05-26 02:06:42,994 epoch 7 - iter 18/37 - loss 0.59360464 - samples/sec: 87.86\n",
            "2020-05-26 02:06:54,629 epoch 7 - iter 21/37 - loss 0.66363371 - samples/sec: 91.84\n",
            "2020-05-26 02:07:07,483 epoch 7 - iter 24/37 - loss 0.65863436 - samples/sec: 87.56\n",
            "2020-05-26 02:07:20,578 epoch 7 - iter 27/37 - loss 0.63824040 - samples/sec: 96.49\n",
            "2020-05-26 02:07:32,997 epoch 7 - iter 30/37 - loss 0.63565352 - samples/sec: 86.58\n",
            "2020-05-26 02:07:44,868 epoch 7 - iter 33/37 - loss 0.62792131 - samples/sec: 93.27\n",
            "2020-05-26 02:07:56,338 epoch 7 - iter 36/37 - loss 0.61502060 - samples/sec: 98.81\n",
            "2020-05-26 02:08:07,533 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:08:07,538 EPOCH 7 done: loss 0.6145 - lr 0.1000000\n",
            "2020-05-26 02:08:09,400 DEV : loss 0.8364132046699524 - score 0.7415\n",
            "2020-05-26 02:08:09,526 BAD EPOCHS (no improvement): 3\n",
            "2020-05-26 02:08:09,532 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:08:10,991 epoch 8 - iter 3/37 - loss 0.73018367 - samples/sec: 81.06\n",
            "2020-05-26 02:08:23,127 epoch 8 - iter 6/37 - loss 0.68467533 - samples/sec: 87.60\n",
            "2020-05-26 02:08:35,181 epoch 8 - iter 9/37 - loss 0.67865740 - samples/sec: 90.23\n",
            "2020-05-26 02:08:46,872 epoch 8 - iter 12/37 - loss 0.64745604 - samples/sec: 87.35\n",
            "2020-05-26 02:08:58,229 epoch 8 - iter 15/37 - loss 0.62569684 - samples/sec: 94.09\n",
            "2020-05-26 02:09:10,381 epoch 8 - iter 18/37 - loss 0.63345180 - samples/sec: 84.50\n",
            "2020-05-26 02:09:21,975 epoch 8 - iter 21/37 - loss 0.63088892 - samples/sec: 94.53\n",
            "2020-05-26 02:09:45,257 epoch 8 - iter 27/37 - loss 0.62198932 - samples/sec: 92.16\n",
            "2020-05-26 02:09:57,743 epoch 8 - iter 30/37 - loss 0.62209852 - samples/sec: 97.85\n",
            "2020-05-26 02:10:09,358 epoch 8 - iter 33/37 - loss 0.61242136 - samples/sec: 89.83\n",
            "2020-05-26 02:10:21,033 epoch 8 - iter 36/37 - loss 0.60308277 - samples/sec: 97.71\n",
            "2020-05-26 02:10:32,406 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:10:32,411 EPOCH 8 done: loss 0.6091 - lr 0.1000000\n",
            "2020-05-26 02:10:34,318 DEV : loss 0.9065431952476501 - score 0.7687\n",
            "2020-05-26 02:10:34,442 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 02:10:36,137 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:10:37,595 epoch 9 - iter 3/37 - loss 0.47820887 - samples/sec: 87.41\n",
            "2020-05-26 02:10:51,187 epoch 9 - iter 6/37 - loss 0.52554451 - samples/sec: 78.62\n",
            "2020-05-26 02:11:02,578 epoch 9 - iter 9/37 - loss 0.63175518 - samples/sec: 86.49\n",
            "2020-05-26 02:11:14,673 epoch 9 - iter 12/37 - loss 0.63928658 - samples/sec: 87.41\n",
            "2020-05-26 02:11:26,248 epoch 9 - iter 15/37 - loss 0.60870379 - samples/sec: 94.33\n",
            "2020-05-26 02:11:37,956 epoch 9 - iter 18/37 - loss 0.57685165 - samples/sec: 88.71\n",
            "2020-05-26 02:11:50,069 epoch 9 - iter 21/37 - loss 0.55850464 - samples/sec: 90.34\n",
            "2020-05-26 02:12:02,217 epoch 9 - iter 24/37 - loss 0.54764725 - samples/sec: 92.61\n",
            "2020-05-26 02:12:14,101 epoch 9 - iter 27/37 - loss 0.54496315 - samples/sec: 96.56\n",
            "2020-05-26 02:12:27,555 epoch 9 - iter 30/37 - loss 0.55366331 - samples/sec: 97.02\n",
            "2020-05-26 02:12:39,434 epoch 9 - iter 33/37 - loss 0.56350513 - samples/sec: 97.48\n",
            "2020-05-26 02:12:51,540 epoch 9 - iter 36/37 - loss 0.56788145 - samples/sec: 94.43\n",
            "2020-05-26 02:13:02,696 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:13:02,701 EPOCH 9 done: loss 0.5653 - lr 0.1000000\n",
            "2020-05-26 02:13:04,537 DEV : loss 0.9147321581840515 - score 0.7551\n",
            "2020-05-26 02:13:04,656 BAD EPOCHS (no improvement): 1\n",
            "2020-05-26 02:13:04,660 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:13:06,375 epoch 10 - iter 3/37 - loss 0.45410344 - samples/sec: 83.48\n",
            "2020-05-26 02:13:18,742 epoch 10 - iter 6/37 - loss 0.56683998 - samples/sec: 89.11\n",
            "2020-05-26 02:13:31,081 epoch 10 - iter 9/37 - loss 0.62324586 - samples/sec: 91.70\n",
            "2020-05-26 02:13:42,697 epoch 10 - iter 12/37 - loss 0.63677777 - samples/sec: 90.59\n",
            "2020-05-26 02:13:54,332 epoch 10 - iter 15/37 - loss 0.62474224 - samples/sec: 91.48\n",
            "2020-05-26 02:14:06,832 epoch 10 - iter 18/37 - loss 0.61648588 - samples/sec: 88.24\n",
            "2020-05-26 02:14:18,674 epoch 10 - iter 21/37 - loss 0.64426205 - samples/sec: 88.07\n",
            "2020-05-26 02:14:30,795 epoch 10 - iter 24/37 - loss 0.61702086 - samples/sec: 93.40\n",
            "2020-05-26 02:14:42,431 epoch 10 - iter 27/37 - loss 0.60905698 - samples/sec: 97.57\n",
            "2020-05-26 02:14:53,953 epoch 10 - iter 30/37 - loss 0.59327515 - samples/sec: 92.70\n",
            "2020-05-26 02:15:05,884 epoch 10 - iter 33/37 - loss 0.58005631 - samples/sec: 93.06\n",
            "2020-05-26 02:15:17,577 epoch 10 - iter 36/37 - loss 0.56900094 - samples/sec: 101.35\n",
            "2020-05-26 02:15:29,775 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:15:29,777 EPOCH 10 done: loss 0.5711 - lr 0.1000000\n",
            "2020-05-26 02:15:31,683 DEV : loss 1.2106125354766846 - score 0.7052\n",
            "2020-05-26 02:15:31,804 BAD EPOCHS (no improvement): 2\n",
            "2020-05-26 02:15:33,623 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 02:15:33,628 Testing using best model ...\n",
            "2020-05-26 02:15:33,635 loading file /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training/best-model.pt\n",
            "2020-05-26 02:15:36,681 0.6164383561643836\t0.6164383561643836\t0.6164383561643836\n",
            "2020-05-26 02:15:36,687 \n",
            "MICRO_AVG: acc 0.7442922374429224 - f1-score 0.6164383561643836\n",
            "MACRO_AVG: acc 0.7442922374429224 - f1-score 0.5543601080815524\n",
            "-1         tp: 40 - fp: 25 - fn: 13 - tn: 68 - precision: 0.6154 - recall: 0.7547 - accuracy: 0.7397 - f1-score: 0.6780\n",
            "0          tp: 43 - fp: 24 - fn: 5 - tn: 74 - precision: 0.6418 - recall: 0.8958 - accuracy: 0.8014 - f1-score: 0.7478\n",
            "1          tp: 7 - fp: 7 - fn: 38 - tn: 94 - precision: 0.5000 - recall: 0.1556 - accuracy: 0.6918 - f1-score: 0.2373\n",
            "2020-05-26 02:15:36,698 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [0.9351709485054016,\n",
              "  0.7792381644248962,\n",
              "  0.8116015791893005,\n",
              "  0.7754347324371338,\n",
              "  0.977726936340332,\n",
              "  1.3314703702926636,\n",
              "  0.8364132046699524,\n",
              "  0.9065431952476501,\n",
              "  0.9147321581840515,\n",
              "  1.2106125354766846],\n",
              " 'dev_score_history': [0.7052154195011338,\n",
              "  0.7369614512471655,\n",
              "  0.7414965986394558,\n",
              "  0.746031746031746,\n",
              "  0.7188208616780045,\n",
              "  0.63718820861678,\n",
              "  0.7414965986394558,\n",
              "  0.7687074829931972,\n",
              "  0.7551020408163265,\n",
              "  0.7052154195011338],\n",
              " 'test_score': 0.7442922374429224,\n",
              " 'train_loss_history': [1.1883643076226518,\n",
              "  0.9111006324355667,\n",
              "  0.8130661345816947,\n",
              "  0.7873718013634553,\n",
              "  0.6987079857168971,\n",
              "  0.7283295320497977,\n",
              "  0.6145436787927473,\n",
              "  0.609109215639733,\n",
              "  0.5653021569187576,\n",
              "  0.571110762454368]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EHmdhl6qYoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# finetune phase1 result with Bert+Flair embeddings\n",
        "#MACRO_AVG: acc 0.7260273972602739 - f1-score 0.5739219483960016"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E2JikhOEZFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "951fe88a-12a7-43b9-e7f3-e73c9ba5598c"
      },
      "source": [
        "from flair.data import Sentence\n",
        "# create example sentence\n",
        "sentence = Sentence(\"Canada's growth weakens as investment drops, consumers fade\")\n",
        "\n",
        "# predict class and print\n",
        "classifier.predict(sentence)\n",
        "\n",
        "print(sentence.labels)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 (0.8477)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwSElzt2Qkai",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "406a64f1-836e-4f59-ba08-b92660279e8f"
      },
      "source": [
        "#  access the Sentence objects in each split directly\n",
        "print(corpus.test[0])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"It is also in the process of taking a seat on CapMan ' s board - a course which has the support of CapMan management .\"   [− Tokens: 26  − Sentence-Labels: {'class': [0 (1.0)]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIrjzZOQQn2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b162f180-aafd-460a-bb89-35dcb0d85a09"
      },
      "source": [
        "print(corpus.train[0])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"Construction is expected to be completed in the summer of 2011 .\"   [− Tokens: 12  − Sentence-Labels: {'class': [0 (1.0)]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lWLmyn0RTwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "d2df3e59-8aa5-4d5e-d1a5-6e81e6658c01"
      },
      "source": [
        "#outputs detailed information on the dataset, each split, and the distribution of class labels.\n",
        "stats = corpus.obtain_statistics()\n",
        "print(stats)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"TRAIN\": {\n",
            "        \"dataset\": \"TRAIN\",\n",
            "        \"total_number_of_documents\": 1168,\n",
            "        \"number_of_documents_per_class\": {\n",
            "            \"0\": 376,\n",
            "            \"-1\": 400,\n",
            "            \"1\": 392\n",
            "        },\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 37019,\n",
            "            \"min\": 5,\n",
            "            \"max\": 69,\n",
            "            \"avg\": 31.694349315068493\n",
            "        }\n",
            "    },\n",
            "    \"TEST\": {\n",
            "        \"dataset\": \"TEST\",\n",
            "        \"total_number_of_documents\": 146,\n",
            "        \"number_of_documents_per_class\": {\n",
            "            \"0\": 48,\n",
            "            \"-1\": 53,\n",
            "            \"1\": 45\n",
            "        },\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 4682,\n",
            "            \"min\": 4,\n",
            "            \"max\": 62,\n",
            "            \"avg\": 32.06849315068493\n",
            "        }\n",
            "    },\n",
            "    \"DEV\": {\n",
            "        \"dataset\": \"DEV\",\n",
            "        \"total_number_of_documents\": 147,\n",
            "        \"number_of_documents_per_class\": {\n",
            "            \"0\": 58,\n",
            "            \"1\": 48,\n",
            "            \"-1\": 41\n",
            "        },\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 4442,\n",
            "            \"min\": 7,\n",
            "            \"max\": 67,\n",
            "            \"avg\": 30.217687074829932\n",
            "        }\n",
            "    }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5maM-7gQWqJ",
        "colab_type": "text"
      },
      "source": [
        "### Second Stage (train on hand annotated datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbrXhSRlnCR3",
        "colab_type": "text"
      },
      "source": [
        "#### Build corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0E6iEy9UgYP",
        "colab_type": "code",
        "outputId": "4011ee16-553a-4a0f-8ef0-578333fa9bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#new_data_folder = './drive/My Drive/capstone/data/second_stage/'\n",
        "new_data_folder = '/content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled/'\n",
        "new_column_name_map = {5: \"text\", 4: \"label_topic\"}\n",
        "\n",
        "corpus: Corpus = CSVClassificationCorpus(new_data_folder,\n",
        "                                         new_column_name_map,\n",
        "                                         skip_header=True, #no header in kaggle data\n",
        "                                         delimiter=',',    # comma separated rows\n",
        "                                         train_file='GDP_train_df_oversampled.csv',\n",
        "                                         dev_file = 'GDP_eva_df.csv',\n",
        "                                         test_file = 'GDP_eva_df.csv'\n",
        ")"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 04:01:52,955 Reading data from /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled\n",
            "2020-05-26 04:01:52,958 Train: /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled/GDP_train_df_oversampled.csv\n",
            "2020-05-26 04:01:52,960 Dev: /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled/GDP_eva_df.csv\n",
            "2020-05-26 04:01:52,962 Test: /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled/GDP_eva_df.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAGDir71nHb6",
        "colab_type": "text"
      },
      "source": [
        "#### Second Stage fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWg4fjqNYMKa",
        "colab_type": "code",
        "outputId": "2e32057d-b239-46d0-8ad2-48ca25ca8951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# benchmark_embeddings = FlairEmbeddings(data_folder + 'best-model.pt')\n",
        "benchmark_classifier = TextClassifier.load(data_folder + 'best-model.pt')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 03:52:11,971 loading file /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotated_sample_for_training/best-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr1XWYH6QZGl",
        "colab_type": "code",
        "outputId": "3d01e8fb-9837-4d0e-834e-5299dbf8edb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer = ModelTrainer(benchmark_classifier, corpus)\n",
        "trainer.train(new_data_folder, max_epochs=10)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 03:52:12,960 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:52:12,966 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentLSTMEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): BertEmbeddings(\n",
            "        (model): BertModel(\n",
            "          (embeddings): BertEmbeddings(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoder(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=3072, out_features=256, bias=True)\n",
            "    (rnn): GRU(256, 512)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-05-26 03:52:12,972 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:52:12,975 Corpus: \"Corpus: 132 train + 33 dev + 33 test sentences\"\n",
            "2020-05-26 03:52:12,977 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:52:12,979 Parameters:\n",
            "2020-05-26 03:52:12,981  - learning_rate: \"0.1\"\n",
            "2020-05-26 03:52:12,983  - mini_batch_size: \"32\"\n",
            "2020-05-26 03:52:12,988  - patience: \"3\"\n",
            "2020-05-26 03:52:12,989  - anneal_factor: \"0.5\"\n",
            "2020-05-26 03:52:12,992  - max_epochs: \"10\"\n",
            "2020-05-26 03:52:12,996  - shuffle: \"True\"\n",
            "2020-05-26 03:52:12,998  - train_with_dev: \"False\"\n",
            "2020-05-26 03:52:13,000  - batch_growth_annealing: \"False\"\n",
            "2020-05-26 03:52:13,003 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:52:13,004 Model training base path: \"/content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled\"\n",
            "2020-05-26 03:52:13,006 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:52:13,010 Device: cuda:0\n",
            "2020-05-26 03:52:13,012 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:52:13,014 Embeddings storage mode: cpu\n",
            "2020-05-26 03:52:13,033 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:52:13,746 epoch 1 - iter 1/5 - loss 1.96003103 - samples/sec: 74.39\n",
            "2020-05-26 03:52:25,364 epoch 1 - iter 2/5 - loss 1.60700214 - samples/sec: 88.45\n",
            "2020-05-26 03:52:36,330 epoch 1 - iter 3/5 - loss 1.48431726 - samples/sec: 84.03\n",
            "2020-05-26 03:52:47,143 epoch 1 - iter 4/5 - loss 1.45847827 - samples/sec: 86.64\n",
            "2020-05-26 03:52:57,390 epoch 1 - iter 5/5 - loss 1.23165728 - samples/sec: 547.71\n",
            "2020-05-26 03:53:08,034 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:53:08,036 EPOCH 1 done: loss 1.2317 - lr 0.1000000\n",
            "2020-05-26 03:53:08,794 DEV : loss 0.9444580674171448 - score 0.6364\n",
            "2020-05-26 03:53:08,842 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 03:53:10,490 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:53:11,229 epoch 2 - iter 1/5 - loss 1.12052655 - samples/sec: 70.65\n",
            "2020-05-26 03:53:23,076 epoch 2 - iter 2/5 - loss 1.05579427 - samples/sec: 94.25\n",
            "2020-05-26 03:53:33,705 epoch 2 - iter 3/5 - loss 1.10256555 - samples/sec: 87.39\n",
            "2020-05-26 03:53:44,434 epoch 2 - iter 4/5 - loss 1.13554661 - samples/sec: 95.31\n",
            "2020-05-26 03:53:55,182 epoch 2 - iter 5/5 - loss 1.10480375 - samples/sec: 519.75\n",
            "2020-05-26 03:54:06,108 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:54:06,114 EPOCH 2 done: loss 1.1048 - lr 0.1000000\n",
            "2020-05-26 03:54:07,359 DEV : loss 0.45924797654151917 - score 0.7374\n",
            "2020-05-26 03:54:07,401 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 03:54:09,065 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:54:10,238 epoch 3 - iter 1/5 - loss 0.80145395 - samples/sec: 81.96\n",
            "2020-05-26 03:54:21,986 epoch 3 - iter 2/5 - loss 0.85645407 - samples/sec: 88.01\n",
            "2020-05-26 03:54:33,226 epoch 3 - iter 3/5 - loss 0.85431375 - samples/sec: 95.24\n",
            "2020-05-26 03:54:43,730 epoch 3 - iter 4/5 - loss 0.83634895 - samples/sec: 89.14\n",
            "2020-05-26 03:54:54,789 epoch 3 - iter 5/5 - loss 0.74585029 - samples/sec: 439.43\n",
            "2020-05-26 03:55:05,700 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:55:05,705 EPOCH 3 done: loss 0.7459 - lr 0.1000000\n",
            "2020-05-26 03:55:06,429 DEV : loss 0.37944352626800537 - score 0.7778\n",
            "2020-05-26 03:55:06,469 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 03:55:08,082 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:55:08,775 epoch 4 - iter 1/5 - loss 0.72710168 - samples/sec: 84.06\n",
            "2020-05-26 03:55:20,368 epoch 4 - iter 2/5 - loss 0.86326680 - samples/sec: 77.36\n",
            "2020-05-26 03:55:31,231 epoch 4 - iter 3/5 - loss 0.75596253 - samples/sec: 86.80\n",
            "2020-05-26 03:55:41,959 epoch 4 - iter 4/5 - loss 0.69968483 - samples/sec: 92.31\n",
            "2020-05-26 03:55:54,769 epoch 4 - iter 5/5 - loss 0.63973669 - samples/sec: 565.05\n",
            "2020-05-26 03:56:05,127 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:56:05,132 EPOCH 4 done: loss 0.6397 - lr 0.1000000\n",
            "2020-05-26 03:56:05,849 DEV : loss 0.4544934928417206 - score 0.7576\n",
            "2020-05-26 03:56:05,891 BAD EPOCHS (no improvement): 1\n",
            "2020-05-26 03:56:05,896 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:56:06,529 epoch 5 - iter 1/5 - loss 0.77579612 - samples/sec: 82.38\n",
            "2020-05-26 03:56:17,306 epoch 5 - iter 2/5 - loss 0.56800693 - samples/sec: 85.67\n",
            "2020-05-26 03:56:27,875 epoch 5 - iter 3/5 - loss 0.49355650 - samples/sec: 97.80\n",
            "2020-05-26 03:56:38,897 epoch 5 - iter 4/5 - loss 0.51418401 - samples/sec: 95.04\n",
            "2020-05-26 03:56:50,034 epoch 5 - iter 5/5 - loss 0.46796063 - samples/sec: 548.70\n",
            "2020-05-26 03:57:01,252 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:57:01,258 EPOCH 5 done: loss 0.4680 - lr 0.1000000\n",
            "2020-05-26 03:57:01,997 DEV : loss 0.7482798099517822 - score 0.7374\n",
            "2020-05-26 03:57:02,044 BAD EPOCHS (no improvement): 2\n",
            "2020-05-26 03:57:02,048 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:57:02,957 epoch 6 - iter 1/5 - loss 0.81076556 - samples/sec: 94.70\n",
            "2020-05-26 03:57:13,788 epoch 6 - iter 2/5 - loss 0.67451671 - samples/sec: 93.60\n",
            "2020-05-26 03:57:24,737 epoch 6 - iter 3/5 - loss 0.65093382 - samples/sec: 90.78\n",
            "2020-05-26 03:57:35,741 epoch 6 - iter 4/5 - loss 0.65065739 - samples/sec: 91.00\n",
            "2020-05-26 03:57:45,980 epoch 6 - iter 5/5 - loss 0.62794137 - samples/sec: 565.09\n",
            "2020-05-26 03:57:56,810 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:57:56,814 EPOCH 6 done: loss 0.6279 - lr 0.1000000\n",
            "2020-05-26 03:57:57,542 DEV : loss 0.36143139004707336 - score 0.8384\n",
            "2020-05-26 03:57:57,586 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 03:57:59,328 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:58:00,042 epoch 7 - iter 1/5 - loss 0.31046256 - samples/sec: 79.91\n",
            "2020-05-26 03:58:11,486 epoch 7 - iter 2/5 - loss 0.25626053 - samples/sec: 84.05\n",
            "2020-05-26 03:58:22,514 epoch 7 - iter 3/5 - loss 0.29194672 - samples/sec: 94.81\n",
            "2020-05-26 03:58:33,264 epoch 7 - iter 4/5 - loss 0.31715562 - samples/sec: 90.01\n",
            "2020-05-26 03:58:43,670 epoch 7 - iter 5/5 - loss 0.28413653 - samples/sec: 554.57\n",
            "2020-05-26 03:58:54,209 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:58:54,215 EPOCH 7 done: loss 0.2841 - lr 0.1000000\n",
            "2020-05-26 03:58:54,929 DEV : loss 0.24568629264831543 - score 0.9192\n",
            "2020-05-26 03:58:54,972 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-26 03:58:56,483 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:58:57,163 epoch 8 - iter 1/5 - loss 0.14797455 - samples/sec: 86.83\n",
            "2020-05-26 03:59:09,114 epoch 8 - iter 2/5 - loss 0.21194914 - samples/sec: 92.81\n",
            "2020-05-26 03:59:20,187 epoch 8 - iter 3/5 - loss 0.18425903 - samples/sec: 83.95\n",
            "2020-05-26 03:59:31,166 epoch 8 - iter 4/5 - loss 0.19477798 - samples/sec: 96.50\n",
            "2020-05-26 03:59:42,117 epoch 8 - iter 5/5 - loss 0.21732018 - samples/sec: 536.80\n",
            "2020-05-26 03:59:53,126 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:59:53,131 EPOCH 8 done: loss 0.2173 - lr 0.1000000\n",
            "2020-05-26 03:59:53,928 DEV : loss 0.33372384309768677 - score 0.8586\n",
            "2020-05-26 03:59:53,970 BAD EPOCHS (no improvement): 1\n",
            "2020-05-26 03:59:53,975 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 03:59:54,598 epoch 9 - iter 1/5 - loss 0.40064240 - samples/sec: 98.81\n",
            "2020-05-26 04:00:05,404 epoch 9 - iter 2/5 - loss 0.28024115 - samples/sec: 88.46\n",
            "2020-05-26 04:00:16,589 epoch 9 - iter 3/5 - loss 0.26109419 - samples/sec: 82.18\n",
            "2020-05-26 04:00:27,229 epoch 9 - iter 4/5 - loss 0.23700718 - samples/sec: 92.95\n",
            "2020-05-26 04:00:38,132 epoch 9 - iter 5/5 - loss 0.24684789 - samples/sec: 543.25\n",
            "2020-05-26 04:00:48,697 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 04:00:48,702 EPOCH 9 done: loss 0.2468 - lr 0.1000000\n",
            "2020-05-26 04:00:49,970 DEV : loss 0.5067477822303772 - score 0.7778\n",
            "2020-05-26 04:00:50,018 BAD EPOCHS (no improvement): 2\n",
            "2020-05-26 04:00:50,024 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 04:00:50,743 epoch 10 - iter 1/5 - loss 0.43977296 - samples/sec: 71.25\n",
            "2020-05-26 04:01:03,273 epoch 10 - iter 2/5 - loss 0.33840697 - samples/sec: 97.46\n",
            "2020-05-26 04:01:14,400 epoch 10 - iter 3/5 - loss 0.25348864 - samples/sec: 87.90\n",
            "2020-05-26 04:01:25,258 epoch 10 - iter 4/5 - loss 0.25054494 - samples/sec: 88.13\n",
            "2020-05-26 04:01:35,813 epoch 10 - iter 5/5 - loss 0.28705164 - samples/sec: 491.31\n",
            "2020-05-26 04:01:46,612 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 04:01:46,619 EPOCH 10 done: loss 0.2871 - lr 0.1000000\n",
            "2020-05-26 04:01:47,390 DEV : loss 0.7507543563842773 - score 0.8182\n",
            "2020-05-26 04:01:47,433 BAD EPOCHS (no improvement): 3\n",
            "2020-05-26 04:01:49,013 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-26 04:01:49,018 Testing using best model ...\n",
            "2020-05-26 04:01:49,027 loading file /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled/best-model.pt\n",
            "2020-05-26 04:01:50,628 0.8787878787878788\t0.8787878787878788\t0.8787878787878788\n",
            "2020-05-26 04:01:50,633 \n",
            "MICRO_AVG: acc 0.9191919191919192 - f1-score 0.8787878787878788\n",
            "MACRO_AVG: acc 0.9191919191919192 - f1-score 0.8770653907496012\n",
            "-1         tp: 10 - fp: 1 - fn: 1 - tn: 21 - precision: 0.9091 - recall: 0.9091 - accuracy: 0.9394 - f1-score: 0.9091\n",
            "0          tp: 8 - fp: 0 - fn: 3 - tn: 22 - precision: 1.0000 - recall: 0.7273 - accuracy: 0.9091 - f1-score: 0.8421\n",
            "1          tp: 11 - fp: 3 - fn: 0 - tn: 19 - precision: 0.7857 - recall: 1.0000 - accuracy: 0.9091 - f1-score: 0.8800\n",
            "2020-05-26 04:01:50,637 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [0.9444580674171448,\n",
              "  0.45924797654151917,\n",
              "  0.37944352626800537,\n",
              "  0.4544934928417206,\n",
              "  0.7482798099517822,\n",
              "  0.36143139004707336,\n",
              "  0.24568629264831543,\n",
              "  0.33372384309768677,\n",
              "  0.5067477822303772,\n",
              "  0.7507543563842773],\n",
              " 'dev_score_history': [0.6363636363636364,\n",
              "  0.7373737373737373,\n",
              "  0.7777777777777778,\n",
              "  0.7575757575757576,\n",
              "  0.7373737373737373,\n",
              "  0.8383838383838383,\n",
              "  0.9191919191919192,\n",
              "  0.8585858585858586,\n",
              "  0.7777777777777778,\n",
              "  0.8181818181818182],\n",
              " 'test_score': 0.9191919191919192,\n",
              " 'train_loss_history': [1.2316572785377502,\n",
              "  1.1048037528991699,\n",
              "  0.7458502948284149,\n",
              "  0.6397366881370544,\n",
              "  0.46796063184738157,\n",
              "  0.627941370010376,\n",
              "  0.28413652777671816,\n",
              "  0.21732018291950225,\n",
              "  0.24684788584709166,\n",
              "  0.2870516419410706]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HKyyw1CSZFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MACRO_AVG: acc 0.8787878787878788 - f1-score 0.8233082706766918\n",
        "\n",
        "## after fixing oversampling to be on train-only\n",
        "\n",
        "#MACRO_AVG: acc 0.9191919191919192 - f1-score 0.8770653907496012"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArPepPU3mIqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "91253a64-a32c-4391-c2f6-412b2454494d"
      },
      "source": [
        "## load the 2nd-stage finetuned model:\n",
        "\n",
        "finetuned_classifier = TextClassifier.load(new_data_folder + 'best-model.pt')"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 04:01:50,662 loading file /content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled/best-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0cksDjXc_rm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b3cb5d6-73b3-4a2e-8f12-4d425f3782e3"
      },
      "source": [
        "# predict same sentence from above:\n",
        "\n",
        "sentence = Sentence(\"Canada's growth weakens as investment drops, consumers fade\")\n",
        "\n",
        "finetuned_classifier.predict(sentence)\n",
        "\n",
        "print(sentence.labels) ## correct"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1 (0.5809)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NECOcOhnh4Se",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "0e912ee7-8675-4460-c457-8007a6bc7e60"
      },
      "source": [
        "dir(sentence.labels)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__add__',\n",
              " '__class__',\n",
              " '__contains__',\n",
              " '__delattr__',\n",
              " '__delitem__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__iadd__',\n",
              " '__imul__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__mul__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__reversed__',\n",
              " '__rmul__',\n",
              " '__setattr__',\n",
              " '__setitem__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " 'append',\n",
              " 'clear',\n",
              " 'copy',\n",
              " 'count',\n",
              " 'extend',\n",
              " 'index',\n",
              " 'insert',\n",
              " 'pop',\n",
              " 'remove',\n",
              " 'reverse',\n",
              " 'sort']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwNDl0gziMz5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "724d7f7b-1e83-4ae6-e213-d3803d016869"
      },
      "source": [
        "dir(sentence.labels[0])\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_score',\n",
              " '_value',\n",
              " 'score',\n",
              " 'to_dict',\n",
              " 'value']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpR-w18zi9Xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16bcf0b7-fe77-4f38-acfb-221af0893486"
      },
      "source": [
        "sentence.labels[0].value"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'-1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gl9mmmVi_pG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1471196-fdf4-485a-db9a-e1dceea90495"
      },
      "source": [
        "sentence.labels[0].score"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5809285640716553"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzPahERUdG4S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a957766d-00df-4934-be3c-37ceeb18a38b"
      },
      "source": [
        "## error analysis\n",
        "\n",
        "# get gold labels\n",
        "#print(corpus.test[0])\n",
        "print(corpus.test[0])\n",
        "print(corpus.test[0].labels)\n",
        "\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"Trump claims victory as GDP grows at fastest pace since 2014 . President Donald Trump said the U.S. economy is on track to reach an annual growth rate of more than 3 per cent , as he celebrated a report Friday that the economy expanded in the second quarter at the fastest pace in four years .\"   [− Tokens: 57  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
            "[1 (1.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_roihowwdy4I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a498ba5-b57a-48fe-bfc3-db6c340d8309"
      },
      "source": [
        "len(corpus.test)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMQq2bGTgGOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdp_test_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/capstone_betterdwelling/annotations_bnn_cbc/oversampled/GDP_eva_df.csv\",usecols=['title_desc_sent_1','title_desc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf09VixzPb5a",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJqJpE8jgmuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "14c32119-1e6e-4f37-8ac8-a93f6cf3821c"
      },
      "source": [
        "gdp_test_df.head()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title_desc_sent_1</th>\n",
              "      <th>title_desc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>S&amp;P 500 just hit another record high — and eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Forget interest rate decisions. Loonie traders...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>With little to say on coronavirus, is Quebec's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Statistics are great unless they measure the w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>What to watch in China's GDP report: Trade, au...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   title_desc_sent_1                                         title_desc\n",
              "0                  1  S&P 500 just hit another record high — and eve...\n",
              "1                  1  Forget interest rate decisions. Loonie traders...\n",
              "2                  0  With little to say on coronavirus, is Quebec's...\n",
              "3                  0  Statistics are great unless they measure the w...\n",
              "4                 -1  What to watch in China's GDP report: Trade, au..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTdgTHtJhEQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fcdabad-e53b-489a-ee59-a18b6031f58f"
      },
      "source": [
        "gdp_test_df['title_desc'].iloc[0]"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'S&P 500 just hit another record high — and everything’s flashing Risk On. Optimism over trade deal with China'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPm2w8npeSsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87a4bf74-6061-4953-ad99-992d341a9bf6"
      },
      "source": [
        "correct = 0\n",
        "\n",
        "for i in range(len(corpus.test)):\n",
        "  print(corpus.test[i])\n",
        "  \n",
        "\n",
        "  #get predictions\n",
        "  #print(gdp_test_df['title_desc'].iloc[i])\n",
        "  sentence = Sentence(gdp_test_df['title_desc'].iloc[i])\n",
        "\n",
        "  finetuned_classifier.predict(sentence)\n",
        "\n",
        "  print(sentence.labels)\n",
        "\n",
        "  #get gold label\n",
        "  print(corpus.test[i].labels)\n",
        "\n",
        "  #calculate correct guesses\n",
        "  if sentence.labels[0].value == corpus.test[i].labels[0].value:\n",
        "    correct += 1\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"S & P 500 just hit another record high — and everything ’s flashing Risk On . Optimism over trade deal with China\"   [− Tokens: 23  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
            "[0 (0.5386)]\n",
            "[1 (1.0)]\n",
            "Sentence: \"Forget interest rate decisions . Loonie traders are banking on immigration to Canada . Flood of immigrants and non-permanent residents to levels not seen in a century a main driver supporting Canada 's economic expansion\"   [− Tokens: 35  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
            "[1 (0.8859)]\n",
            "[1 (1.0)]\n",
            "Sentence: \"With little to say on coronavirus , is Quebec 's budget ' of the future ' out of sync with the present ? . Finance minister ‚ Äôs budget bets outbreak is more public health than economic problem\"   [− Tokens: 38  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
            "[0 (0.9825)]\n",
            "[0 (1.0)]\n",
            "Sentence: \"Statistics are great unless they measure the wrong things : Don Pittis . Statistics Canada looks to Big Data to size up Canadians , but are the numbers biased ?\"   [− Tokens: 30  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
            "[0 (0.9905)]\n",
            "[0 (1.0)]\n",
            "Sentence: \"What to watch in China 's GDP report : Trade , autos , manufacturing . With China ’s economic expansion expected to slow as trade wars heat up , a closer look at the data may offer a better look at what ’s really happening in the world ’s second-largest economy .\"   [− Tokens: 52  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.8614)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"OPINION | Yes , Alberta 's budget raises taxes . Yes , it cuts AISH . No , it 's not ' austerity. ' . Trevor Tombe cuts through the spin and extracts the facts from ' the most consequential budget in years'\"   [− Tokens: 43  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
            "[0 (0.9279)]\n",
            "[0 (1.0)]\n",
            "Sentence: \"Consumer worries cloud best Canadian output gain in two years . Canada ’s economy recorded a stronger-than-expected rebound in the second quarter as exports recovered , but surprisingly weak consumption and business investment will cast doubts on the expansion ’s sustainability .\"   [− Tokens: 42  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
            "[1 (0.7199)]\n",
            "[0 (1.0)]\n",
            "Sentence: \"Forget about getting fresh trade data — the U.S. shutdown is now hitting Canada . Canada 's statistics agency will stop releasing monthly merchandise trade data indefinitely because of the U.S. government shutdown\"   [− Tokens: 33  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.9943)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"China 's growth slides to weakest pace in almost three decades . China ’s economy slowed to the weakest pace since quarterly data began in 1992 amid the ongoing trade standoff with the U.S. , while monthly indicators provided signs that a stabilization is emerging .\"   [− Tokens: 46  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.8207)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"Rail blockades causing containers to pile up at Canadian ports . Containers sitting idle at major import hubs of Vancouver , Montreal and Halifax\"   [− Tokens: 24  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
            "[0 (0.9961)]\n",
            "[0 (1.0)]\n",
            "Sentence: \"Fed signals patience on rate moves ‘ for some time’ . No strong case to move in either direction\"   [− Tokens: 19  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
            "[0 (0.9886)]\n",
            "[0 (1.0)]\n",
            "Sentence: \"British immigration overhaul takes aim at ' cheap labour from Europe ' . Changes likely to impact service , farming and health-care industries , among others\"   [− Tokens: 26  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
            "[0 (0.9892)]\n",
            "[0 (1.0)]\n",
            "Sentence: \"Trudeau 's big ‚ Äî and underfunded ‚ Äî health care promises . Liberals' pledge to ' fix ' gaps in health care system would cost billions more\"   [− Tokens: 28  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
            "[0 (0.8682)]\n",
            "[0 (1.0)]\n",
            "Sentence: \"Setback to Montreal retail reopening shows rocky path to getting economy running again . The high number of COVID-19 cases in Montreal hospitals caused Premier Francois Legault to postpone store openings by a week\"   [− Tokens: 34  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.7826)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"Bank of Canada ’s Stephen Poloz gets chance today to put rate cut speculation on ice . Investors now see a strong chance of a cut by the Bank of Canada over the next 12 months\"   [− Tokens: 36  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[1 (0.8212)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"Weak Canadian GDP data opens door for stimulus budget : Scotiabank . Canada ’s sluggish economic data opens the door for Finance Minister Bill Morneau to spend more in his pre-election budget , according to Bank of Nova Scotia .\"   [− Tokens: 40  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.6015)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"Edmonton economy to improve only slightly in 2020 , city 's chief economist says . Capital city can expect ‚ Äòvery modest ‚ Äô growth , John Rose says\"   [− Tokens: 29  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
            "[0 (0.8225)]\n",
            "[1 (1.0)]\n",
            "Sentence: \"Companies look to cash in on out-of-this-world profits in new space economy . Canadian firms among many investing billions in extraterrestrial ventures\"   [− Tokens: 22  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
            "[1 (0.7831)]\n",
            "[1 (1.0)]\n",
            "Sentence: \"Canada 's yield curve inverts the most in 12 years on Trump 's Mexico tariffs threat . Investors are worried the tariffs will derail the revised NAFTA\"   [− Tokens: 27  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.9269)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"Oil rebound drives Canada 's best two-month GDP gain since 2017 . Canada recorded a second strong month of growth in April , driven by rebounding oil output that is returning the nation ’s economy to a more solid footing .\"   [− Tokens: 41  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
            "[1 (0.9936)]\n",
            "[1 (1.0)]\n",
            "Sentence: \"IMF ready to mobilize $ 1 trillion in loans to help countries counter coronavirus outbreak . ' As the virus spreads , the case for a coordinated and synchronized global fiscal stimulus is becoming stronger by the hour '\"   [− Tokens: 39  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.9276)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"Wage reductions , more layoffs for City of Winnipeg workers part of worst-case COVID-19 scenarios . City comes up with action plans for 3 possible outcomes of coronavirus pandemic\"   [− Tokens: 29  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.8395)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"U.S. second-quarter growth revised up to 4.2 % on software , trade . The U.S. economy expanded in the second quarter at a slightly faster pace than previously estimated on revisions to imports and software spending , bolstering the strongest period of growth since 2014 , according to Commerce Department data released Wednesday .\"   [− Tokens: 54  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
            "[1 (0.9468)]\n",
            "[1 (1.0)]\n",
            "Sentence: \"‘ Crisis like no other’ : IMF warns ‘ Great Lockdown’ recession will be worst in almost 100 years . In a further sign of pessimism , the IMF sketched out three alternative scenarios in which the virus lasted longer than expected , returned in 2021 or both\"   [− Tokens: 48  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[0 (0.9687)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"U.S. first-quarter GDP growth revised down to 2 % on services . The U.S. economy expanded in the first quarter at a slower pace than previously estimated , reflecting downward revisions to spending on services and to inventory investment , according to Commerce Department data released Thursday .\"   [− Tokens: 48  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.5055)]\n",
            "[-1 (1.0)]\n",
            "Sentence: \"Canada 's economy slows even as business investment perks up . Canada ’s economy slowed sharply in the third quarter , as a drop in exports and draw down in business inventories masked a rebound in domestic demand .\"   [− Tokens: 39  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
            "[-1 (0.9292)]\n",
            "[-1 (1.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1s2hW88WyyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd6768fd-9ac3-408b-a0b2-503c2332c940"
      },
      "source": [
        "len(corpus.test)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlCXaV3cX74p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct #after train-only oversampling : 21/26 = 81% accuracy (Bert-only embedding)\n",
        "\n",
        "# Error analysis\n",
        "\n",
        "#reasonable wrongs\n",
        "\n",
        "## tricky one -- econ 'stronger-than-expected\". arguable golden label\n",
        "# Sentence: \"Consumer worries cloud best Canadian output gain in two years . Canada ’s economy recorded a stronger-than-expected rebound in the second quarter as exports recovered , but surprisingly weak consumption and business investment will cast doubts on the expansion ’s sustainability .\"   [− Tokens: 42  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
        "# [1 (0.7199)]\n",
        "# [0 (1.0)]\n",
        "\n",
        "## doubt over golden label... not related to econ but related to interest rate\n",
        "# Sentence: \"Bank of Canada ’s Stephen Poloz gets chance today to put rate cut speculation on ice . Investors now see a strong chance of a cut by the Bank of Canada over the next 12 months\"   [− Tokens: 36  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
        "# [1 (0.8212)]\n",
        "# [-1 (1.0)]\n",
        "\n",
        "## also doubt over golden label. \"improve only slightly\" -- signals lower-than-expectation?\n",
        "# Sentence: \"Edmonton economy to improve only slightly in 2020 , city 's chief economist says . Capital city can expect ‚ Äòvery modest ‚ Äô growth , John Rose says\"   [− Tokens: 29  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
        "# [0 (0.8225)]\n",
        "# [1 (1.0)]\n",
        "\n",
        "#unreasonable wrongs\n",
        "\n",
        "## more related to stock market but 'optimism over trade deal' bodes well for economy. a tricky one? low confidence score\n",
        "# Sentence: \"S & P 500 just hit another record high — and everything ’s flashing Risk On . Optimism over trade deal with China\"   [− Tokens: 23  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
        "# [0 (0.5386)]\n",
        "# [1 (1.0)]\n",
        "\n",
        "## recession, crisis are very negative words. how come?\n",
        "# Sentence: \"‘ Crisis like no other’ : IMF warns ‘ Great Lockdown’ recession will be worst in almost 100 years . In a further sign of pessimism , the IMF sketched out three alternative scenarios in which the virus lasted longer than expected , returned in 2021 or both\"   [− Tokens: 48  − Sentence-Labels: {'class': [-1 (1.0)]}]\n",
        "# [0 (0.9687)]\n",
        "# [-1 (1.0)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL29Ak69ebux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e4b7bb3-495f-42d9-ca31-1b24532fecac"
      },
      "source": [
        " correct #27/33 = 82% accuracy with Bert+Flair embeddings\n",
        "\n",
        "\n",
        " #before phase2 finetuning, only 66% accuracy with 1500 training examples of less GDP-specific sentiment (vs 77% acc with oversampling Kaggle dataset)\n",
        "\n",
        " #reasonable wrongs\n",
        "\n",
        "## words in negative sense but meaning is neutral\n",
        "# Sentence: \"Why the Canadian economy seems divorced from traditional signals : Don Pittis . With housing and oil off the boil , why has n't the Canadian economy gone into free fall ?\"   [− Tokens: 32  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
        "# [-1 (0.9096)]\n",
        "# [0 (1.0)]\n",
        "\n",
        "## not learning enough that sentiment should be Canada-specific\n",
        "# Sentence: \"Trump is threatening Iran with more sanctions — but what 's left to target ? . Current sanctions have already sent the country spiralling into a deep recession\"   [− Tokens: 28  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
        "# [-1 (0.9762)]\n",
        "# [0 (1.0)]\n",
        "\n",
        "## not learning enough that sentiment should be GDP-specific\n",
        "# Sentence: \"If you thought August was bad , get ready for the worst month for Canadian stocks . In the past 10 years , the TSX has dropped an average 1.5 % in September\"   [− Tokens: 33  − Sentence-Labels: {'class': [0 (1.0)]}]\n",
        "# [-1 (0.5721)]\n",
        "# [0 (1.0)]\n",
        "\n",
        "## not that reasonable wrongs -- all predicted to the opposite extreme... problematic!\n",
        "\n",
        "## 'stabilized' overshadowed by 'slowing' and 'weakest'? -- quite inconfident prediction\n",
        "# Sentence: \"China 's economy grew 6 % in fourth quarter as demand stabilized . China ’s economy stabilized last quarter after slowing to the weakest pace in almost three decades , with the first acceleration in investment since June signaling that a firmer recovery could be underway .\"   [− Tokens: 47  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
        "# [-1 (0.5108)]\n",
        "# [1 (1.0)]\n",
        "\n",
        "## 'fastest growth' vs \"trade tension\"/ \"threat\"\n",
        "# Sentence: \"Oil drives Canada 's fastest economic growth spurt in a year . Canada ’s economy grew at the fastest pace in a year , further evidence of a solid expansion even as trade tensions with the U.S. remain a threat .\"   [− Tokens: 41  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
        "# [-1 (0.6974)]\n",
        "# [1 (1.0)]\n",
        "\n",
        "## 'boom','rosy' vs 'bumpy ride' -- description balanced out but positive headline should have more weight.\n",
        "# Sentence: \"Mining boom to drive economic growth in territories beyond rest of Canada : report . The Conference Board of Canada says outlook rosy for Nunavut and Yukon , but N.W.T. in for a bumpy ride\"   [− Tokens: 35  − Sentence-Labels: {'class': [1 (1.0)]}]\n",
        "# [-1 (0.5803)]\n",
        "# [1 (1.0)]"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJLlgMV4jWPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## check accuracy if only count confident predictions(condifence score > 0.7?)\n",
        "confident_total = 0\n",
        "confident_correct = 0\n",
        "\n",
        "for i in range(len(corpus.test)):\n",
        "  #print(corpus.test[i])\n",
        "  \n",
        "\n",
        "  #get predictions\n",
        "  #print(gdp_test_df['title_desc'].iloc[i])\n",
        "  sentence = Sentence(gdp_test_df['title_desc'].iloc[i])\n",
        "\n",
        "  finetuned_classifier.predict(sentence)\n",
        "\n",
        "  #print(sentence.labels)\n",
        "\n",
        "  #get gold label\n",
        "  #print(corpus.test[i].labels)\n",
        "\n",
        "  #calculate correct guesses\n",
        "  if sentence.labels[0].score > 0.7:\n",
        "    confident_total += 1\n",
        "    if sentence.labels[0].value == corpus.test[i].labels[0].value:\n",
        "      confident_correct += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBIUuPVupWc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbfe0453-6453-4346-e0cf-62bab02a5255"
      },
      "source": [
        "confident_correct"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uaIXLevpj-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0ea9676-5938-40d2-fa78-3ea64599163f"
      },
      "source": [
        "confident_total"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnQz3cTGpmcS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9c2e508-85e4-4278-c68b-de7e45c11883"
      },
      "source": [
        "confident_correct/confident_total ## higher accuracy, pick only confident predictions for good visualization! also for correlation calculation?"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8260869565217391"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNda2ah3prLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## slightly better accuracy on train-only oversampling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8Cjmilbb6uY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## try XL-net"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}