{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Initial flair training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybjXa4UJ3g7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVY0ZiGk38i_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f717f084-9640-4de1-dc6c-70c48ae22136"
      },
      "source": [
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/flairNLP/flair.git\n",
            "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-gdgaj6pn\n",
            "  Running command git clone -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-gdgaj6pn\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.5.10)\n",
            "Requirement already satisfied, skipping upgrade: transformers>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2.9.1)\n",
            "Requirement already satisfied, skipping upgrade: langdetect in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (5.4.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.2.10)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.8.7)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.5.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair==0.4.5) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (8.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (0.1.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.8.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (20.3)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (0.13.1)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.5) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5) (0.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.6.0->flair==0.4.5) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.4.5) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair==0.4.5) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (1.13.10)\n",
            "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.17.0,>=1.16.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (1.16.10)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.10->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.15.2)\n",
            "Building wheels for collected packages: flair\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.4.5-cp36-none-any.whl size=148505 sha256=de8f8acddd6a187c403f45e87a8ae50aa436c4008131c08305cb3ba0c1772eb0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dl7yk0qi/wheels/84/82/73/d2b3b59b7be74ea05f2c6d64132efe27df52daffb47d1dc7bb\n",
            "Successfully built flair\n",
            "Installing collected packages: flair\n",
            "  Found existing installation: flair 0.4.5\n",
            "    Uninstalling flair-0.4.5:\n",
            "      Successfully uninstalled flair-0.4.5\n",
            "Successfully installed flair-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP7gl75D39dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings, DocumentRNNEmbeddings, BertEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from pathlib import Path\n",
        "from flair.datasets import CSVClassificationCorpus\n",
        "from flair.data import Corpus\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfpsjw8I4Si-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VBdZ1L34XwO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a34bb22f-7788-4af4-d56a-3490b707f40e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U90p69Xd5KIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = \"./drive/My Drive/capstone/data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkUisuXz7tuK",
        "colab_type": "text"
      },
      "source": [
        "## Train on benchmark dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-_Bmpa6gn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark = pd.read_csv(data_folder + \"all-data.csv\",encoding = \"ISO-8859-1\", header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVaSsyAn72AU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7838f8a3-43c7-44bd-8f79-b849a454836a"
      },
      "source": [
        "benchmark.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0                                                  1\n",
              "0   neutral  According to Gran , the company has no plans t...\n",
              "1   neutral  Technopolis plans to develop in stages an area...\n",
              "2  negative  The international electronic industry company ...\n",
              "3  positive  With the new production plant the company woul...\n",
              "4  positive  According to the company 's updated strategy f..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOidIWSy75C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark = benchmark[[0, 1]].rename(columns={0:\"label\", 1:\"text\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5dAal-w7-xO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_label(label):\n",
        "  'convert label from text to -1, 0, and 1.'\n",
        "    \n",
        "  if label == 'neutral':\n",
        "      return 0\n",
        "  elif label == 'negative':\n",
        "      return -1\n",
        "  else:\n",
        "      return 1\n",
        "\n",
        "benchmark['label'] = benchmark['label'].apply(convert_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DmpujRg8MPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "aa090a66-d10a-466c-ffd9-45883546a4ae"
      },
      "source": [
        "benchmark.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0  According to Gran , the company has no plans t...\n",
              "1      0  Technopolis plans to develop in stages an area...\n",
              "2     -1  The international electronic industry company ...\n",
              "3      1  With the new production plant the company woul...\n",
              "4      1  According to the company 's updated strategy f..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hLbF9Uk8UxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9e578780-14c8-4147-aa6f-3c71dab39bf5"
      },
      "source": [
        "#benchmark['label'] = '__label__' + benchmark['label'].astype(str)\n",
        "#benchmark.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>__label__0</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>__label__0</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>__label__-1</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>__label__1</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>__label__1</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                               text\n",
              "0   __label__0  According to Gran , the company has no plans t...\n",
              "1   __label__0  Technopolis plans to develop in stages an area...\n",
              "2  __label__-1  The international electronic industry company ...\n",
              "3   __label__1  With the new production plant the company woul...\n",
              "4   __label__1  According to the company 's updated strategy f..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t_MVMXq8pvf",
        "colab_type": "text"
      },
      "source": [
        "#### Create train, dev and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUbbfcSC8ekr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark.iloc[0:int(len(benchmark)*0.8)].to_csv(data_folder + 'train.csv', sep='\\t', index = False, header = False)\n",
        "benchmark.iloc[int(len(benchmark)*0.8):int(len(benchmark)*0.9)].to_csv(data_folder + 'test.csv', sep='\\t', index = False, header = False)\n",
        "benchmark.iloc[int(len(benchmark)*0.9):].to_csv(data_folder + 'dev.csv', sep='\\t', index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7QE3IS69txr",
        "colab_type": "text"
      },
      "source": [
        "#### Build corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz-2gNy58wDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "008820bc-d348-4509-e7d4-cb194650fbf3"
      },
      "source": [
        "# corpus = NLPTaskDataFetcher.load_classification_corpus(Path(data_folder), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
        "column_name_map = {1: \"text\", 0: \"label_topic\"}\n",
        "\n",
        "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
        "                                         column_name_map,\n",
        "                                         skip_header=True, #no header in kaggle data\n",
        "                                         delimiter='\\t',    # comma separated rows\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-21 16:26:20,848 Reading data from drive/My Drive/capstone/data\n",
            "2020-05-21 16:26:20,850 Train: drive/My Drive/capstone/data/train.csv\n",
            "2020-05-21 16:26:20,851 Dev: drive/My Drive/capstone/data/dev.csv\n",
            "2020-05-21 16:26:20,853 Test: drive/My Drive/capstone/data/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb1lynSQ98-z",
        "colab_type": "text"
      },
      "source": [
        "#### Create word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u9PFUze9_5y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7b721bd0-12e8-40db-b21d-da6a04def3fc"
      },
      "source": [
        "word_embeddings = [BertEmbeddings(), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wik8DDxm-DKy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83e5a2a7-f1e7-49a7-bc3f-8dc7ab2e1c02"
      },
      "source": [
        "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "trainer.train(data_folder, max_epochs=10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-21 16:29:35,472 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4359/4359 [00:03<00:00, 1112.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-21 16:29:39,766 [b'0', b'-1', b'1']\n",
            "2020-05-21 16:29:39,788 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:29:39,794 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentLSTMEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): BertEmbeddings(\n",
            "        (model): BertModel(\n",
            "          (embeddings): BertEmbeddings(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoder(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (list_embedding_1): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.25, inplace=False)\n",
            "          (encoder): Embedding(275, 100)\n",
            "          (rnn): LSTM(100, 1024)\n",
            "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (list_embedding_2): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.25, inplace=False)\n",
            "          (encoder): Embedding(275, 100)\n",
            "          (rnn): LSTM(100, 1024)\n",
            "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=5120, out_features=256, bias=True)\n",
            "    (rnn): GRU(256, 512)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-05-21 16:29:39,800 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:29:39,802 Corpus: \"Corpus: 3875 train + 484 dev + 484 test sentences\"\n",
            "2020-05-21 16:29:39,804 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:29:39,806 Parameters:\n",
            "2020-05-21 16:29:39,807  - learning_rate: \"0.1\"\n",
            "2020-05-21 16:29:39,809  - mini_batch_size: \"32\"\n",
            "2020-05-21 16:29:39,811  - patience: \"3\"\n",
            "2020-05-21 16:29:39,812  - anneal_factor: \"0.5\"\n",
            "2020-05-21 16:29:39,814  - max_epochs: \"10\"\n",
            "2020-05-21 16:29:39,816  - shuffle: \"True\"\n",
            "2020-05-21 16:29:39,818  - train_with_dev: \"False\"\n",
            "2020-05-21 16:29:39,819  - batch_growth_annealing: \"False\"\n",
            "2020-05-21 16:29:39,821 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:29:39,823 Model training base path: \"drive/My Drive/capstone/data\"\n",
            "2020-05-21 16:29:39,824 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:29:39,827 Device: cuda:0\n",
            "2020-05-21 16:29:39,828 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:29:39,830 Embeddings storage mode: cpu\n",
            "2020-05-21 16:29:39,851 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-21 16:29:50,593 epoch 1 - iter 12/122 - loss 1.09119564 - samples/sec: 38.15\n",
            "2020-05-21 16:30:14,205 epoch 1 - iter 24/122 - loss 0.97279288 - samples/sec: 38.31\n",
            "2020-05-21 16:30:38,825 epoch 1 - iter 36/122 - loss 0.91534476 - samples/sec: 35.48\n",
            "2020-05-21 16:31:03,061 epoch 1 - iter 48/122 - loss 0.89804776 - samples/sec: 36.38\n",
            "2020-05-21 16:31:26,959 epoch 1 - iter 60/122 - loss 0.87090534 - samples/sec: 37.57\n",
            "2020-05-21 16:31:51,281 epoch 1 - iter 72/122 - loss 0.83535755 - samples/sec: 36.86\n",
            "2020-05-21 16:32:15,061 epoch 1 - iter 84/122 - loss 0.81772747 - samples/sec: 38.12\n",
            "2020-05-21 16:32:38,666 epoch 1 - iter 96/122 - loss 0.80579187 - samples/sec: 38.81\n",
            "2020-05-21 16:33:02,459 epoch 1 - iter 108/122 - loss 0.78620375 - samples/sec: 37.99\n",
            "2020-05-21 16:33:26,034 epoch 1 - iter 120/122 - loss 0.77099588 - samples/sec: 38.60\n",
            "2020-05-21 16:33:40,910 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:33:40,912 EPOCH 1 done: loss 0.7647 - lr 0.1000000\n",
            "2020-05-21 16:33:54,599 DEV : loss 2.529153823852539 - score 0.4573\n",
            "2020-05-21 16:33:54,984 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-21 16:33:57,081 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:34:08,221 epoch 2 - iter 12/122 - loss 0.72595409 - samples/sec: 35.72\n",
            "2020-05-21 16:34:32,426 epoch 2 - iter 24/122 - loss 0.70496794 - samples/sec: 38.81\n",
            "2020-05-21 16:34:56,750 epoch 2 - iter 36/122 - loss 0.67516031 - samples/sec: 37.17\n",
            "2020-05-21 16:35:21,009 epoch 2 - iter 48/122 - loss 0.65274719 - samples/sec: 38.15\n",
            "2020-05-21 16:35:45,560 epoch 2 - iter 60/122 - loss 0.64360269 - samples/sec: 36.45\n",
            "2020-05-21 16:36:09,748 epoch 2 - iter 72/122 - loss 0.64316558 - samples/sec: 38.81\n",
            "2020-05-21 16:36:33,812 epoch 2 - iter 84/122 - loss 0.64310581 - samples/sec: 38.20\n",
            "2020-05-21 16:36:57,979 epoch 2 - iter 96/122 - loss 0.63342750 - samples/sec: 39.38\n",
            "2020-05-21 16:37:22,196 epoch 2 - iter 108/122 - loss 0.63072278 - samples/sec: 37.58\n",
            "2020-05-21 16:37:46,677 epoch 2 - iter 120/122 - loss 0.62294082 - samples/sec: 36.98\n",
            "2020-05-21 16:38:02,049 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:38:02,060 EPOCH 2 done: loss 0.6208 - lr 0.1000000\n",
            "2020-05-21 16:38:15,580 DEV : loss 3.1829891204833984 - score 0.4614\n",
            "2020-05-21 16:38:15,966 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-21 16:38:18,073 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:38:29,503 epoch 3 - iter 12/122 - loss 0.58463517 - samples/sec: 36.02\n",
            "2020-05-21 16:38:53,642 epoch 3 - iter 24/122 - loss 0.58255493 - samples/sec: 39.01\n",
            "2020-05-21 16:39:17,920 epoch 3 - iter 36/122 - loss 0.55384365 - samples/sec: 37.76\n",
            "2020-05-21 16:39:42,357 epoch 3 - iter 48/122 - loss 0.59384931 - samples/sec: 37.48\n",
            "2020-05-21 16:40:06,357 epoch 3 - iter 60/122 - loss 0.58687549 - samples/sec: 38.79\n",
            "2020-05-21 16:40:30,891 epoch 3 - iter 72/122 - loss 0.58886292 - samples/sec: 37.24\n",
            "2020-05-21 16:40:55,121 epoch 3 - iter 84/122 - loss 0.58038466 - samples/sec: 37.96\n",
            "2020-05-21 16:41:20,064 epoch 3 - iter 96/122 - loss 0.58874306 - samples/sec: 36.27\n",
            "2020-05-21 16:41:44,134 epoch 3 - iter 108/122 - loss 0.58167136 - samples/sec: 38.37\n",
            "2020-05-21 16:42:07,966 epoch 3 - iter 120/122 - loss 0.58005416 - samples/sec: 40.70\n",
            "2020-05-21 16:42:23,074 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:42:23,080 EPOCH 3 done: loss 0.5782 - lr 0.1000000\n",
            "2020-05-21 16:42:36,601 DEV : loss 3.4412078857421875 - score 0.4601\n",
            "2020-05-21 16:42:36,999 BAD EPOCHS (no improvement): 1\n",
            "2020-05-21 16:42:37,006 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:42:47,377 epoch 4 - iter 12/122 - loss 0.53962262 - samples/sec: 39.64\n",
            "2020-05-21 16:43:11,840 epoch 4 - iter 24/122 - loss 0.52920142 - samples/sec: 36.76\n",
            "2020-05-21 16:43:36,367 epoch 4 - iter 36/122 - loss 0.54239694 - samples/sec: 38.04\n",
            "2020-05-21 16:44:01,351 epoch 4 - iter 48/122 - loss 0.55065964 - samples/sec: 35.90\n",
            "2020-05-21 16:44:26,086 epoch 4 - iter 60/122 - loss 0.54383565 - samples/sec: 36.12\n",
            "2020-05-21 16:44:50,179 epoch 4 - iter 72/122 - loss 0.53965773 - samples/sec: 38.66\n",
            "2020-05-21 16:45:14,300 epoch 4 - iter 84/122 - loss 0.55005360 - samples/sec: 39.07\n",
            "2020-05-21 16:45:38,593 epoch 4 - iter 96/122 - loss 0.54060782 - samples/sec: 37.81\n",
            "2020-05-21 16:46:03,067 epoch 4 - iter 108/122 - loss 0.54096571 - samples/sec: 38.19\n",
            "2020-05-21 16:46:26,936 epoch 4 - iter 120/122 - loss 0.54516127 - samples/sec: 38.83\n",
            "2020-05-21 16:46:42,238 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:46:42,245 EPOCH 4 done: loss 0.5436 - lr 0.1000000\n",
            "2020-05-21 16:46:55,488 DEV : loss 2.397822618484497 - score 0.4931\n",
            "2020-05-21 16:46:55,895 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-21 16:46:58,020 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:47:09,153 epoch 5 - iter 12/122 - loss 0.53050374 - samples/sec: 35.79\n",
            "2020-05-21 16:47:33,456 epoch 5 - iter 24/122 - loss 0.51990946 - samples/sec: 37.76\n",
            "2020-05-21 16:47:58,552 epoch 5 - iter 36/122 - loss 0.51089662 - samples/sec: 35.61\n",
            "2020-05-21 16:48:22,137 epoch 5 - iter 48/122 - loss 0.52580919 - samples/sec: 40.09\n",
            "2020-05-21 16:48:47,354 epoch 5 - iter 60/122 - loss 0.51551390 - samples/sec: 35.59\n",
            "2020-05-21 16:49:11,390 epoch 5 - iter 72/122 - loss 0.50933392 - samples/sec: 39.35\n",
            "2020-05-21 16:49:35,597 epoch 5 - iter 84/122 - loss 0.51547963 - samples/sec: 37.64\n",
            "2020-05-21 16:50:00,038 epoch 5 - iter 96/122 - loss 0.51495154 - samples/sec: 37.96\n",
            "2020-05-21 16:50:24,337 epoch 5 - iter 108/122 - loss 0.50986412 - samples/sec: 38.36\n",
            "2020-05-21 16:50:48,083 epoch 5 - iter 120/122 - loss 0.51842824 - samples/sec: 39.81\n",
            "2020-05-21 16:51:03,517 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:51:03,519 EPOCH 5 done: loss 0.5132 - lr 0.1000000\n",
            "2020-05-21 16:51:17,211 DEV : loss 4.3872294425964355 - score 0.4656\n",
            "2020-05-21 16:51:17,596 BAD EPOCHS (no improvement): 1\n",
            "2020-05-21 16:51:17,601 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:51:28,513 epoch 6 - iter 12/122 - loss 0.58686733 - samples/sec: 37.67\n",
            "2020-05-21 16:51:52,777 epoch 6 - iter 24/122 - loss 0.56179280 - samples/sec: 38.29\n",
            "2020-05-21 16:52:16,810 epoch 6 - iter 36/122 - loss 0.53840048 - samples/sec: 38.28\n",
            "2020-05-21 16:52:41,719 epoch 6 - iter 48/122 - loss 0.50567033 - samples/sec: 35.95\n",
            "2020-05-21 16:53:05,647 epoch 6 - iter 60/122 - loss 0.50945099 - samples/sec: 40.02\n",
            "2020-05-21 16:53:29,561 epoch 6 - iter 72/122 - loss 0.50944635 - samples/sec: 39.16\n",
            "2020-05-21 16:53:53,555 epoch 6 - iter 84/122 - loss 0.49587498 - samples/sec: 38.92\n",
            "2020-05-21 16:54:18,334 epoch 6 - iter 96/122 - loss 0.50472958 - samples/sec: 36.73\n",
            "2020-05-21 16:54:42,846 epoch 6 - iter 108/122 - loss 0.50345630 - samples/sec: 36.85\n",
            "2020-05-21 16:55:07,070 epoch 6 - iter 120/122 - loss 0.49768067 - samples/sec: 38.65\n",
            "2020-05-21 16:55:22,277 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:55:22,279 EPOCH 6 done: loss 0.5030 - lr 0.1000000\n",
            "2020-05-21 16:55:35,713 DEV : loss 0.838103175163269 - score 0.8278\n",
            "2020-05-21 16:55:36,105 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-21 16:55:38,233 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:55:49,767 epoch 7 - iter 12/122 - loss 0.53162228 - samples/sec: 35.51\n",
            "2020-05-21 16:56:14,405 epoch 7 - iter 24/122 - loss 0.53441648 - samples/sec: 37.57\n",
            "2020-05-21 16:56:38,400 epoch 7 - iter 36/122 - loss 0.49410732 - samples/sec: 38.92\n",
            "2020-05-21 16:57:02,642 epoch 7 - iter 48/122 - loss 0.50839427 - samples/sec: 38.59\n",
            "2020-05-21 16:57:26,855 epoch 7 - iter 60/122 - loss 0.49031883 - samples/sec: 37.95\n",
            "2020-05-21 16:57:51,658 epoch 7 - iter 72/122 - loss 0.49133255 - samples/sec: 36.97\n",
            "2020-05-21 16:58:15,805 epoch 7 - iter 84/122 - loss 0.48199746 - samples/sec: 38.99\n",
            "2020-05-21 16:58:40,098 epoch 7 - iter 96/122 - loss 0.49097232 - samples/sec: 37.67\n",
            "2020-05-21 16:59:04,450 epoch 7 - iter 108/122 - loss 0.49096394 - samples/sec: 38.71\n",
            "2020-05-21 16:59:28,547 epoch 7 - iter 120/122 - loss 0.48240288 - samples/sec: 38.36\n",
            "2020-05-21 16:59:44,108 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 16:59:44,113 EPOCH 7 done: loss 0.4849 - lr 0.1000000\n",
            "2020-05-21 16:59:57,585 DEV : loss 2.6835856437683105 - score 0.4959\n",
            "2020-05-21 16:59:57,967 BAD EPOCHS (no improvement): 1\n",
            "2020-05-21 16:59:57,977 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 17:00:08,305 epoch 8 - iter 12/122 - loss 0.45628495 - samples/sec: 38.85\n",
            "2020-05-21 17:00:32,546 epoch 8 - iter 24/122 - loss 0.44235220 - samples/sec: 37.80\n",
            "2020-05-21 17:00:57,125 epoch 8 - iter 36/122 - loss 0.43522993 - samples/sec: 37.39\n",
            "2020-05-21 17:01:21,697 epoch 8 - iter 48/122 - loss 0.44626013 - samples/sec: 37.31\n",
            "2020-05-21 17:01:45,987 epoch 8 - iter 60/122 - loss 0.43738766 - samples/sec: 38.93\n",
            "2020-05-21 17:02:10,319 epoch 8 - iter 72/122 - loss 0.43461946 - samples/sec: 37.66\n",
            "2020-05-21 17:02:34,451 epoch 8 - iter 84/122 - loss 0.44325628 - samples/sec: 38.68\n",
            "2020-05-21 17:02:58,921 epoch 8 - iter 96/122 - loss 0.44537294 - samples/sec: 37.41\n",
            "2020-05-21 17:03:23,054 epoch 8 - iter 108/122 - loss 0.45251539 - samples/sec: 37.94\n",
            "2020-05-21 17:03:47,511 epoch 8 - iter 120/122 - loss 0.44689582 - samples/sec: 36.68\n",
            "2020-05-21 17:04:02,848 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 17:04:02,850 EPOCH 8 done: loss 0.4452 - lr 0.1000000\n",
            "2020-05-21 17:04:17,574 DEV : loss 2.2200350761413574 - score 0.5317\n",
            "2020-05-21 17:04:18,141 BAD EPOCHS (no improvement): 2\n",
            "2020-05-21 17:04:18,286 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 17:04:28,608 epoch 9 - iter 12/122 - loss 0.40900066 - samples/sec: 38.80\n",
            "2020-05-21 17:04:53,189 epoch 9 - iter 24/122 - loss 0.42059592 - samples/sec: 37.62\n",
            "2020-05-21 17:05:18,110 epoch 9 - iter 36/122 - loss 0.40174897 - samples/sec: 35.83\n",
            "2020-05-21 17:05:42,424 epoch 9 - iter 48/122 - loss 0.43987238 - samples/sec: 37.64\n",
            "2020-05-21 17:06:06,260 epoch 9 - iter 60/122 - loss 0.44420176 - samples/sec: 40.01\n",
            "2020-05-21 17:06:30,463 epoch 9 - iter 72/122 - loss 0.43552556 - samples/sec: 38.37\n",
            "2020-05-21 17:06:54,760 epoch 9 - iter 84/122 - loss 0.44591622 - samples/sec: 37.72\n",
            "2020-05-21 17:07:19,554 epoch 9 - iter 96/122 - loss 0.43903321 - samples/sec: 37.08\n",
            "2020-05-21 17:07:43,830 epoch 9 - iter 108/122 - loss 0.43683020 - samples/sec: 38.49\n",
            "2020-05-21 17:08:08,168 epoch 9 - iter 120/122 - loss 0.43283624 - samples/sec: 37.90\n",
            "2020-05-21 17:08:23,563 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 17:08:23,565 EPOCH 9 done: loss 0.4306 - lr 0.1000000\n",
            "2020-05-21 17:08:36,915 DEV : loss 3.7298941612243652 - score 0.4848\n",
            "2020-05-21 17:08:37,316 BAD EPOCHS (no improvement): 3\n",
            "2020-05-21 17:08:37,324 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 17:08:48,056 epoch 10 - iter 12/122 - loss 0.43597335 - samples/sec: 38.04\n",
            "2020-05-21 17:09:12,367 epoch 10 - iter 24/122 - loss 0.45056421 - samples/sec: 38.04\n",
            "2020-05-21 17:09:36,695 epoch 10 - iter 36/122 - loss 0.40717999 - samples/sec: 37.56\n",
            "2020-05-21 17:10:01,021 epoch 10 - iter 48/122 - loss 0.39227999 - samples/sec: 38.16\n",
            "2020-05-21 17:10:25,454 epoch 10 - iter 60/122 - loss 0.40978403 - samples/sec: 36.96\n",
            "2020-05-21 17:10:49,119 epoch 10 - iter 72/122 - loss 0.39414817 - samples/sec: 40.26\n",
            "2020-05-21 17:11:13,221 epoch 10 - iter 84/122 - loss 0.40436783 - samples/sec: 38.75\n",
            "2020-05-21 17:11:37,752 epoch 10 - iter 96/122 - loss 0.40853052 - samples/sec: 37.40\n",
            "2020-05-21 17:12:02,272 epoch 10 - iter 108/122 - loss 0.42139218 - samples/sec: 36.52\n",
            "2020-05-21 17:12:26,074 epoch 10 - iter 120/122 - loss 0.42404910 - samples/sec: 40.05\n",
            "2020-05-21 17:12:41,347 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 17:12:41,351 EPOCH 10 done: loss 0.4298 - lr 0.1000000\n",
            "2020-05-21 17:12:54,717 DEV : loss 4.844223499298096 - score 0.4353\n",
            "Epoch    10: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2020-05-21 17:12:55,080 BAD EPOCHS (no improvement): 4\n",
            "2020-05-21 17:12:57,154 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-21 17:12:57,160 Testing using best model ...\n",
            "2020-05-21 17:12:57,168 loading file drive/My Drive/capstone/data/best-model.pt\n",
            "2020-05-21 17:13:10,543 0.518595041322314\t0.518595041322314\t0.518595041322314\n",
            "2020-05-21 17:13:10,551 \n",
            "MICRO_AVG: acc 0.6790633608815427 - f1-score 0.518595041322314\n",
            "MACRO_AVG: acc 0.6790633608815426 - f1-score 0.4403770130350528\n",
            "-1         tp: 149 - fp: 217 - fn: 6 - tn: 112 - precision: 0.4071 - recall: 0.9613 - accuracy: 0.5393 - f1-score: 0.5720\n",
            "0          tp: 96 - fp: 4 - fn: 207 - tn: 177 - precision: 0.9600 - recall: 0.3168 - accuracy: 0.5640 - f1-score: 0.4764\n",
            "1          tp: 6 - fp: 12 - fn: 20 - tn: 446 - precision: 0.3333 - recall: 0.2308 - accuracy: 0.9339 - f1-score: 0.2727\n",
            "2020-05-21 17:13:10,556 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [2.529153823852539,\n",
              "  3.1829891204833984,\n",
              "  3.4412078857421875,\n",
              "  2.397822618484497,\n",
              "  4.3872294425964355,\n",
              "  0.838103175163269,\n",
              "  2.6835856437683105,\n",
              "  2.2200350761413574,\n",
              "  3.7298941612243652,\n",
              "  4.844223499298096],\n",
              " 'dev_score_history': [0.4573002754820937,\n",
              "  0.46143250688705234,\n",
              "  0.46005509641873277,\n",
              "  0.4931129476584022,\n",
              "  0.465564738292011,\n",
              "  0.8278236914600551,\n",
              "  0.49586776859504134,\n",
              "  0.5316804407713499,\n",
              "  0.48484848484848486,\n",
              "  0.43526170798898073],\n",
              " 'test_score': 0.6790633608815427,\n",
              " 'train_loss_history': [0.7646573711125577,\n",
              "  0.620830442328922,\n",
              "  0.5782061121502861,\n",
              "  0.5436141591091626,\n",
              "  0.5132139423770494,\n",
              "  0.5029887211371641,\n",
              "  0.48485090681275383,\n",
              "  0.44522049644442857,\n",
              "  0.4305564264049295,\n",
              "  0.4298317696227402]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E2JikhOEZFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}