{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Two_stage_flair_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybjXa4UJ3g7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVY0ZiGk38i_",
        "colab_type": "code",
        "outputId": "e62a361d-adfe-493e-c7ca-47cb53221dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/flairNLP/flair.git\n",
            "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-jglogkum\n",
            "  Running command git clone -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-jglogkum\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers>=2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 14.6MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.1.2)\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.6.0)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/f9/9f2b6c672c8f8bb87a4c1bd52c1b57213627b035305aad745d015b2a62ae/pytest-5.4.2-py3-none-any.whl (247kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.8.7)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.5.0+cu101)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 53.4MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (4.41.1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (1.18.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (3.0.12)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (8.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.8.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (20.4)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (0.1.9)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.5) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.6.0->flair==0.4.5) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.4.5) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (1.13.13)\n",
            "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair==0.4.5) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (1.16.13)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.15.2)\n",
            "Building wheels for collected packages: flair\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.4.5-cp36-none-any.whl size=148505 sha256=bb3d7605d349e1c56e20c26b9e3171568b7dbb697782c90ebcc826d16f37c25a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a7kh_mgq/wheels/84/82/73/d2b3b59b7be74ea05f2c6d64132efe27df52daffb47d1dc7bb\n",
            "Successfully built flair\n",
            "Building wheels for collected packages: sqlitedict, mpld3, segtok, langdetect, sacremoses\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=e4cca3f904172ede6bc01af18ff5a7d63af5141b5b810d0c900a368f7f5bb6c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=a077fa3683d3393f56254ec033ecbe5382fddead74a84a0f1317ab1a520aba9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25020 sha256=4b1d01643a8c55252151fa7221979566670b5b4f0d46f129432cde4d774c94f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=f78e1361b8b7d564acf914023c6fcbefead2728fef0a310fe0a8a7090b93b7fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=022a514618745355cb9095abe8333e7bcd4e46ea9e157138d91d8daedbd242a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sqlitedict mpld3 segtok langdetect sacremoses\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, sqlitedict, bpemb, pluggy, pytest, deprecated, mpld3, segtok, langdetect, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed bpemb-0.3.0 deprecated-1.2.10 flair-0.4.5 langdetect-1.0.8 mpld3-0.3 pluggy-0.13.1 pytest-5.4.2 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.91 sqlitedict-1.6.0 tokenizers-0.7.0 transformers-2.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP7gl75D39dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings, DocumentRNNEmbeddings, BertEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from pathlib import Path\n",
        "from flair.datasets import CSVClassificationCorpus\n",
        "from flair.data import Corpus\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VBdZ1L34XwO",
        "colab_type": "code",
        "outputId": "8963e107-f730-4ee7-a29e-873ea2b955e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm4Y_PUUwePN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U90p69Xd5KIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = \"/content/drive/My Drive/Capstone/data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkUisuXz7tuK",
        "colab_type": "text"
      },
      "source": [
        "### First Stage (Train on benchmark dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-_Bmpa6gn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark = pd.read_csv(data_folder + \"combined_benchmark.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVaSsyAn72AU",
        "colab_type": "code",
        "outputId": "ac651847-5809-4958-b289-9cd8b46cda9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "benchmark = benchmark[['label', 'text']]\n",
        "benchmark.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Why not subscribe to the magazine ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>Tornio Works employs 2,300 of whom more than 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>The move is aimed at boosting sales , cost-eff...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>As a result of the merger , the largest profes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>18 March 2010 A leakage in the gypsum pond was...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0                Why not subscribe to the magazine ?\n",
              "1     -1  Tornio Works employs 2,300 of whom more than 1...\n",
              "2      1  The move is aimed at boosting sales , cost-eff...\n",
              "3      0  As a result of the merger , the largest profes...\n",
              "4     -1  18 March 2010 A leakage in the gypsum pond was..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t_MVMXq8pvf",
        "colab_type": "text"
      },
      "source": [
        "#### Create train, dev and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUbbfcSC8ekr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark = benchmark.sample(frac=1)\n",
        "benchmark.iloc[0:int(len(benchmark)*0.8)].to_csv(data_folder + 'train.csv', sep='\\t', index = False, header = False)\n",
        "benchmark.iloc[int(len(benchmark)*0.8):int(len(benchmark)*0.9)].to_csv(data_folder + 'test.csv', sep='\\t', index = False, header = False)\n",
        "benchmark.iloc[int(len(benchmark)*0.9):].to_csv(data_folder + 'dev.csv', sep='\\t', index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFvvbnacsxJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WMydYBXszIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kaggle_X_train, kaggle_X_te, kaggle_Y_train, kaggle_Y_te = train_test_split(benchmark['text'],benchmark['label'], test_size = 0.2, random_state = 0, stratify = benchmark['label'])\n",
        "kaggle_X_dev, kaggle_X_test, kaggle_Y_dev, kaggle_Y_test = train_test_split(kaggle_X_te,kaggle_Y_te, test_size = 0.5, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXPW_syOwjgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhMvAKu3s2My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark_train_df =  pd.DataFrame({ 'label': kaggle_Y_train, 'text': kaggle_X_train   })\n",
        "benchmark_dev_df = pd.DataFrame({ 'label': kaggle_Y_dev , 'text': kaggle_X_dev  })\n",
        "benchmark_test_df = pd.DataFrame({ 'label': kaggle_Y_test, 'text': kaggle_X_test   })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mabSa5mrs4Iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark_train_df.to_csv( data_folder + \"train.csv\", index = False, header = False)\n",
        "benchmark_dev_df.to_csv( data_folder + \"dev.csv\", index = False, header = False)\n",
        "benchmark_test_df.to_csv( data_folder + \"test.csv\", index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D-hQy9HvIzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7QE3IS69txr",
        "colab_type": "text"
      },
      "source": [
        "#### Build corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz-2gNy58wDI",
        "colab_type": "code",
        "outputId": "47247523-fce6-4313-8751-47583d86360e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# corpus = NLPTaskDataFetcher.load_classification_corpus(Path(data_folder), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
        "column_name_map = {1: \"text\", 0: \"label_topic\"}\n",
        "\n",
        "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
        "                                         column_name_map,\n",
        "                                         skip_header=False, #no header in kaggle data\n",
        "                                         delimiter='\\t',    # comma separated rows\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-22 00:27:43,337 Reading data from /content/drive/My Drive/Capstone/data\n",
            "2020-05-22 00:27:43,340 Train: /content/drive/My Drive/Capstone/data/train.csv\n",
            "2020-05-22 00:27:43,345 Dev: /content/drive/My Drive/Capstone/data/dev.csv\n",
            "2020-05-22 00:27:43,347 Test: /content/drive/My Drive/Capstone/data/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb1lynSQ98-z",
        "colab_type": "text"
      },
      "source": [
        "#### Create word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u9PFUze9_5y",
        "colab_type": "code",
        "outputId": "02efb363-ce15-4606-a6da-35f54b0ecd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "word_embeddings = [BertEmbeddings(), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HovCqFfSMjmJ",
        "colab_type": "text"
      },
      "source": [
        "#### First Stage Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wik8DDxm-DKy",
        "colab_type": "code",
        "outputId": "930d530f-1a8b-4fda-f04e-e5d3b40aa20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "trainer.train(data_folder, max_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-22 00:28:38,684 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1314/1314 [00:01<00:00, 805.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-22 00:28:40,610 [b'0', b'1', b'-1']\n",
            "2020-05-22 00:28:40,623 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:28:40,627 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentLSTMEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): BertEmbeddings(\n",
            "        (model): BertModel(\n",
            "          (embeddings): BertEmbeddings(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoder(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (list_embedding_1): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.25, inplace=False)\n",
            "          (encoder): Embedding(275, 100)\n",
            "          (rnn): LSTM(100, 1024)\n",
            "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (list_embedding_2): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.25, inplace=False)\n",
            "          (encoder): Embedding(275, 100)\n",
            "          (rnn): LSTM(100, 1024)\n",
            "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=5120, out_features=256, bias=True)\n",
            "    (rnn): GRU(256, 512)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-05-22 00:28:40,632 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:28:40,633 Corpus: \"Corpus: 1168 train + 147 dev + 146 test sentences\"\n",
            "2020-05-22 00:28:40,634 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:28:40,635 Parameters:\n",
            "2020-05-22 00:28:40,635  - learning_rate: \"0.1\"\n",
            "2020-05-22 00:28:40,636  - mini_batch_size: \"32\"\n",
            "2020-05-22 00:28:40,638  - patience: \"3\"\n",
            "2020-05-22 00:28:40,639  - anneal_factor: \"0.5\"\n",
            "2020-05-22 00:28:40,640  - max_epochs: \"10\"\n",
            "2020-05-22 00:28:40,641  - shuffle: \"True\"\n",
            "2020-05-22 00:28:40,642  - train_with_dev: \"False\"\n",
            "2020-05-22 00:28:40,644  - batch_growth_annealing: \"False\"\n",
            "2020-05-22 00:28:40,646 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:28:40,647 Model training base path: \"/content/drive/My Drive/Capstone/data\"\n",
            "2020-05-22 00:28:40,649 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:28:40,651 Device: cuda:0\n",
            "2020-05-22 00:28:40,652 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:28:40,653 Embeddings storage mode: cpu\n",
            "2020-05-22 00:28:40,665 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-22 00:28:42,774 epoch 1 - iter 3/37 - loss 1.47190928 - samples/sec: 53.79\n",
            "2020-05-22 00:28:52,919 epoch 1 - iter 6/37 - loss 1.48636297 - samples/sec: 59.70\n",
            "2020-05-22 00:29:03,053 epoch 1 - iter 9/37 - loss 1.42424431 - samples/sec: 62.67\n",
            "2020-05-22 00:29:13,600 epoch 1 - iter 12/37 - loss 1.33435683 - samples/sec: 64.34\n",
            "2020-05-22 00:29:23,750 epoch 1 - iter 15/37 - loss 1.26441330 - samples/sec: 63.38\n",
            "2020-05-22 00:29:33,755 epoch 1 - iter 18/37 - loss 1.22004982 - samples/sec: 65.24\n",
            "2020-05-22 00:29:44,029 epoch 1 - iter 21/37 - loss 1.21731525 - samples/sec: 61.67\n",
            "2020-05-22 00:29:53,974 epoch 1 - iter 24/37 - loss 1.23220348 - samples/sec: 67.34\n",
            "2020-05-22 00:30:04,015 epoch 1 - iter 27/37 - loss 1.20821805 - samples/sec: 67.07\n",
            "2020-05-22 00:30:14,026 epoch 1 - iter 30/37 - loss 1.17964252 - samples/sec: 64.34\n",
            "2020-05-22 00:30:24,257 epoch 1 - iter 33/37 - loss 1.15531065 - samples/sec: 61.37\n",
            "2020-05-22 00:30:34,405 epoch 1 - iter 36/37 - loss 1.15251064 - samples/sec: 63.47\n",
            "2020-05-22 00:30:43,599 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:30:43,601 EPOCH 1 done: loss 1.1510 - lr 0.1000000\n",
            "2020-05-22 00:30:46,045 DEV : loss 1.2375831604003906 - score 0.5918\n",
            "2020-05-22 00:30:46,175 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 00:30:48,095 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:30:50,224 epoch 2 - iter 3/37 - loss 1.10517786 - samples/sec: 56.93\n",
            "2020-05-22 00:31:01,996 epoch 2 - iter 6/37 - loss 1.00401107 - samples/sec: 58.81\n",
            "2020-05-22 00:31:12,019 epoch 2 - iter 9/37 - loss 1.00575615 - samples/sec: 67.59\n",
            "2020-05-22 00:31:21,963 epoch 2 - iter 12/37 - loss 1.01196276 - samples/sec: 67.74\n",
            "2020-05-22 00:31:32,244 epoch 2 - iter 15/37 - loss 0.98417585 - samples/sec: 60.69\n",
            "2020-05-22 00:31:42,347 epoch 2 - iter 18/37 - loss 0.95005316 - samples/sec: 63.80\n",
            "2020-05-22 00:31:53,521 epoch 2 - iter 21/37 - loss 0.93894888 - samples/sec: 57.57\n",
            "2020-05-22 00:32:03,899 epoch 2 - iter 24/37 - loss 0.97889507 - samples/sec: 62.64\n",
            "2020-05-22 00:32:13,798 epoch 2 - iter 27/37 - loss 0.98422565 - samples/sec: 69.81\n",
            "2020-05-22 00:32:23,649 epoch 2 - iter 30/37 - loss 0.97077200 - samples/sec: 72.64\n",
            "2020-05-22 00:32:33,747 epoch 2 - iter 33/37 - loss 0.96553474 - samples/sec: 61.15\n",
            "2020-05-22 00:32:43,790 epoch 2 - iter 36/37 - loss 0.95369236 - samples/sec: 66.92\n",
            "2020-05-22 00:32:52,903 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:32:52,904 EPOCH 2 done: loss 0.9459 - lr 0.1000000\n",
            "2020-05-22 00:32:55,365 DEV : loss 0.9462193846702576 - score 0.6825\n",
            "2020-05-22 00:32:55,484 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 00:32:57,438 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:32:59,724 epoch 3 - iter 3/37 - loss 0.76595666 - samples/sec: 60.76\n",
            "2020-05-22 00:33:11,349 epoch 3 - iter 6/37 - loss 0.78023536 - samples/sec: 64.10\n",
            "2020-05-22 00:33:21,460 epoch 3 - iter 9/37 - loss 0.85095540 - samples/sec: 69.15\n",
            "2020-05-22 00:33:31,540 epoch 3 - iter 12/37 - loss 0.81648877 - samples/sec: 62.24\n",
            "2020-05-22 00:33:41,817 epoch 3 - iter 15/37 - loss 0.79638615 - samples/sec: 61.71\n",
            "2020-05-22 00:33:52,048 epoch 3 - iter 18/37 - loss 0.80616916 - samples/sec: 64.25\n",
            "2020-05-22 00:34:02,148 epoch 3 - iter 21/37 - loss 0.78819947 - samples/sec: 66.45\n",
            "2020-05-22 00:34:12,421 epoch 3 - iter 24/37 - loss 0.78204037 - samples/sec: 62.67\n",
            "2020-05-22 00:34:22,705 epoch 3 - iter 27/37 - loss 0.81026023 - samples/sec: 64.69\n",
            "2020-05-22 00:34:32,730 epoch 3 - iter 30/37 - loss 0.81233717 - samples/sec: 64.51\n",
            "2020-05-22 00:34:42,927 epoch 3 - iter 33/37 - loss 0.82131972 - samples/sec: 68.69\n",
            "2020-05-22 00:34:52,984 epoch 3 - iter 36/37 - loss 0.80998603 - samples/sec: 66.17\n",
            "2020-05-22 00:35:02,053 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:35:02,054 EPOCH 3 done: loss 0.8122 - lr 0.1000000\n",
            "2020-05-22 00:35:04,791 DEV : loss 0.9772723317146301 - score 0.6508\n",
            "2020-05-22 00:35:04,911 BAD EPOCHS (no improvement): 1\n",
            "2020-05-22 00:35:04,915 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:35:06,897 epoch 4 - iter 3/37 - loss 0.73647503 - samples/sec: 57.99\n",
            "2020-05-22 00:35:17,068 epoch 4 - iter 6/37 - loss 0.69445882 - samples/sec: 64.60\n",
            "2020-05-22 00:35:27,427 epoch 4 - iter 9/37 - loss 0.66673577 - samples/sec: 63.18\n",
            "2020-05-22 00:35:37,605 epoch 4 - iter 12/37 - loss 0.71696045 - samples/sec: 66.46\n",
            "2020-05-22 00:35:47,719 epoch 4 - iter 15/37 - loss 0.73464040 - samples/sec: 64.54\n",
            "2020-05-22 00:35:57,949 epoch 4 - iter 18/37 - loss 0.78240563 - samples/sec: 59.28\n",
            "2020-05-22 00:36:08,018 epoch 4 - iter 21/37 - loss 0.76978057 - samples/sec: 70.96\n",
            "2020-05-22 00:36:18,128 epoch 4 - iter 24/37 - loss 0.77100183 - samples/sec: 65.82\n",
            "2020-05-22 00:36:28,256 epoch 4 - iter 27/37 - loss 0.77131453 - samples/sec: 64.73\n",
            "2020-05-22 00:36:38,249 epoch 4 - iter 30/37 - loss 0.76383892 - samples/sec: 69.86\n",
            "2020-05-22 00:36:48,317 epoch 4 - iter 33/37 - loss 0.75202953 - samples/sec: 70.15\n",
            "2020-05-22 00:36:58,901 epoch 4 - iter 36/37 - loss 0.75411987 - samples/sec: 65.51\n",
            "2020-05-22 00:37:08,499 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:37:08,500 EPOCH 4 done: loss 0.7517 - lr 0.1000000\n",
            "2020-05-22 00:37:10,973 DEV : loss 0.8877629637718201 - score 0.7506\n",
            "2020-05-22 00:37:11,101 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 00:37:13,064 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:37:15,186 epoch 5 - iter 3/37 - loss 1.02723285 - samples/sec: 65.75\n",
            "2020-05-22 00:37:26,906 epoch 5 - iter 6/37 - loss 0.86904939 - samples/sec: 68.99\n",
            "2020-05-22 00:37:37,085 epoch 5 - iter 9/37 - loss 0.82852126 - samples/sec: 62.08\n",
            "2020-05-22 00:37:47,302 epoch 5 - iter 12/37 - loss 0.78070698 - samples/sec: 65.37\n",
            "2020-05-22 00:37:57,684 epoch 5 - iter 15/37 - loss 0.79275288 - samples/sec: 61.89\n",
            "2020-05-22 00:38:07,747 epoch 5 - iter 18/37 - loss 0.84622479 - samples/sec: 65.94\n",
            "2020-05-22 00:38:17,903 epoch 5 - iter 21/37 - loss 0.83093162 - samples/sec: 62.41\n",
            "2020-05-22 00:38:27,992 epoch 5 - iter 24/37 - loss 0.79928239 - samples/sec: 64.52\n",
            "2020-05-22 00:38:38,232 epoch 5 - iter 27/37 - loss 0.77259026 - samples/sec: 64.52\n",
            "2020-05-22 00:38:48,315 epoch 5 - iter 30/37 - loss 0.75177813 - samples/sec: 64.80\n",
            "2020-05-22 00:38:58,381 epoch 5 - iter 33/37 - loss 0.73742028 - samples/sec: 68.34\n",
            "2020-05-22 00:39:08,451 epoch 5 - iter 36/37 - loss 0.72905740 - samples/sec: 66.19\n",
            "2020-05-22 00:39:17,649 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:39:17,653 EPOCH 5 done: loss 0.7436 - lr 0.1000000\n",
            "2020-05-22 00:39:20,291 DEV : loss 1.028766393661499 - score 0.6961\n",
            "2020-05-22 00:39:20,421 BAD EPOCHS (no improvement): 1\n",
            "2020-05-22 00:39:20,426 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:39:22,389 epoch 6 - iter 3/37 - loss 0.71856501 - samples/sec: 58.05\n",
            "2020-05-22 00:39:32,400 epoch 6 - iter 6/37 - loss 0.63861203 - samples/sec: 65.14\n",
            "2020-05-22 00:39:42,411 epoch 6 - iter 9/37 - loss 0.65292016 - samples/sec: 65.98\n",
            "2020-05-22 00:39:52,659 epoch 6 - iter 12/37 - loss 0.63235377 - samples/sec: 64.91\n",
            "2020-05-22 00:40:02,730 epoch 6 - iter 15/37 - loss 0.68291334 - samples/sec: 66.73\n",
            "2020-05-22 00:40:12,955 epoch 6 - iter 18/37 - loss 0.69776568 - samples/sec: 61.17\n",
            "2020-05-22 00:40:23,268 epoch 6 - iter 21/37 - loss 0.69036843 - samples/sec: 64.22\n",
            "2020-05-22 00:40:33,259 epoch 6 - iter 24/37 - loss 0.70958960 - samples/sec: 65.28\n",
            "2020-05-22 00:40:43,518 epoch 6 - iter 27/37 - loss 0.71094829 - samples/sec: 66.17\n",
            "2020-05-22 00:40:53,601 epoch 6 - iter 30/37 - loss 0.74041819 - samples/sec: 64.78\n",
            "2020-05-22 00:41:03,508 epoch 6 - iter 33/37 - loss 0.73237062 - samples/sec: 69.62\n",
            "2020-05-22 00:41:13,701 epoch 6 - iter 36/37 - loss 0.72543525 - samples/sec: 65.26\n",
            "2020-05-22 00:41:22,856 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:41:22,857 EPOCH 6 done: loss 0.7226 - lr 0.1000000\n",
            "2020-05-22 00:41:25,349 DEV : loss 0.7877547144889832 - score 0.7506\n",
            "2020-05-22 00:41:25,470 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 00:41:27,410 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:41:29,369 epoch 7 - iter 3/37 - loss 0.70845089 - samples/sec: 62.11\n",
            "2020-05-22 00:41:41,032 epoch 7 - iter 6/37 - loss 0.66023472 - samples/sec: 49.77\n",
            "2020-05-22 00:41:51,185 epoch 7 - iter 9/37 - loss 0.61241963 - samples/sec: 63.15\n",
            "2020-05-22 00:42:01,193 epoch 7 - iter 12/37 - loss 0.61922744 - samples/sec: 63.21\n",
            "2020-05-22 00:42:11,967 epoch 7 - iter 15/37 - loss 0.66987626 - samples/sec: 65.28\n",
            "2020-05-22 00:42:22,355 epoch 7 - iter 18/37 - loss 0.67236908 - samples/sec: 61.55\n",
            "2020-05-22 00:42:32,481 epoch 7 - iter 21/37 - loss 0.65863030 - samples/sec: 64.64\n",
            "2020-05-22 00:42:42,655 epoch 7 - iter 24/37 - loss 0.66767500 - samples/sec: 62.93\n",
            "2020-05-22 00:42:52,855 epoch 7 - iter 27/37 - loss 0.65936151 - samples/sec: 61.56\n",
            "2020-05-22 00:43:03,101 epoch 7 - iter 30/37 - loss 0.69008875 - samples/sec: 58.45\n",
            "2020-05-22 00:43:13,115 epoch 7 - iter 33/37 - loss 0.70465889 - samples/sec: 64.58\n",
            "2020-05-22 00:43:22,990 epoch 7 - iter 36/37 - loss 0.70314911 - samples/sec: 70.96\n",
            "2020-05-22 00:43:32,076 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:43:32,080 EPOCH 7 done: loss 0.6998 - lr 0.1000000\n",
            "2020-05-22 00:43:34,585 DEV : loss 1.1813462972640991 - score 0.7098\n",
            "2020-05-22 00:43:34,708 BAD EPOCHS (no improvement): 1\n",
            "2020-05-22 00:43:34,713 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:43:37,011 epoch 8 - iter 3/37 - loss 0.65076057 - samples/sec: 58.04\n",
            "2020-05-22 00:43:47,170 epoch 8 - iter 6/37 - loss 0.52995312 - samples/sec: 66.34\n",
            "2020-05-22 00:43:57,279 epoch 8 - iter 9/37 - loss 0.52950460 - samples/sec: 63.10\n",
            "2020-05-22 00:44:07,476 epoch 8 - iter 12/37 - loss 0.53269508 - samples/sec: 61.07\n",
            "2020-05-22 00:44:17,721 epoch 8 - iter 15/37 - loss 0.54847997 - samples/sec: 59.49\n",
            "2020-05-22 00:44:27,774 epoch 8 - iter 18/37 - loss 0.58328570 - samples/sec: 63.35\n",
            "2020-05-22 00:44:37,937 epoch 8 - iter 21/37 - loss 0.62127760 - samples/sec: 62.50\n",
            "2020-05-22 00:44:48,191 epoch 8 - iter 24/37 - loss 0.61346795 - samples/sec: 62.34\n",
            "2020-05-22 00:44:58,364 epoch 8 - iter 27/37 - loss 0.63329995 - samples/sec: 71.64\n",
            "2020-05-22 00:45:08,735 epoch 8 - iter 30/37 - loss 0.62927956 - samples/sec: 64.08\n",
            "2020-05-22 00:45:18,955 epoch 8 - iter 33/37 - loss 0.62293789 - samples/sec: 64.62\n",
            "2020-05-22 00:45:29,016 epoch 8 - iter 36/37 - loss 0.61464858 - samples/sec: 66.09\n",
            "2020-05-22 00:45:38,168 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:45:38,169 EPOCH 8 done: loss 0.6084 - lr 0.1000000\n",
            "2020-05-22 00:45:40,873 DEV : loss 0.9026913642883301 - score 0.7732\n",
            "2020-05-22 00:45:40,997 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 00:45:43,007 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:45:45,011 epoch 9 - iter 3/37 - loss 0.71821594 - samples/sec: 57.32\n",
            "2020-05-22 00:45:56,978 epoch 9 - iter 6/37 - loss 0.60259252 - samples/sec: 65.14\n",
            "2020-05-22 00:46:09,705 epoch 9 - iter 9/37 - loss 0.60026504 - samples/sec: 66.03\n",
            "2020-05-22 00:46:22,616 epoch 9 - iter 12/37 - loss 0.58111617 - samples/sec: 60.26\n",
            "2020-05-22 00:46:35,510 epoch 9 - iter 15/37 - loss 0.56525840 - samples/sec: 66.16\n",
            "2020-05-22 00:46:48,398 epoch 9 - iter 18/37 - loss 0.61346471 - samples/sec: 65.43\n",
            "2020-05-22 00:47:01,363 epoch 9 - iter 21/37 - loss 0.59466037 - samples/sec: 64.22\n",
            "2020-05-22 00:47:14,394 epoch 9 - iter 24/37 - loss 0.60037046 - samples/sec: 57.26\n",
            "2020-05-22 00:47:27,406 epoch 9 - iter 27/37 - loss 0.59575170 - samples/sec: 70.10\n",
            "2020-05-22 00:47:40,345 epoch 9 - iter 30/37 - loss 0.62612170 - samples/sec: 64.56\n",
            "2020-05-22 00:47:53,199 epoch 9 - iter 33/37 - loss 0.62687283 - samples/sec: 65.99\n",
            "2020-05-22 00:48:06,365 epoch 9 - iter 36/37 - loss 0.61867512 - samples/sec: 57.39\n",
            "2020-05-22 00:48:18,239 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:48:18,240 EPOCH 9 done: loss 0.6112 - lr 0.1000000\n",
            "2020-05-22 00:48:20,677 DEV : loss 0.7203583717346191 - score 0.7687\n",
            "2020-05-22 00:48:20,811 BAD EPOCHS (no improvement): 1\n",
            "2020-05-22 00:48:20,815 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:48:22,709 epoch 10 - iter 3/37 - loss 0.72649026 - samples/sec: 62.84\n",
            "2020-05-22 00:48:35,771 epoch 10 - iter 6/37 - loss 0.63209823 - samples/sec: 61.37\n",
            "2020-05-22 00:48:48,774 epoch 10 - iter 9/37 - loss 0.68187443 - samples/sec: 66.19\n",
            "2020-05-22 00:49:01,537 epoch 10 - iter 12/37 - loss 0.64478938 - samples/sec: 66.62\n",
            "2020-05-22 00:49:14,338 epoch 10 - iter 15/37 - loss 0.60612730 - samples/sec: 70.78\n",
            "2020-05-22 00:49:27,201 epoch 10 - iter 18/37 - loss 0.60470709 - samples/sec: 63.83\n",
            "2020-05-22 00:49:40,232 epoch 10 - iter 21/37 - loss 0.59384167 - samples/sec: 61.05\n",
            "2020-05-22 00:49:53,141 epoch 10 - iter 24/37 - loss 0.58482362 - samples/sec: 64.69\n",
            "2020-05-22 00:50:06,038 epoch 10 - iter 27/37 - loss 0.58210304 - samples/sec: 67.29\n",
            "2020-05-22 00:50:18,844 epoch 10 - iter 30/37 - loss 0.59282182 - samples/sec: 65.18\n",
            "2020-05-22 00:50:31,871 epoch 10 - iter 33/37 - loss 0.57724328 - samples/sec: 66.53\n",
            "2020-05-22 00:50:44,751 epoch 10 - iter 36/37 - loss 0.56769652 - samples/sec: 64.52\n",
            "2020-05-22 00:50:56,661 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:50:56,663 EPOCH 10 done: loss 0.5666 - lr 0.1000000\n",
            "2020-05-22 00:50:59,115 DEV : loss 0.8858896493911743 - score 0.737\n",
            "2020-05-22 00:50:59,232 BAD EPOCHS (no improvement): 2\n",
            "2020-05-22 00:51:01,090 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 00:51:01,094 Testing using best model ...\n",
            "2020-05-22 00:51:01,099 loading file /content/drive/My Drive/Capstone/data/best-model.pt\n",
            "2020-05-22 00:51:04,673 0.6917808219178082\t0.6917808219178082\t0.6917808219178082\n",
            "2020-05-22 00:51:04,678 \n",
            "MICRO_AVG: acc 0.7945205479452054 - f1-score 0.6917808219178082\n",
            "MACRO_AVG: acc 0.7945205479452054 - f1-score 0.6201620247482661\n",
            "-1         tp: 46 - fp: 24 - fn: 2 - tn: 74 - precision: 0.6571 - recall: 0.9583 - accuracy: 0.8219 - f1-score: 0.7797\n",
            "0          tp: 47 - fp: 19 - fn: 6 - tn: 74 - precision: 0.7121 - recall: 0.8868 - accuracy: 0.8288 - f1-score: 0.7899\n",
            "1          tp: 8 - fp: 2 - fn: 37 - tn: 99 - precision: 0.8000 - recall: 0.1778 - accuracy: 0.7329 - f1-score: 0.2909\n",
            "2020-05-22 00:51:04,680 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [1.2375831604003906,\n",
              "  0.9462193846702576,\n",
              "  0.9772723317146301,\n",
              "  0.8877629637718201,\n",
              "  1.028766393661499,\n",
              "  0.7877547144889832,\n",
              "  1.1813462972640991,\n",
              "  0.9026913642883301,\n",
              "  0.7203583717346191,\n",
              "  0.8858896493911743],\n",
              " 'dev_score_history': [0.5918367346938775,\n",
              "  0.6825396825396826,\n",
              "  0.6507936507936508,\n",
              "  0.7505668934240363,\n",
              "  0.6961451247165533,\n",
              "  0.7505668934240363,\n",
              "  0.7097505668934241,\n",
              "  0.7732426303854876,\n",
              "  0.7687074829931972,\n",
              "  0.7369614512471655],\n",
              " 'test_score': 0.7945205479452054,\n",
              " 'train_loss_history': [1.1509819352948987,\n",
              "  0.945921003818512,\n",
              "  0.8122080242311632,\n",
              "  0.751708149104505,\n",
              "  0.743634190108325,\n",
              "  0.7226126612843694,\n",
              "  0.6997873477033667,\n",
              "  0.6083846905746976,\n",
              "  0.6112446994394869,\n",
              "  0.5666100415023597]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E2JikhOEZFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5maM-7gQWqJ",
        "colab_type": "text"
      },
      "source": [
        "### Second Stage (train on hand annotated datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbrXhSRlnCR3",
        "colab_type": "text"
      },
      "source": [
        "#### Build corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS7RONnU1uQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data_folder = '/content/drive/My Drive/Capstone/second_stage_data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWROK9BK3tAp",
        "colab_type": "code",
        "outputId": "6c1aba83-6bbb-41ec-b7c3-4d8d558b2dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "gdp_data = pd.read_csv('/content/drive/My Drive/Capstone/second_stage_data/gdp_df_oversampled.csv')\n",
        "gdp_data.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>description</th>\n",
              "      <th>publishedAt</th>\n",
              "      <th>title</th>\n",
              "      <th>title_desc_sent_1</th>\n",
              "      <th>title_desc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>In New York, Dow Jones industrial average up 4...</td>\n",
              "      <td>2019-11-27</td>\n",
              "      <td>North American markets reach new record highs ...</td>\n",
              "      <td>1</td>\n",
              "      <td>North American markets reach new record highs ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>107</td>\n",
              "      <td>Canadian gross domestic product grew at the fa...</td>\n",
              "      <td>2018-12-21</td>\n",
              "      <td>Canada's GDP grows at fastest pace in 5 months...</td>\n",
              "      <td>1</td>\n",
              "      <td>Canada's GDP grows at fastest pace in 5 months...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>123</td>\n",
              "      <td>U.S. President Donald Trump predicted data on ...</td>\n",
              "      <td>2018-07-27</td>\n",
              "      <td>Trump predicts data will show U.S. economy in ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Trump predicts data will show U.S. economy in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80</td>\n",
              "      <td>Canadian firms among many investing billions i...</td>\n",
              "      <td>2019-07-17</td>\n",
              "      <td>Companies look to cash in on out-of-this-world...</td>\n",
              "      <td>1</td>\n",
              "      <td>Companies look to cash in on out-of-this-world...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>61</td>\n",
              "      <td>That’s despite new revenues including 3% tax o...</td>\n",
              "      <td>2019-09-30</td>\n",
              "      <td>Liberals promises new spending and four more y...</td>\n",
              "      <td>1</td>\n",
              "      <td>Liberals promises new spending and four more y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         title_desc\n",
              "0          50  ...  North American markets reach new record highs ...\n",
              "1         107  ...  Canada's GDP grows at fastest pace in 5 months...\n",
              "2         123  ...  Trump predicts data will show U.S. economy in ...\n",
              "3          80  ...  Companies look to cash in on out-of-this-world...\n",
              "4          61  ...  Liberals promises new spending and four more y...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UrHJ-ZtKiHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4c7q_y74H2k",
        "colab_type": "code",
        "outputId": "0d4b2df6-9419-49d6-dab4-e75237a0d6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "new_df = gdp_data[['title_desc_sent_1', 'title_desc']]\n",
        "new_df.columns= ['label', 'text']\n",
        "new_df.tail()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>0</td>\n",
              "      <td>Vancouver needs 10,000 affordable housing unit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0</td>\n",
              "      <td>Lessons from the Great Crash of 1929 for inves...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0</td>\n",
              "      <td>Two more months of traffic pain until CN finis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>0</td>\n",
              "      <td>Fed signals patience on rate moves ‘for some t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0</td>\n",
              "      <td>With little to say on coronavirus, is Quebec's...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                               text\n",
              "160      0  Vancouver needs 10,000 affordable housing unit...\n",
              "161      0  Lessons from the Great Crash of 1929 for inves...\n",
              "162      0  Two more months of traffic pain until CN finis...\n",
              "163      0  Fed signals patience on rate moves ‘for some t...\n",
              "164      0  With little to say on coronavirus, is Quebec's..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSuqQHZw4fQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L05DOTq4Lrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kaggle_X_train, kaggle_X_te, kaggle_Y_train, kaggle_Y_te = train_test_split(new_df['text'],new_df['label'], test_size = 0.2, random_state = 0, stratify = new_df['label'])\n",
        "kaggle_X_dev, kaggle_X_test, kaggle_Y_dev, kaggle_Y_test = train_test_split(kaggle_X_te,kaggle_Y_te, test_size = 0.5, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN7YR-3j4PoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark_train_df =  pd.DataFrame({ 'label': kaggle_Y_train, 'text': kaggle_X_train   })\n",
        "benchmark_dev_df = pd.DataFrame({ 'label': kaggle_Y_dev , 'text': kaggle_X_dev  })\n",
        "benchmark_test_df = pd.DataFrame({ 'label': kaggle_Y_test, 'text': kaggle_X_test   })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq1TM7Qr4T9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benchmark_train_df.to_csv( new_data_folder + \"train.csv\",  sep='\\t', index = False, header = False)\n",
        "benchmark_dev_df.to_csv( new_data_folder + \"dev.csv\",sep='\\t', index = False, header = False)\n",
        "benchmark_test_df.to_csv( new_data_folder + \"test.csv\", sep='\\t',index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBqALIs571Km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7993cf91-392a-40c6-b844-3a33c9f095c0"
      },
      "source": [
        "x = pd.read_csv('/content/drive/My Drive/Capstone/second_stage_data/train.csv', delimiter = '\\t')\n",
        "x.head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>U.S. economy grew at unrevised 4.2% pace in second quarter. The U.S. economy grew in the second quarter at an unrevised 4.2 per cent pace, the fastest since late 2014, indicating a solid foundation for this quarter, Commerce Department data showed Thursday.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>Alberta government focuses on high debt, not i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Statistics are great unless they measure the w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Vancouver needs 10,000 affordable housing unit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>U.S. fourth-quarter GDP revisions show weaker ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>The Toronto Raptors' NBA playoff run helped bo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   1 U.S. economy grew at unrevised 4.2% pace in second quarter. The U.S. economy grew in the second quarter at an unrevised 4.2 per cent pace, the fastest since late 2014, indicating a solid foundation for this quarter, Commerce Department data showed Thursday.\n",
              "0 -1  Alberta government focuses on high debt, not i...                                                                                                                                                                                                               \n",
              "1  0  Statistics are great unless they measure the w...                                                                                                                                                                                                               \n",
              "2  0  Vancouver needs 10,000 affordable housing unit...                                                                                                                                                                                                               \n",
              "3 -1  U.S. fourth-quarter GDP revisions show weaker ...                                                                                                                                                                                                               \n",
              "4  1  The Toronto Raptors' NBA playoff run helped bo...                                                                                                                                                                                                               "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0E6iEy9UgYP",
        "colab_type": "code",
        "outputId": "e7c73281-409a-4ba3-b35f-306ccc4c1ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "new_column_name_map = {1: \"text\", 0: \"label_topic\"}\n",
        "\n",
        "corpus: Corpus = CSVClassificationCorpus(new_data_folder,\n",
        "                                         new_column_name_map,\n",
        "                                         skip_header=False, #no header in kaggle data\n",
        "                                         delimiter='\\t',    # comma separated rows\n",
        ")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-22 01:12:34,392 Reading data from /content/drive/My Drive/Capstone/second_stage_data\n",
            "2020-05-22 01:12:34,394 Train: /content/drive/My Drive/Capstone/second_stage_data/train.csv\n",
            "2020-05-22 01:12:34,395 Dev: /content/drive/My Drive/Capstone/second_stage_data/dev.csv\n",
            "2020-05-22 01:12:34,397 Test: /content/drive/My Drive/Capstone/second_stage_data/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAGDir71nHb6",
        "colab_type": "text"
      },
      "source": [
        "#### Second Stage fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWg4fjqNYMKa",
        "colab_type": "code",
        "outputId": "65141636-428d-4b1d-8fda-b1342aa139e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# benchmark_embeddings = FlairEmbeddings(data_folder + 'best-model.pt')\n",
        "benchmark_classifier = TextClassifier.load(data_folder + 'best-model.pt')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-22 01:12:39,079 loading file /content/drive/My Drive/Capstone/data/best-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr1XWYH6QZGl",
        "colab_type": "code",
        "outputId": "d3f5e8c9-ee8b-438e-9a40-da43eb2704ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer = ModelTrainer(benchmark_classifier, corpus)\n",
        "trainer.train(new_data_folder, max_epochs=10)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-22 01:12:53,841 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:12:53,849 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentLSTMEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): BertEmbeddings(\n",
            "        (model): BertModel(\n",
            "          (embeddings): BertEmbeddings(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoder(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (list_embedding_1): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.25, inplace=False)\n",
            "          (encoder): Embedding(275, 100)\n",
            "          (rnn): LSTM(100, 1024)\n",
            "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (list_embedding_2): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.25, inplace=False)\n",
            "          (encoder): Embedding(275, 100)\n",
            "          (rnn): LSTM(100, 1024)\n",
            "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=5120, out_features=256, bias=True)\n",
            "    (rnn): GRU(256, 512)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-05-22 01:12:53,854 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:12:53,862 Corpus: \"Corpus: 132 train + 16 dev + 17 test sentences\"\n",
            "2020-05-22 01:12:53,866 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:12:53,868 Parameters:\n",
            "2020-05-22 01:12:53,870  - learning_rate: \"0.1\"\n",
            "2020-05-22 01:12:53,872  - mini_batch_size: \"32\"\n",
            "2020-05-22 01:12:53,873  - patience: \"3\"\n",
            "2020-05-22 01:12:53,875  - anneal_factor: \"0.5\"\n",
            "2020-05-22 01:12:53,877  - max_epochs: \"10\"\n",
            "2020-05-22 01:12:53,879  - shuffle: \"True\"\n",
            "2020-05-22 01:12:53,881  - train_with_dev: \"False\"\n",
            "2020-05-22 01:12:53,883  - batch_growth_annealing: \"False\"\n",
            "2020-05-22 01:12:53,885 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:12:53,889 Model training base path: \"/content/drive/My Drive/Capstone/second_stage_data\"\n",
            "2020-05-22 01:12:53,891 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:12:53,895 Device: cuda:0\n",
            "2020-05-22 01:12:53,897 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:12:53,900 Embeddings storage mode: cpu\n",
            "2020-05-22 01:12:53,910 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:12:54,784 epoch 1 - iter 1/5 - loss 1.85720742 - samples/sec: 54.04\n",
            "2020-05-22 01:13:03,827 epoch 1 - iter 2/5 - loss 1.37348250 - samples/sec: 58.31\n",
            "2020-05-22 01:13:13,637 epoch 1 - iter 3/5 - loss 1.18820713 - samples/sec: 51.42\n",
            "2020-05-22 01:13:22,675 epoch 1 - iter 4/5 - loss 1.08049597 - samples/sec: 59.50\n",
            "2020-05-22 01:13:31,082 epoch 1 - iter 5/5 - loss 1.12558705 - samples/sec: 273.54\n",
            "2020-05-22 01:13:39,492 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:13:39,496 EPOCH 1 done: loss 1.1256 - lr 0.1000000\n",
            "2020-05-22 01:13:40,243 DEV : loss 1.1998101472854614 - score 0.6667\n",
            "2020-05-22 01:13:40,269 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 01:13:42,267 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:13:43,169 epoch 2 - iter 1/5 - loss 0.80696166 - samples/sec: 57.25\n",
            "2020-05-22 01:13:52,685 epoch 2 - iter 2/5 - loss 0.87262508 - samples/sec: 35.05\n",
            "2020-05-22 01:14:02,604 epoch 2 - iter 3/5 - loss 0.76993314 - samples/sec: 60.56\n",
            "2020-05-22 01:14:11,566 epoch 2 - iter 4/5 - loss 0.79949616 - samples/sec: 55.54\n",
            "2020-05-22 01:14:19,976 epoch 2 - iter 5/5 - loss 0.84260443 - samples/sec: 255.27\n",
            "2020-05-22 01:14:28,356 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:14:28,361 EPOCH 2 done: loss 0.8426 - lr 0.1000000\n",
            "2020-05-22 01:14:29,167 DEV : loss 1.0167970657348633 - score 0.625\n",
            "2020-05-22 01:14:29,193 BAD EPOCHS (no improvement): 1\n",
            "2020-05-22 01:14:29,197 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:14:30,030 epoch 3 - iter 1/5 - loss 0.61314821 - samples/sec: 55.85\n",
            "2020-05-22 01:14:38,921 epoch 3 - iter 2/5 - loss 0.79544631 - samples/sec: 59.25\n",
            "2020-05-22 01:14:47,867 epoch 3 - iter 3/5 - loss 0.93004173 - samples/sec: 56.40\n",
            "2020-05-22 01:14:56,740 epoch 3 - iter 4/5 - loss 0.88969074 - samples/sec: 65.56\n",
            "2020-05-22 01:15:05,122 epoch 3 - iter 5/5 - loss 0.77675801 - samples/sec: 304.40\n",
            "2020-05-22 01:15:13,733 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:15:13,739 EPOCH 3 done: loss 0.7768 - lr 0.1000000\n",
            "2020-05-22 01:15:14,445 DEV : loss 1.59540855884552 - score 0.6667\n",
            "2020-05-22 01:15:14,469 BAD EPOCHS (no improvement): 2\n",
            "2020-05-22 01:15:14,474 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:15:15,313 epoch 4 - iter 1/5 - loss 1.57566381 - samples/sec: 58.53\n",
            "2020-05-22 01:15:24,212 epoch 4 - iter 2/5 - loss 1.38973719 - samples/sec: 58.04\n",
            "2020-05-22 01:15:33,229 epoch 4 - iter 3/5 - loss 1.28014274 - samples/sec: 58.65\n",
            "2020-05-22 01:15:42,160 epoch 4 - iter 4/5 - loss 1.10771954 - samples/sec: 59.91\n",
            "2020-05-22 01:15:50,542 epoch 4 - iter 5/5 - loss 0.95583116 - samples/sec: 284.14\n",
            "2020-05-22 01:15:58,952 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:15:58,957 EPOCH 4 done: loss 0.9558 - lr 0.1000000\n",
            "2020-05-22 01:16:00,238 DEV : loss 1.1989225149154663 - score 0.7083\n",
            "2020-05-22 01:16:00,262 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 01:16:02,393 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:16:03,273 epoch 5 - iter 1/5 - loss 0.89570528 - samples/sec: 52.98\n",
            "2020-05-22 01:16:13,989 epoch 5 - iter 2/5 - loss 0.76728028 - samples/sec: 58.32\n",
            "2020-05-22 01:16:23,103 epoch 5 - iter 3/5 - loss 0.73659988 - samples/sec: 59.36\n",
            "2020-05-22 01:16:32,046 epoch 5 - iter 4/5 - loss 0.72280714 - samples/sec: 57.25\n",
            "2020-05-22 01:16:40,465 epoch 5 - iter 5/5 - loss 0.79000981 - samples/sec: 246.99\n",
            "2020-05-22 01:16:48,925 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:16:48,930 EPOCH 5 done: loss 0.7900 - lr 0.1000000\n",
            "2020-05-22 01:16:49,610 DEV : loss 1.1810652017593384 - score 0.7083\n",
            "2020-05-22 01:16:49,636 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 01:16:51,398 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:16:52,335 epoch 6 - iter 1/5 - loss 0.54105514 - samples/sec: 53.19\n",
            "2020-05-22 01:17:02,453 epoch 6 - iter 2/5 - loss 0.69662449 - samples/sec: 48.34\n",
            "2020-05-22 01:17:11,500 epoch 6 - iter 3/5 - loss 0.68078192 - samples/sec: 57.67\n",
            "2020-05-22 01:17:20,440 epoch 6 - iter 4/5 - loss 0.65842088 - samples/sec: 57.83\n",
            "2020-05-22 01:17:28,956 epoch 6 - iter 5/5 - loss 0.54273115 - samples/sec: 245.98\n",
            "2020-05-22 01:17:37,446 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:17:37,448 EPOCH 6 done: loss 0.5427 - lr 0.1000000\n",
            "2020-05-22 01:17:38,134 DEV : loss 0.7167848348617554 - score 0.7083\n",
            "2020-05-22 01:17:38,157 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 01:17:39,989 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:17:41,171 epoch 7 - iter 1/5 - loss 0.25711292 - samples/sec: 57.76\n",
            "2020-05-22 01:17:50,802 epoch 7 - iter 2/5 - loss 0.29124406 - samples/sec: 43.17\n",
            "2020-05-22 01:18:00,607 epoch 7 - iter 3/5 - loss 0.28820607 - samples/sec: 63.85\n",
            "2020-05-22 01:18:09,529 epoch 7 - iter 4/5 - loss 0.30612909 - samples/sec: 58.42\n",
            "2020-05-22 01:18:18,639 epoch 7 - iter 5/5 - loss 0.31362391 - samples/sec: 261.83\n",
            "2020-05-22 01:18:28,455 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:18:28,456 EPOCH 7 done: loss 0.3136 - lr 0.1000000\n",
            "2020-05-22 01:18:29,165 DEV : loss 1.9236620664596558 - score 0.625\n",
            "2020-05-22 01:18:29,193 BAD EPOCHS (no improvement): 1\n",
            "2020-05-22 01:18:29,197 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:18:30,000 epoch 8 - iter 1/5 - loss 0.45015770 - samples/sec: 58.13\n",
            "2020-05-22 01:18:38,700 epoch 8 - iter 2/5 - loss 0.42987204 - samples/sec: 60.11\n",
            "2020-05-22 01:18:47,658 epoch 8 - iter 3/5 - loss 0.40429660 - samples/sec: 55.47\n",
            "2020-05-22 01:18:56,511 epoch 8 - iter 4/5 - loss 0.38042458 - samples/sec: 57.28\n",
            "2020-05-22 01:19:05,119 epoch 8 - iter 5/5 - loss 0.35695274 - samples/sec: 261.53\n",
            "2020-05-22 01:19:13,459 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:19:13,463 EPOCH 8 done: loss 0.3570 - lr 0.1000000\n",
            "2020-05-22 01:19:14,152 DEV : loss 0.7530584931373596 - score 0.75\n",
            "2020-05-22 01:19:14,176 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 01:19:16,020 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:19:16,946 epoch 9 - iter 1/5 - loss 0.34474602 - samples/sec: 52.83\n",
            "2020-05-22 01:19:26,276 epoch 9 - iter 2/5 - loss 0.55958144 - samples/sec: 48.12\n",
            "2020-05-22 01:19:36,398 epoch 9 - iter 3/5 - loss 0.50551282 - samples/sec: 59.22\n",
            "2020-05-22 01:19:45,547 epoch 9 - iter 4/5 - loss 0.47735773 - samples/sec: 57.96\n",
            "2020-05-22 01:19:54,062 epoch 9 - iter 5/5 - loss 0.44070249 - samples/sec: 255.86\n",
            "2020-05-22 01:20:02,674 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:20:02,679 EPOCH 9 done: loss 0.4407 - lr 0.1000000\n",
            "2020-05-22 01:20:03,382 DEV : loss 1.4163832664489746 - score 0.75\n",
            "2020-05-22 01:20:03,405 BAD EPOCHS (no improvement): 1\n",
            "2020-05-22 01:20:03,410 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:20:04,187 epoch 10 - iter 1/5 - loss 0.61526614 - samples/sec: 61.93\n",
            "2020-05-22 01:20:12,983 epoch 10 - iter 2/5 - loss 0.69662061 - samples/sec: 57.73\n",
            "2020-05-22 01:20:22,004 epoch 10 - iter 3/5 - loss 0.52983112 - samples/sec: 59.23\n",
            "2020-05-22 01:20:30,899 epoch 10 - iter 4/5 - loss 0.43654189 - samples/sec: 56.83\n",
            "2020-05-22 01:20:39,439 epoch 10 - iter 5/5 - loss 0.35590793 - samples/sec: 210.27\n",
            "2020-05-22 01:20:48,163 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:20:48,168 EPOCH 10 done: loss 0.3559 - lr 0.1000000\n",
            "2020-05-22 01:20:48,880 DEV : loss 0.7204463481903076 - score 0.7917\n",
            "2020-05-22 01:20:48,907 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-05-22 01:20:52,730 ----------------------------------------------------------------------------------------------------\n",
            "2020-05-22 01:20:52,735 Testing using best model ...\n",
            "2020-05-22 01:20:52,742 loading file /content/drive/My Drive/Capstone/second_stage_data/best-model.pt\n",
            "2020-05-22 01:20:54,401 1.0\t1.0\t1.0\n",
            "2020-05-22 01:20:54,407 \n",
            "MICRO_AVG: acc 1.0 - f1-score 1.0\n",
            "MACRO_AVG: acc 1.0 - f1-score 1.0\n",
            "-1         tp: 5 - fp: 0 - fn: 0 - tn: 12 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
            "0          tp: 8 - fp: 0 - fn: 0 - tn: 9 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
            "1          tp: 4 - fp: 0 - fn: 0 - tn: 13 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
            "2020-05-22 01:20:54,411 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [1.1998101472854614,\n",
              "  1.0167970657348633,\n",
              "  1.59540855884552,\n",
              "  1.1989225149154663,\n",
              "  1.1810652017593384,\n",
              "  0.7167848348617554,\n",
              "  1.9236620664596558,\n",
              "  0.7530584931373596,\n",
              "  1.4163832664489746,\n",
              "  0.7204463481903076],\n",
              " 'dev_score_history': [0.6666666666666666,\n",
              "  0.625,\n",
              "  0.6666666666666666,\n",
              "  0.7083333333333334,\n",
              "  0.7083333333333334,\n",
              "  0.7083333333333334,\n",
              "  0.625,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.7916666666666666],\n",
              " 'test_score': 1.0,\n",
              " 'train_loss_history': [1.1255870461463928,\n",
              "  0.8426044344902038,\n",
              "  0.7767580091953278,\n",
              "  0.9558311581611634,\n",
              "  0.7900098085403442,\n",
              "  0.5427311539649964,\n",
              "  0.3136239111423492,\n",
              "  0.3569527447223663,\n",
              "  0.440702486038208,\n",
              "  0.35590792894363404]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gC2Q82mKjDY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b15d3a61-47bd-4eab-f5a7-c1f70f365dfe"
      },
      "source": [
        "final_classifier = TextClassifier.load(new_data_folder + 'final-model.pt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-22 02:17:46,944 loading file /content/drive/My Drive/Capstone/second_stage_data/final-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL8pNZGVLTLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "96a1a6c0-61f4-4356-91a1-ac270b623e7e"
      },
      "source": [
        "final_classifier.predict(\"Canada's economy shrinks in October from auto strike spillover. Canada’s economy contracted in October for the first time in eight months, as the United Auto Workers strike in the U.S. weighed on plant production\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"Canada's economy shrinks in October from auto strike spillover. Canada’s economy contracted in October for the first time in eight months, as the United Auto Workers strike in the U.S. weighed on plant production\"   [− Tokens: 34  − Sentence-Labels: {'class': [-1 (0.9992)]}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7t8ee-qLVPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "32b61885-a028-4a6a-84b2-925c55759ab2"
      },
      "source": [
        "final_classifier.predict(\"The candidates: Great Slave. Patrick Scott, Katrina Nokleby face off for seat vacated by former minister Glen Abernethy\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"The candidates: Great Slave. Patrick Scott, Katrina Nokleby face off for seat vacated by former minister Glen Abernethy\"   [− Tokens: 18  − Sentence-Labels: {'class': [0 (0.9757)]}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "195bXcMJLa13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d3d3bbc1-3f53-4dc5-d2a8-83f111f0b15a"
      },
      "source": [
        "final_classifier.predict(\"North American markets reach new record highs on stronger U.S. economic growth. In New York, Dow Jones industrial average up 42.32 points to 28,164\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"North American markets reach new record highs on stronger U.S. economic growth. In New York, Dow Jones industrial average up 42.32 points to 28,164\"   [− Tokens: 24  − Sentence-Labels: {'class': [1 (0.9907)]}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK-v_qGAKkjS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e768084c-e8f9-4aaf-8f84-91d879ca4c4c"
      },
      "source": [
        "final_classifier.predict(\"There's a lot of economic uncertainty right now as the world tries to curb the spread of COVID-19. But will the Bank of Canada's cut to interest rates help our economy? And how do events know when to pull the plug and cancel it all?\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence: \"There's a lot of economic uncertainty right now as the world tries to curb the spread of COVID-19. But will the Bank of Canada's cut to interest rates help our economy? And how do events know when to pull the plug and cancel it all?\"   [− Tokens: 45  − Sentence-Labels: {'class': [-1 (0.5661)]}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIaF9kOrLH-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}