{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7293,
     "status": "ok",
     "timestamp": 1590105375960,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "9WKl4yjQqlaG",
    "outputId": "a4bd53a5-4f03-4b03-d9b0-e43f9e388353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.4.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.0+cu101)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.10)\n",
      "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.8)\n",
      "Requirement already satisfied: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (5.4.2)\n",
      "Requirement already satisfied: transformers>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (2.9.1)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
      "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.10)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (1.18.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.12.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.4)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.1.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.6.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.13.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (8.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (19.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair) (0.0.43)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair) (2.23.0)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair) (0.7.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair) (0.7)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair) (0.1.91)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.15.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.10.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (1.13.13)\n",
      "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.6.0->flair) (7.1.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair) (2020.4.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (1.16.13)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1216,
     "status": "error",
     "timestamp": 1590105466518,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "ybjXa4UJ3g7a",
    "outputId": "cbf731c3-f8be-4cdd-830d-d14bae8923ed"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a4637bbefc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpython\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15191,
     "status": "ok",
     "timestamp": 1590105506011,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "NVY0ZiGk38i_",
    "outputId": "98db0be2-e654-4dc5-8f4b-4001f0f795b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/flairNLP/flair.git\n",
      "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-jntwyqx5\n",
      "  Running command git clone -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-jntwyqx5\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied, skipping upgrade: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: transformers>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2.9.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: langdetect in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.5.10)\n",
      "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.2.10)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.5.0+cu101)\n",
      "Requirement already satisfied, skipping upgrade: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (5.4.2)\n",
      "Requirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.7)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.6.0->flair==0.4.5) (0.1.91)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->flair==0.4.5) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.5) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair==0.4.5) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (8.3.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (2.4)\n",
      "Requirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (3.10.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.6.0->flair==0.4.5) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.6.0->flair==0.4.5) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (1.13.13)\n",
      "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair==0.4.5) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.4.5) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (1.16.13)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (0.15.2)\n",
      "Building wheels for collected packages: flair\n",
      "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flair: filename=flair-0.4.5-cp36-none-any.whl size=148505 sha256=81fc8106f234124928b107ada961608f0ea7a693a2897a1155a7687beb102140\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1ue0xxmi/wheels/84/82/73/d2b3b59b7be74ea05f2c6d64132efe27df52daffb47d1dc7bb\n",
      "Successfully built flair\n",
      "Installing collected packages: flair\n",
      "  Found existing installation: flair 0.4.5\n",
      "    Uninstalling flair-0.4.5:\n",
      "      Successfully uninstalled flair-0.4.5\n",
      "Successfully installed flair-0.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/flairNLP/flair.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmNnJAE-o4hX"
   },
   "outputs": [],
   "source": [
    "# pip install qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gP7gl75D39dn"
   },
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings, DocumentRNNEmbeddings, BertEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.data import Corpus\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3974,
     "status": "ok",
     "timestamp": 1590105530741,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "_VBdZ1L34XwO",
    "outputId": "324192f3-58a9-42bf-e2a8-eab4f4a42156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U90p69Xd5KIE"
   },
   "outputs": [],
   "source": [
    "data_folder = \"/content/drive/My Drive/Capstone/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkUisuXz7tuK"
   },
   "source": [
    "## Train on benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6K-_Bmpa6gn8"
   },
   "outputs": [],
   "source": [
    "benchmark = pd.read_csv(data_folder + \"combined_benchmark.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1606,
     "status": "ok",
     "timestamp": 1590105533847,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "HVaSsyAn72AU",
    "outputId": "73cf8b4a-6e65-42df-9187-6d015019d5c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3980</td>\n",
       "      <td>0</td>\n",
       "      <td>Why not subscribe to the magazine ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4286</td>\n",
       "      <td>-1</td>\n",
       "      <td>Tornio Works employs 2,300 of whom more than 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "      <td>The move is aimed at boosting sales , cost-eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1178</td>\n",
       "      <td>0</td>\n",
       "      <td>As a result of the merger , the largest profes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3491</td>\n",
       "      <td>-1</td>\n",
       "      <td>18 March 2010 A leakage in the gypsum pond was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                               text\n",
       "0        3980      0                Why not subscribe to the magazine ?\n",
       "1        4286     -1  Tornio Works employs 2,300 of whom more than 1...\n",
       "2         906      1  The move is aimed at boosting sales , cost-eff...\n",
       "3        1178      0  As a result of the merger , the largest profes...\n",
       "4        3491     -1  18 March 2010 A leakage in the gypsum pond was..."
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2R4ij2pZ-db"
   },
   "outputs": [],
   "source": [
    "benchmark = benchmark.drop(columns = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1275,
     "status": "ok",
     "timestamp": 1590105534263,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "pw80l5PVaGRt",
    "outputId": "407f5ca4-351a-4304-c3a4-facb38cbbc7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Why not subscribe to the magazine ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>Tornio Works employs 2,300 of whom more than 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The move is aimed at boosting sales , cost-eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>As a result of the merger , the largest profes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>18 March 2010 A leakage in the gypsum pond was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0                Why not subscribe to the magazine ?\n",
       "1     -1  Tornio Works employs 2,300 of whom more than 1...\n",
       "2      1  The move is aimed at boosting sales , cost-eff...\n",
       "3      0  As a result of the merger , the largest profes...\n",
       "4     -1  18 March 2010 A leakage in the gypsum pond was..."
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5dAal-w7-xO"
   },
   "outputs": [],
   "source": [
    "# def convert_label(label):\n",
    "#   'convert label from text to -1, 0, and 1.'\n",
    "    \n",
    "#   if label == 'neutral':\n",
    "#       return 0\n",
    "#   elif label == 'negative':\n",
    "#       return -1\n",
    "#   else:\n",
    "#       return 1\n",
    "\n",
    "# benchmark['label'] = benchmark['label'].apply(convert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DmpujRg8MPq"
   },
   "outputs": [],
   "source": [
    "# benchmark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hLbF9Uk8UxR"
   },
   "outputs": [],
   "source": [
    "#benchmark['label'] = '__label__' + benchmark['label'].astype(str)\n",
    "#benchmark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t_MVMXq8pvf"
   },
   "source": [
    "#### Create train, dev and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lk74QcpTawES"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_I-ZGeeaflU"
   },
   "outputs": [],
   "source": [
    "kaggle_X_train, kaggle_X_te, kaggle_Y_train, kaggle_Y_te = train_test_split(benchmark['text'],benchmark['label'], test_size = 0.2, random_state = 0, stratify = benchmark['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Xv-gONSa_dV"
   },
   "outputs": [],
   "source": [
    "kaggle_X_dev, kaggle_X_test, kaggle_Y_dev, kaggle_Y_test = train_test_split(kaggle_X_te,kaggle_Y_te, test_size = 0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2042,
     "status": "ok",
     "timestamp": 1590105539697,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "nR_N-t0lazS-",
    "outputId": "51378e08-9a59-48fd-bfd4-5d96a9e3a3ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168,)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w36JGs38bHXo"
   },
   "outputs": [],
   "source": [
    "benchmark_train_df =  pd.DataFrame({'text': kaggle_X_train, 'label': kaggle_Y_train   })\n",
    "benchmark_dev_df = pd.DataFrame({'text': kaggle_X_dev, 'label': kaggle_Y_dev   })\n",
    "benchmark_test_df = pd.DataFrame({'text': kaggle_X_test, 'label': kaggle_Y_test   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKoXR0v_cnb_"
   },
   "outputs": [],
   "source": [
    "benchmark_train_df.to_csv( data_folder + \"benchmark_train_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUbbfcSC8ekr"
   },
   "outputs": [],
   "source": [
    "benchmark_dev_df.to_csv( data_folder + \"benchmark_dev_set.csv\")\n",
    "benchmark_test_df.to_csv( data_folder + \"benchmark_test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7QE3IS69txr"
   },
   "source": [
    "#### Build corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2468,
     "status": "ok",
     "timestamp": 1590105546710,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "YPXL15Q_d4kq",
    "outputId": "8bc8a5e4-ce65-4faf-9018-aea88f55dd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-21 23:59:05,001 Reading data from /content/drive/My Drive/Capstone/data\n",
      "2020-05-21 23:59:05,005 Train: /content/drive/My Drive/Capstone/data/benchmark_train_set.csv\n",
      "2020-05-21 23:59:05,006 Dev: /content/drive/My Drive/Capstone/data/benchmark_dev_set.csv\n",
      "2020-05-21 23:59:05,007 Test: /content/drive/My Drive/Capstone/data/benchmark_test_set.csv\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "column_name_map = {1: \"text\", 0: \"label_topic\"}\n",
    "\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder, column_name_map,\n",
    "                              train_file='benchmark_train_set.csv',\n",
    "                              test_file='benchmark_test_set.csv',\n",
    "                              dev_file='benchmark_dev_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zz-2gNy58wDI"
   },
   "outputs": [],
   "source": [
    "# # corpus = NLPTaskDataFetcher.load_classification_corpus(Path(data_folder), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
    "# column_name_map = {1: \"text\", 0: \"label_topic\"}\n",
    "\n",
    "# corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "#                                          column_name_map,\n",
    "#                                           #no header in kaggle data\n",
    "#                                          delimiter='\\t',    # comma separated rows\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb1lynSQ98-z"
   },
   "source": [
    "#### Create word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4Z5WhllonIl"
   },
   "outputs": [],
   "source": [
    "# pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11304,
     "status": "ok",
     "timestamp": 1590105562606,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "4u9PFUze9_5y",
    "outputId": "920745d0-2990-403f-e061-bc72af7f6556"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = [BertEmbeddings(), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bXEoDSDwgXOO",
    "outputId": "cf59f4b6-4c29-4c40-ddb9-8b56d46318cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-22 00:00:10,558 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  \n",
      "100%|██████████| 1315/1315 [00:01<00:00, 993.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-22 00:00:12,225 [b'1399', b'399', b'406', b'770', b'823', b'753', b'643', b'725', b'784', b'1335', b'30', b'1210', b'1016', b'903', b'133', b'1143', b'940', b'309', b'4', b'1447', b'833', b'509', b'685', b'532', b'1414', b'780', b'389', b'64', b'19', b'513', b'104', b'657', b'1286', b'721', b'765', b'526', b'1440', b'983', b'326', b'210', b'1084', b'1167', b'1353', b'37', b'1426', b'959', b'319', b'25', b'1443', b'573', b'381', b'788', b'1412', b'482', b'208', b'396', b'1444', b'232', b'183', b'1202', b'918', b'567', b'1096', b'153', b'1139', b'696', b'901', b'480', b'1280', b'73', b'1146', b'531', b'1183', b'840', b'726', b'363', b'438', b'1332', b'1195', b'1027', b'229', b'680', b'165', b'1155', b'417', b'510', b'94', b'1301', b'978', b'1050', b'1026', b'578', b'131', b'521', b'97', b'164', b'1203', b'1107', b'704', b'675', b'519', b'541', b'1352', b'787', b'1076', b'67', b'815', b'134', b'101', b'1307', b'982', b'927', b'930', b'1114', b'58', b'259', b'416', b'1254', b'710', b'18', b'275', b'756', b'1011', b'998', b'426', b'755', b'120', b'414', b'719', b'84', b'1310', b'775', b'524', b'650', b'177', b'761', b'253', b'729', b'1074', b'1389', b'1348', b'47', b'362', b'441', b'551', b'251', b'1200', b'1079', b'508', b'830', b'1105', b'7', b'654', b'41', b'877', b'591', b'1343', b'307', b'1409', b'1218', b'105', b'345', b'1459', b'1041', b'60', b'1024', b'548', b'922', b'1425', b'1430', b'995', b'642', b'143', b'564', b'576', b'1321', b'311', b'746', b'582', b'660', b'581', b'463', b'1033', b'200', b'603', b'653', b'433', b'507', b'557', b'72', b'994', b'129', b'1226', b'505', b'54', b'921', b'460', b'863', b'442', b'46', b'146', b'616', b'1347', b'613', b'1369', b'890', b'472', b'1190', b'427', b'751', b'150', b'34', b'899', b'189', b'255', b'1342', b'96', b'439', b'125', b'481', b'492', b'377', b'733', b'1022', b'33', b'269', b'195', b'320', b'50', b'957', b'220', b'799', b'199', b'880', b'1370', b'894', b'75', b'368', b'763', b'558', b'652', b'186', b'1002', b'1217', b'681', b'683', b'670', b'912', b'168', b'1390', b'747', b'1086', b'606', b'226', b'370', b'1362', b'109', b'160', b'981', b'1038', b'231', b'674', b'1259', b'658', b'702', b'1393', b'589', b'711', b'1095', b'1214', b'552', b'639', b'1149', b'374', b'744', b'135', b'141', b'300', b'664', b'900', b'1408', b'1237', b'218', b'287', b'1381', b'1126', b'993', b'898', b'391', b'737', b'1172', b'340', b'324', b'410', b'920', b'1305', b'528', b'1018', b'849', b'1267', b'279', b'647', b'533', b'946', b'537', b'1417', b'223', b'122', b'587', b'471', b'802', b'516', b'430', b'359', b'690', b'688', b'644', b'216', b'622', b'271', b'462', b'83', b'276', b'467', b'1109', b'1221', b'1222', b'979', b'811', b'968', b'451', b'347', b'364', b'1170', b'666', b'709', b'357', b'478', b'23', b'821', b'1358', b'1401', b'1413', b'142', b'243', b'298', b'449', b'236', b'665', b'1145', b'343', b'378', b'1064', b'1162', b'293', b'988', b'264', b'715', b'796', b'977', b'687', b'1169', b'748', b'563', b'571', b'455', b'655', b'1258', b'8', b'1046', b'278', b'1215', b'1185', b'108', b'872', b'893', b'1209', b'1297', b'1008', b'727', b'1132', b'843', b'825', b'1047', b'1080', b'110', b'1004', b'1106', b'1012', b'1056', b'163', b'572', b'1242', b'318', b'1326', b'569', b'835', b'452', b'1110', b'351', b'446', b'1293', b'607', b'740', b'1083', b'506', b'2', b'734', b'1025', b'1067', b'731', b'856', b'169', b'757', b'1253', b'878', b'518', b'114', b'522', b'3', b'862', b'593', b'257', b'846', b'494', b'173', b'1244', b'349', b'1054', b'80', b'549', b'1337', b'238', b'1075', b'57', b'1009', b'515', b'1163', b'1452', b'1152', b'1168', b'1340', b'838', b'1078', b'76', b'1220', b'201', b'577', b'140', b'48', b'628', b'754', b'386', b'1322', b'1134', b'874', b'1108', b'56', b'184', b'190', b'11', b'594', b'933', b'669', b'1451', b'61', b'477', b'597', b'870', b'304', b'656', b'824', b'530', b'1069', b'919', b'1365', b'600', b'1336', b'16', b'876', b'440', b'1150', b'1448', b'1288', b'385', b'490', b'499', b'947', b'1345', b'32', b'1042', b'1363', b'436', b'1232', b'1302', b'1098', b'989', b'612', b'476', b'404', b'375', b'1208', b'1137', b'1439', b'383', b'1308', b'422', b'245', b'1387', b'5', b'17', b'717', b'206', b'1019', b'1225', b'943', b'1173', b'286', b'1442', b'614', b'713', b'1234', b'924', b'1266', b'560', b'682', b'703', b'881', b'387', b'322', b'1432', b'1460', b'89', b'148', b'380', b'354', b'237', b'1029', b'944', b'976', b'272', b'339', b'1051', b'1249', b'454', b'149', b'66', b'813', b'1411', b'1449', b'371', b'74', b'697', b'1323', b'281', b'1127', b'774', b'261', b'123', b'633', b'334', b'325', b'545', b'465', b'914', b'31', b'1007', b'1135', b'542', b'625', b'953', b'1281', b'742', b'473', b'636', b'402', b'1147', b'1124', b'1053', b'909', b'691', b'706', b'39', b'1212', b'158', b'212', b'810', b'852', b'841', b'1015', b'812', b'559', b'1396', b'864', b'1256', b'1379', b'800', b'561', b'464', b'641', b'1313', b'224', b'1199', b'358', b'958', b'1418', b'1309', b'985', b'671', b'22', b'1372', b'361', b'839', b'1366', b'1128', b'768', b'951', b'782', b'1072', b'574', b'954', b'474', b'975', b'1206', b'1023', b'297', b'118', b'157', b'1111', b'35', b'771', b'602', b'873', b'1314', b'923', b'1035', b'321', b'971', b'689', b'1049', b'1189', b'59', b'1416', b'585', b'935', b'882', b'352', b'252', b'580', b'1136', b'1160', b'448', b'365', b'12', b'925', b'1241', b'534', b'247', b'960', b'1115', b'1092', b'411', b'1233', b'797', b'1028', b'457', b'350', b'103', b'323', b'1194', b'547', b'256', b'356', b'635', b'1103', b'590', b'1410', b'346', b'948', b'891', b'315', b'456', b'136', b'805', b'95', b'1285', b'1368', b'193', b'646', b'1454', b'525', b'1112', b'116', b'916', b'244', b'262', b'632', b'484', b'217', b'1036', b'1138', b'91', b'126', b'803', b'1355', b'401', b'990', b'996', b'170', b'1296', b'21', b'500', b'69', b'429', b'1207', b'332', b'1404', b'991', b'498', b'20', b'228', b'724', b'1063', b'1238', b'648', b'1230', b'739', b'1275', b'1113', b'1204', b'527', b'848', b'1434', b'767', b'1251', b'1269', b'817', b'437', b'809', b'1278', b'1', b'1407', b'1304', b'502', b'198', b'1037', b'855', b'592', b'934', b'1071', b'207', b'827', b'431', b'1357', b'997', b'1014', b'483', b'458', b'804', b'28', b'132', b'741', b'1435', b'792', b'906', b'487', b'192', b'902', b'277', b'1406', b'836', b'1228', b'1262', b'290', b'1330', b'686', b'1392', b'634', b'1129', b'1356', b'539', b'969', b'678', b'772', b'128', b'956', b'535', b'1325', b'1045', b'1104', b'393', b'1174', b'1236', b'159', b'584', b'1431', b'333', b'781', b'544', b'26', b'1062', b'295', b'831', b'1154', b'280', b'659', b'1265', b'1405', b'466', b'783', b'162', b'242', b'1171', b'130', b'1161', b'1327', b'886', b'1445', b'869', b'700', b'147', b'1382', b'469', b'556', b'980', b'764', b'156', b'1043', b'205', b'301', b'45', b'523', b'479', b'382', b'884', b'1274', b'79', b'418', b'93', b'555', b'292', b'1044', b'1331', b'194', b'40', b'586', b'29', b'617', b'834', b'230', b'845', b'904', b'1294', b'1052', b'1453', b'77', b'579', b'778', b'679', b'1312', b'942', b'360', b'1324', b'263', b'249', b'749', b'379', b'553', b'859', b'144', b'1376', b'390', b'447', b'151', b'819', b'453', b'1315', b'1003', b'268', b'15', b'917', b'496', b'1088', b'517', b'1429', b'423', b'1438', b'113', b'568', b'445', b'313', b'1077', b'314', b'1279', b'701', b'52', b'621', b'55', b'121', b'202', b'1298', b'106', b'844', b'1000', b'392', b'871', b'1291', b'932', b'915', b'475', b'38', b'895', b'49', b'1385', b'908', b'1383', b'562', b'630', b'1187', b'1141', b'538', b'673', b'1117', b'609', b'1034', b'1085', b'493', b'180', b'265', b'1123', b'185', b'51', b'1133', b'1001', b'714', b'1403', b'816', b'554', b'1130', b'1329', b'939', b'543', b'71', b'963', b'766', b'1227', b'112', b'369', b'984', b'1398', b'818', b'937', b'459', b'1395', b'99', b'629', b'712', b'233', b'790', b'1260', b'107', b'408', b'907', b'145', b'1039', b'786', b'1179', b'0', b'1125', b'367', b'879', b'931', b'501', b'720', b'1320', b'1181', b'620', b'353', b'1423', b'1120', b'497', b'1299', b'175', b'1020', b'181', b'139', b'461', b'154', b'1287', b'1005', b'806', b'1235', b'82', b'814', b'808', b'444', b'267', b'604', b'1317', b'1344', b'485', b'328', b'43', b'730', b'117', b'1229', b'987', b'837', b'645', b'285', b'250', b'1263', b'762', b'13', b'1284', b'341', b'785', b'1157', b'1420', b'1377', b'750', b'1240', b'1006', b'828', b'1211', b'1066', b'1388', b'400', b'1250', b'850', b'868', b'222', b'235', b'491', b'1311', b'219', b'100', b'1013', b'1360', b'388', b'822', b'299', b'283', b'1213', b'999', b'826', b'1231', b'1300', b'626', b'489', b'70', b'503', b'127', b'1384', b'970', b'618', b'888', b'24', b'1070', b'693', b'638', b'1433', b'1316', b'1017', b'288', b'282', b'1057', b'6', b'1159', b'1334', b'312', b'708', b'204', b'504', b'273', b'98', b'1268', b'723', b'188', b'883', b'1283', b'215', b'595', b'196', b'692', b'413', b'1216', b'718', b'1359', b'102', b'1205', b'337', b'1446', b'1261', b'1441', b'705', b'514', b'1188', b'488', b'777', b'965', b'1178', b'308', b'344', b'88', b'412', b'179', b'372', b'1257', b'296', b'1061', b'695', b'760', b'1102', b'1180', b'974', b'769', b'1378', b'615', b'78', b'1272', b'1094', b'1176', b'570', b'270', b'913', b'306', b'1437', b'892', b'1424', b'274', b'1121', b'403', b'68', b'221', b'1349', b'1059', b'291', b'1142', b'1245', b'1089', b'911', b'861', b'225', b'1239', b'1248', b'155', b'550', b'851', b'1371', b'176', b'1341', b'992', b'1456', b'115', b'85', b'966', b'1100', b'728', b'1010', b'303', b'86', b'672', b'1118', b'92', b'1099', b'795', b'520', b'1186', b'565', b'637', b'1140', b'1081', b'14', b'1303', b'1151', b'1380', b'1131', b'1361', b'1328', b'676', b'897', b'407', b'1166', b'1032', b'829', b'832', b'1224', b'598', b'779', b'1354', b'1373', b'1255', b'1415', b'732', b'172', b'619', b'1060', b'887', b'167', b'1156', b'397', b'941', b'807', b'858', b'42', b'1400', b'611', b'428', b'27', b'540', b'336', b'174', b'896', b'842', b'1338', b'1122', b'667', b'240', b'1191', b'1457', b'776', b'1021', b'1175', b'752', b'1097', b'1055', b'605', b'1264', b'663', b'424', b'254', b'305', b'1030', b'138', b'624', b'1219', b'327', b'171', b'1277', b'161', b'373', b'1087', b'266', b'1397', b'662', b'197', b'596', b'302', b'342', b'294', b'178', b'745', b'1270', b'1184', b'63', b'866', b'415', b'938', b'1273', b'1391', b'1319', b'1093', b'1201', b'62', b'588', b'1252', b'1198', b'1282', b'885', b'425', b'759', b'928', b'65', b'857', b'1031', b'512', b'203', b'707', b'1374', b'214', b'289', b'794', b'1402', b'1223', b'716', b'1196', b'1394', b'335', b'1182', b'875', b'213', b'1246', b'1091', b'945', b'182', b'330', b'950', b'1346', b'394', b'631', b'865', b'1306', b'599', b'1351', b'36', b'209', b'1165', b'81', b'661', b'511', b'867', b'601', b'961', b'405']\n",
      "2020-05-22 00:00:12,266 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-22 00:00:12,271 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentLSTMEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): BertEmbeddings(\n",
      "        (model): BertModel(\n",
      "          (embeddings): BertEmbeddings(\n",
      "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "            (position_embeddings): Embedding(512, 768)\n",
      "            (token_type_embeddings): Embedding(2, 768)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (encoder): BertEncoder(\n",
      "            (layer): ModuleList(\n",
      "              (0): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (pooler): BertPooler(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (activation): Tanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=5120, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=1315, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-05-22 00:00:12,272 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-22 00:00:12,274 Corpus: \"Corpus: 1168 train + 146 dev + 147 test sentences\"\n",
      "2020-05-22 00:00:12,275 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-22 00:00:12,276 Parameters:\n",
      "2020-05-22 00:00:12,279  - learning_rate: \"0.1\"\n",
      "2020-05-22 00:00:12,280  - mini_batch_size: \"32\"\n",
      "2020-05-22 00:00:12,282  - patience: \"3\"\n",
      "2020-05-22 00:00:12,284  - anneal_factor: \"0.5\"\n",
      "2020-05-22 00:00:12,285  - max_epochs: \"1\"\n",
      "2020-05-22 00:00:12,287  - shuffle: \"True\"\n",
      "2020-05-22 00:00:12,288  - train_with_dev: \"False\"\n",
      "2020-05-22 00:00:12,289  - batch_growth_annealing: \"False\"\n",
      "2020-05-22 00:00:12,291 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-22 00:00:12,293 Model training base path: \"/content/drive/My Drive/Capstone/data\"\n",
      "2020-05-22 00:00:12,295 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-22 00:00:12,297 Device: cuda:0\n",
      "2020-05-22 00:00:12,299 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-22 00:00:12,300 Embeddings storage mode: cpu\n",
      "2020-05-22 00:00:12,311 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-22 00:00:14,378 epoch 1 - iter 3/37 - loss 7.21808227 - samples/sec: 58.85\n",
      "2020-05-22 00:00:26,773 epoch 1 - iter 6/37 - loss 7.23706818 - samples/sec: 62.96\n",
      "2020-05-22 00:00:38,796 epoch 1 - iter 9/37 - loss 7.22830682 - samples/sec: 60.53\n",
      "2020-05-22 00:00:51,716 epoch 1 - iter 12/37 - loss 7.23504174 - samples/sec: 54.71\n",
      "2020-05-22 00:01:03,903 epoch 1 - iter 15/37 - loss 7.23724051 - samples/sec: 63.24\n",
      "2020-05-22 00:01:15,941 epoch 1 - iter 18/37 - loss 7.23996364 - samples/sec: 63.34\n",
      "2020-05-22 00:01:28,571 epoch 1 - iter 21/37 - loss 7.25162854 - samples/sec: 61.16\n",
      "2020-05-22 00:01:40,660 epoch 1 - iter 24/37 - loss 7.25376248 - samples/sec: 61.81\n",
      "2020-05-22 00:01:52,846 epoch 1 - iter 27/37 - loss 7.25307117 - samples/sec: 58.58\n",
      "2020-05-22 00:02:04,881 epoch 1 - iter 30/37 - loss 7.25638606 - samples/sec: 65.82\n",
      "2020-05-22 00:02:16,826 epoch 1 - iter 33/37 - loss 7.25896494 - samples/sec: 67.94\n",
      "2020-05-22 00:02:28,877 epoch 1 - iter 36/37 - loss 7.25579302 - samples/sec: 63.39\n",
      "2020-05-22 00:02:39,841 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-22 00:02:39,843 EPOCH 1 done: loss 7.2574 - lr 0.1000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train(data_folder, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1484695,
     "status": "ok",
     "timestamp": 1590104285194,
     "user": {
      "displayName": "Naga Sirisha",
      "photoUrl": "",
      "userId": "07356107766325774737"
     },
     "user_tz": 420
    },
    "id": "Wik8DDxm-DKy",
    "outputId": "6803f050-d7a8-480e-dbe6-3b1cc5220ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-21 23:13:20,721 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated method __init__. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "100%|██████████| 1315/1315 [00:01<00:00, 1083.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-21 23:13:22,245 [b'1399', b'399', b'406', b'770', b'823', b'753', b'643', b'725', b'784', b'1335', b'30', b'1210', b'1016', b'903', b'133', b'1143', b'940', b'309', b'4', b'1447', b'833', b'509', b'685', b'532', b'1414', b'780', b'389', b'64', b'19', b'513', b'104', b'657', b'1286', b'721', b'765', b'526', b'1440', b'983', b'326', b'210', b'1084', b'1167', b'1353', b'37', b'1426', b'959', b'319', b'25', b'1443', b'573', b'381', b'788', b'1412', b'482', b'208', b'396', b'1444', b'232', b'183', b'1202', b'918', b'567', b'1096', b'153', b'1139', b'696', b'901', b'480', b'1280', b'73', b'1146', b'531', b'1183', b'840', b'726', b'363', b'438', b'1332', b'1195', b'1027', b'229', b'680', b'165', b'1155', b'417', b'510', b'94', b'1301', b'978', b'1050', b'1026', b'578', b'131', b'521', b'97', b'164', b'1203', b'1107', b'704', b'675', b'519', b'541', b'1352', b'787', b'1076', b'67', b'815', b'134', b'101', b'1307', b'982', b'927', b'930', b'1114', b'58', b'259', b'416', b'1254', b'710', b'18', b'275', b'756', b'1011', b'998', b'426', b'755', b'120', b'414', b'719', b'84', b'1310', b'775', b'524', b'650', b'177', b'761', b'253', b'729', b'1074', b'1389', b'1348', b'47', b'362', b'441', b'551', b'251', b'1200', b'1079', b'508', b'830', b'1105', b'7', b'654', b'41', b'877', b'591', b'1343', b'307', b'1409', b'1218', b'105', b'345', b'1459', b'1041', b'60', b'1024', b'548', b'922', b'1425', b'1430', b'995', b'642', b'143', b'564', b'576', b'1321', b'311', b'746', b'582', b'660', b'581', b'463', b'1033', b'200', b'603', b'653', b'433', b'507', b'557', b'72', b'994', b'129', b'1226', b'505', b'54', b'921', b'460', b'863', b'442', b'46', b'146', b'616', b'1347', b'613', b'1369', b'890', b'472', b'1190', b'427', b'751', b'150', b'34', b'899', b'189', b'255', b'1342', b'96', b'439', b'125', b'481', b'492', b'377', b'733', b'1022', b'33', b'269', b'195', b'320', b'50', b'957', b'220', b'799', b'199', b'880', b'1370', b'894', b'75', b'368', b'763', b'558', b'652', b'186', b'1002', b'1217', b'681', b'683', b'670', b'912', b'168', b'1390', b'747', b'1086', b'606', b'226', b'370', b'1362', b'109', b'160', b'981', b'1038', b'231', b'674', b'1259', b'658', b'702', b'1393', b'589', b'711', b'1095', b'1214', b'552', b'639', b'1149', b'374', b'744', b'135', b'141', b'300', b'664', b'900', b'1408', b'1237', b'218', b'287', b'1381', b'1126', b'993', b'898', b'391', b'737', b'1172', b'340', b'324', b'410', b'920', b'1305', b'528', b'1018', b'849', b'1267', b'279', b'647', b'533', b'946', b'537', b'1417', b'223', b'122', b'587', b'471', b'802', b'516', b'430', b'359', b'690', b'688', b'644', b'216', b'622', b'271', b'462', b'83', b'276', b'467', b'1109', b'1221', b'1222', b'979', b'811', b'968', b'451', b'347', b'364', b'1170', b'666', b'709', b'357', b'478', b'23', b'821', b'1358', b'1401', b'1413', b'142', b'243', b'298', b'449', b'236', b'665', b'1145', b'343', b'378', b'1064', b'1162', b'293', b'988', b'264', b'715', b'796', b'977', b'687', b'1169', b'748', b'563', b'571', b'455', b'655', b'1258', b'8', b'1046', b'278', b'1215', b'1185', b'108', b'872', b'893', b'1209', b'1297', b'1008', b'727', b'1132', b'843', b'825', b'1047', b'1080', b'110', b'1004', b'1106', b'1012', b'1056', b'163', b'572', b'1242', b'318', b'1326', b'569', b'835', b'452', b'1110', b'351', b'446', b'1293', b'607', b'740', b'1083', b'506', b'2', b'734', b'1025', b'1067', b'731', b'856', b'169', b'757', b'1253', b'878', b'518', b'114', b'522', b'3', b'862', b'593', b'257', b'846', b'494', b'173', b'1244', b'349', b'1054', b'80', b'549', b'1337', b'238', b'1075', b'57', b'1009', b'515', b'1163', b'1452', b'1152', b'1168', b'1340', b'838', b'1078', b'76', b'1220', b'201', b'577', b'140', b'48', b'628', b'754', b'386', b'1322', b'1134', b'874', b'1108', b'56', b'184', b'190', b'11', b'594', b'933', b'669', b'1451', b'61', b'477', b'597', b'870', b'304', b'656', b'824', b'530', b'1069', b'919', b'1365', b'600', b'1336', b'16', b'876', b'440', b'1150', b'1448', b'1288', b'385', b'490', b'499', b'947', b'1345', b'32', b'1042', b'1363', b'436', b'1232', b'1302', b'1098', b'989', b'612', b'476', b'404', b'375', b'1208', b'1137', b'1439', b'383', b'1308', b'422', b'245', b'1387', b'5', b'17', b'717', b'206', b'1019', b'1225', b'943', b'1173', b'286', b'1442', b'614', b'713', b'1234', b'924', b'1266', b'560', b'682', b'703', b'881', b'387', b'322', b'1432', b'1460', b'89', b'148', b'380', b'354', b'237', b'1029', b'944', b'976', b'272', b'339', b'1051', b'1249', b'454', b'149', b'66', b'813', b'1411', b'1449', b'371', b'74', b'697', b'1323', b'281', b'1127', b'774', b'261', b'123', b'633', b'334', b'325', b'545', b'465', b'914', b'31', b'1007', b'1135', b'542', b'625', b'953', b'1281', b'742', b'473', b'636', b'402', b'1147', b'1124', b'1053', b'909', b'691', b'706', b'39', b'1212', b'158', b'212', b'810', b'852', b'841', b'1015', b'812', b'559', b'1396', b'864', b'1256', b'1379', b'800', b'561', b'464', b'641', b'1313', b'224', b'1199', b'358', b'958', b'1418', b'1309', b'985', b'671', b'22', b'1372', b'361', b'839', b'1366', b'1128', b'768', b'951', b'782', b'1072', b'574', b'954', b'474', b'975', b'1206', b'1023', b'297', b'118', b'157', b'1111', b'35', b'771', b'602', b'873', b'1314', b'923', b'1035', b'321', b'971', b'689', b'1049', b'1189', b'59', b'1416', b'585', b'935', b'882', b'352', b'252', b'580', b'1136', b'1160', b'448', b'365', b'12', b'925', b'1241', b'534', b'247', b'960', b'1115', b'1092', b'411', b'1233', b'797', b'1028', b'457', b'350', b'103', b'323', b'1194', b'547', b'256', b'356', b'635', b'1103', b'590', b'1410', b'346', b'948', b'891', b'315', b'456', b'136', b'805', b'95', b'1285', b'1368', b'193', b'646', b'1454', b'525', b'1112', b'116', b'916', b'244', b'262', b'632', b'484', b'217', b'1036', b'1138', b'91', b'126', b'803', b'1355', b'401', b'990', b'996', b'170', b'1296', b'21', b'500', b'69', b'429', b'1207', b'332', b'1404', b'991', b'498', b'20', b'228', b'724', b'1063', b'1238', b'648', b'1230', b'739', b'1275', b'1113', b'1204', b'527', b'848', b'1434', b'767', b'1251', b'1269', b'817', b'437', b'809', b'1278', b'1', b'1407', b'1304', b'502', b'198', b'1037', b'855', b'592', b'934', b'1071', b'207', b'827', b'431', b'1357', b'997', b'1014', b'483', b'458', b'804', b'28', b'132', b'741', b'1435', b'792', b'906', b'487', b'192', b'902', b'277', b'1406', b'836', b'1228', b'1262', b'290', b'1330', b'686', b'1392', b'634', b'1129', b'1356', b'539', b'969', b'678', b'772', b'128', b'956', b'535', b'1325', b'1045', b'1104', b'393', b'1174', b'1236', b'159', b'584', b'1431', b'333', b'781', b'544', b'26', b'1062', b'295', b'831', b'1154', b'280', b'659', b'1265', b'1405', b'466', b'783', b'162', b'242', b'1171', b'130', b'1161', b'1327', b'886', b'1445', b'869', b'700', b'147', b'1382', b'469', b'556', b'980', b'764', b'156', b'1043', b'205', b'301', b'45', b'523', b'479', b'382', b'884', b'1274', b'79', b'418', b'93', b'555', b'292', b'1044', b'1331', b'194', b'40', b'586', b'29', b'617', b'834', b'230', b'845', b'904', b'1294', b'1052', b'1453', b'77', b'579', b'778', b'679', b'1312', b'942', b'360', b'1324', b'263', b'249', b'749', b'379', b'553', b'859', b'144', b'1376', b'390', b'447', b'151', b'819', b'453', b'1315', b'1003', b'268', b'15', b'917', b'496', b'1088', b'517', b'1429', b'423', b'1438', b'113', b'568', b'445', b'313', b'1077', b'314', b'1279', b'701', b'52', b'621', b'55', b'121', b'202', b'1298', b'106', b'844', b'1000', b'392', b'871', b'1291', b'932', b'915', b'475', b'38', b'895', b'49', b'1385', b'908', b'1383', b'562', b'630', b'1187', b'1141', b'538', b'673', b'1117', b'609', b'1034', b'1085', b'493', b'180', b'265', b'1123', b'185', b'51', b'1133', b'1001', b'714', b'1403', b'816', b'554', b'1130', b'1329', b'939', b'543', b'71', b'963', b'766', b'1227', b'112', b'369', b'984', b'1398', b'818', b'937', b'459', b'1395', b'99', b'629', b'712', b'233', b'790', b'1260', b'107', b'408', b'907', b'145', b'1039', b'786', b'1179', b'0', b'1125', b'367', b'879', b'931', b'501', b'720', b'1320', b'1181', b'620', b'353', b'1423', b'1120', b'497', b'1299', b'175', b'1020', b'181', b'139', b'461', b'154', b'1287', b'1005', b'806', b'1235', b'82', b'814', b'808', b'444', b'267', b'604', b'1317', b'1344', b'485', b'328', b'43', b'730', b'117', b'1229', b'987', b'837', b'645', b'285', b'250', b'1263', b'762', b'13', b'1284', b'341', b'785', b'1157', b'1420', b'1377', b'750', b'1240', b'1006', b'828', b'1211', b'1066', b'1388', b'400', b'1250', b'850', b'868', b'222', b'235', b'491', b'1311', b'219', b'100', b'1013', b'1360', b'388', b'822', b'299', b'283', b'1213', b'999', b'826', b'1231', b'1300', b'626', b'489', b'70', b'503', b'127', b'1384', b'970', b'618', b'888', b'24', b'1070', b'693', b'638', b'1433', b'1316', b'1017', b'288', b'282', b'1057', b'6', b'1159', b'1334', b'312', b'708', b'204', b'504', b'273', b'98', b'1268', b'723', b'188', b'883', b'1283', b'215', b'595', b'196', b'692', b'413', b'1216', b'718', b'1359', b'102', b'1205', b'337', b'1446', b'1261', b'1441', b'705', b'514', b'1188', b'488', b'777', b'965', b'1178', b'308', b'344', b'88', b'412', b'179', b'372', b'1257', b'296', b'1061', b'695', b'760', b'1102', b'1180', b'974', b'769', b'1378', b'615', b'78', b'1272', b'1094', b'1176', b'570', b'270', b'913', b'306', b'1437', b'892', b'1424', b'274', b'1121', b'403', b'68', b'221', b'1349', b'1059', b'291', b'1142', b'1245', b'1089', b'911', b'861', b'225', b'1239', b'1248', b'155', b'550', b'851', b'1371', b'176', b'1341', b'992', b'1456', b'115', b'85', b'966', b'1100', b'728', b'1010', b'303', b'86', b'672', b'1118', b'92', b'1099', b'795', b'520', b'1186', b'565', b'637', b'1140', b'1081', b'14', b'1303', b'1151', b'1380', b'1131', b'1361', b'1328', b'676', b'897', b'407', b'1166', b'1032', b'829', b'832', b'1224', b'598', b'779', b'1354', b'1373', b'1255', b'1415', b'732', b'172', b'619', b'1060', b'887', b'167', b'1156', b'397', b'941', b'807', b'858', b'42', b'1400', b'611', b'428', b'27', b'540', b'336', b'174', b'896', b'842', b'1338', b'1122', b'667', b'240', b'1191', b'1457', b'776', b'1021', b'1175', b'752', b'1097', b'1055', b'605', b'1264', b'663', b'424', b'254', b'305', b'1030', b'138', b'624', b'1219', b'327', b'171', b'1277', b'161', b'373', b'1087', b'266', b'1397', b'662', b'197', b'596', b'302', b'342', b'294', b'178', b'745', b'1270', b'1184', b'63', b'866', b'415', b'938', b'1273', b'1391', b'1319', b'1093', b'1201', b'62', b'588', b'1252', b'1198', b'1282', b'885', b'425', b'759', b'928', b'65', b'857', b'1031', b'512', b'203', b'707', b'1374', b'214', b'289', b'794', b'1402', b'1223', b'716', b'1196', b'1394', b'335', b'1182', b'875', b'213', b'1246', b'1091', b'945', b'182', b'330', b'950', b'1346', b'394', b'631', b'865', b'1306', b'599', b'1351', b'36', b'209', b'1165', b'81', b'661', b'511', b'867', b'601', b'961', b'405']\n",
      "2020-05-21 23:13:22,268 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:13:22,272 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentLSTMEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): BertEmbeddings(\n",
      "        (model): BertModel(\n",
      "          (embeddings): BertEmbeddings(\n",
      "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "            (position_embeddings): Embedding(512, 768)\n",
      "            (token_type_embeddings): Embedding(2, 768)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (encoder): BertEncoder(\n",
      "            (layer): ModuleList(\n",
      "              (0): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (pooler): BertPooler(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (activation): Tanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=5120, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=1315, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-05-21 23:13:22,278 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:13:22,281 Corpus: \"Corpus: 1168 train + 146 dev + 147 test sentences\"\n",
      "2020-05-21 23:13:22,284 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:13:22,286 Parameters:\n",
      "2020-05-21 23:13:22,287  - learning_rate: \"0.1\"\n",
      "2020-05-21 23:13:22,288  - mini_batch_size: \"32\"\n",
      "2020-05-21 23:13:22,290  - patience: \"3\"\n",
      "2020-05-21 23:13:22,291  - anneal_factor: \"0.5\"\n",
      "2020-05-21 23:13:22,293  - max_epochs: \"10\"\n",
      "2020-05-21 23:13:22,294  - shuffle: \"True\"\n",
      "2020-05-21 23:13:22,296  - train_with_dev: \"False\"\n",
      "2020-05-21 23:13:22,297  - batch_growth_annealing: \"False\"\n",
      "2020-05-21 23:13:22,298 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:13:22,300 Model training base path: \"/content/drive/My Drive/Capstone/data\"\n",
      "2020-05-21 23:13:22,301 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:13:22,302 Device: cuda:0\n",
      "2020-05-21 23:13:22,303 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:13:22,304 Embeddings storage mode: cpu\n",
      "2020-05-21 23:13:22,318 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-21 23:13:24,368 epoch 1 - iter 3/37 - loss 7.19225105 - samples/sec: 57.55\n",
      "2020-05-21 23:13:36,271 epoch 1 - iter 6/37 - loss 7.19376230 - samples/sec: 64.68\n",
      "2020-05-21 23:13:48,441 epoch 1 - iter 9/37 - loss 7.20930031 - samples/sec: 59.34\n",
      "2020-05-21 23:14:00,101 epoch 1 - iter 12/37 - loss 7.23223650 - samples/sec: 65.69\n",
      "2020-05-21 23:14:12,201 epoch 1 - iter 15/37 - loss 7.23462626 - samples/sec: 60.08\n",
      "2020-05-21 23:14:23,886 epoch 1 - iter 18/37 - loss 7.24993417 - samples/sec: 60.49\n",
      "2020-05-21 23:14:35,451 epoch 1 - iter 21/37 - loss 7.25064498 - samples/sec: 61.43\n",
      "2020-05-21 23:14:47,614 epoch 1 - iter 24/37 - loss 7.25656825 - samples/sec: 63.06\n",
      "2020-05-21 23:14:59,339 epoch 1 - iter 27/37 - loss 7.25968006 - samples/sec: 63.41\n",
      "2020-05-21 23:15:10,903 epoch 1 - iter 30/37 - loss 7.26173560 - samples/sec: 66.30\n",
      "2020-05-21 23:15:22,753 epoch 1 - iter 33/37 - loss 7.26206453 - samples/sec: 66.39\n",
      "2020-05-21 23:15:34,508 epoch 1 - iter 36/37 - loss 7.26313762 - samples/sec: 58.94\n",
      "2020-05-21 23:15:45,227 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:15:45,231 EPOCH 1 done: loss 7.2611 - lr 0.1000000\n",
      "2020-05-21 23:15:47,995 DEV : loss 7.103290557861328 - score 0.9992\n",
      "2020-05-21 23:15:48,139 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-21 23:15:50,082 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:15:52,127 epoch 2 - iter 3/37 - loss 7.17459520 - samples/sec: 56.96\n",
      "2020-05-21 23:16:05,403 epoch 2 - iter 6/37 - loss 7.13944173 - samples/sec: 60.17\n",
      "2020-05-21 23:16:16,987 epoch 2 - iter 9/37 - loss 7.13267104 - samples/sec: 61.20\n",
      "2020-05-21 23:16:28,901 epoch 2 - iter 12/37 - loss 7.13299847 - samples/sec: 64.58\n",
      "2020-05-21 23:16:40,825 epoch 2 - iter 15/37 - loss 7.15067151 - samples/sec: 60.11\n",
      "2020-05-21 23:16:52,916 epoch 2 - iter 18/37 - loss 7.15957469 - samples/sec: 61.41\n",
      "2020-05-21 23:17:06,279 epoch 2 - iter 21/37 - loss 7.15480902 - samples/sec: 62.49\n",
      "2020-05-21 23:17:18,293 epoch 2 - iter 24/37 - loss 7.15972787 - samples/sec: 64.69\n",
      "2020-05-21 23:17:29,839 epoch 2 - iter 27/37 - loss 7.16238942 - samples/sec: 62.95\n",
      "2020-05-21 23:17:41,879 epoch 2 - iter 30/37 - loss 7.16395737 - samples/sec: 63.08\n",
      "2020-05-21 23:17:53,603 epoch 2 - iter 33/37 - loss 7.16557806 - samples/sec: 67.31\n",
      "2020-05-21 23:18:05,290 epoch 2 - iter 36/37 - loss 7.16589177 - samples/sec: 64.92\n",
      "2020-05-21 23:18:15,916 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:18:15,918 EPOCH 2 done: loss 7.1663 - lr 0.1000000\n",
      "2020-05-21 23:18:18,708 DEV : loss 7.0794572830200195 - score 0.9992\n",
      "2020-05-21 23:18:18,849 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-05-21 23:18:20,789 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:18:23,140 epoch 3 - iter 3/37 - loss 6.99586821 - samples/sec: 58.69\n",
      "2020-05-21 23:18:36,140 epoch 3 - iter 6/37 - loss 7.00511718 - samples/sec: 60.27\n",
      "2020-05-21 23:18:48,082 epoch 3 - iter 9/37 - loss 7.02115424 - samples/sec: 58.74\n",
      "2020-05-21 23:18:59,829 epoch 3 - iter 12/37 - loss 7.02222033 - samples/sec: 62.47\n",
      "2020-05-21 23:19:11,896 epoch 3 - iter 15/37 - loss 7.01932936 - samples/sec: 58.80\n",
      "2020-05-21 23:19:23,241 epoch 3 - iter 18/37 - loss 7.02797098 - samples/sec: 67.46\n",
      "2020-05-21 23:19:35,001 epoch 3 - iter 21/37 - loss 7.02846829 - samples/sec: 58.27\n",
      "2020-05-21 23:19:46,820 epoch 3 - iter 24/37 - loss 7.02712409 - samples/sec: 63.64\n",
      "2020-05-21 23:19:58,436 epoch 3 - iter 27/37 - loss 7.02979406 - samples/sec: 68.75\n",
      "2020-05-21 23:20:10,545 epoch 3 - iter 30/37 - loss 7.03669982 - samples/sec: 64.43\n",
      "2020-05-21 23:20:22,340 epoch 3 - iter 33/37 - loss 7.03652873 - samples/sec: 61.67\n",
      "2020-05-21 23:20:34,090 epoch 3 - iter 36/37 - loss 7.04075985 - samples/sec: 66.50\n",
      "2020-05-21 23:20:45,794 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:20:45,799 EPOCH 3 done: loss 7.0408 - lr 0.1000000\n",
      "2020-05-21 23:20:48,527 DEV : loss 7.157044887542725 - score 0.9992\n",
      "2020-05-21 23:20:48,877 BAD EPOCHS (no improvement): 1\n",
      "2020-05-21 23:20:48,881 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:20:50,855 epoch 4 - iter 3/37 - loss 6.91360569 - samples/sec: 57.04\n",
      "2020-05-21 23:21:02,838 epoch 4 - iter 6/37 - loss 6.89701454 - samples/sec: 61.38\n",
      "2020-05-21 23:21:14,860 epoch 4 - iter 9/37 - loss 6.90034850 - samples/sec: 61.28\n",
      "2020-05-21 23:21:26,762 epoch 4 - iter 12/37 - loss 6.90463646 - samples/sec: 56.34\n",
      "2020-05-21 23:21:38,319 epoch 4 - iter 15/37 - loss 6.90518341 - samples/sec: 62.30\n",
      "2020-05-21 23:21:50,097 epoch 4 - iter 18/37 - loss 6.92092419 - samples/sec: 65.43\n",
      "2020-05-21 23:22:02,099 epoch 4 - iter 21/37 - loss 6.91690506 - samples/sec: 60.63\n",
      "2020-05-21 23:22:15,328 epoch 4 - iter 24/37 - loss 6.92014613 - samples/sec: 63.82\n",
      "2020-05-21 23:22:27,425 epoch 4 - iter 27/37 - loss 6.91035936 - samples/sec: 65.31\n",
      "2020-05-21 23:22:39,132 epoch 4 - iter 30/37 - loss 6.90454345 - samples/sec: 63.71\n",
      "2020-05-21 23:22:50,713 epoch 4 - iter 33/37 - loss 6.90531869 - samples/sec: 64.85\n",
      "2020-05-21 23:23:02,285 epoch 4 - iter 36/37 - loss 6.90097307 - samples/sec: 64.07\n",
      "2020-05-21 23:23:12,996 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:23:13,000 EPOCH 4 done: loss 6.9005 - lr 0.1000000\n",
      "2020-05-21 23:23:16,096 DEV : loss 7.181530952453613 - score 0.9992\n",
      "2020-05-21 23:23:16,228 BAD EPOCHS (no improvement): 2\n",
      "2020-05-21 23:23:16,234 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:23:18,223 epoch 5 - iter 3/37 - loss 6.69370270 - samples/sec: 61.42\n",
      "2020-05-21 23:23:29,965 epoch 5 - iter 6/37 - loss 6.67670266 - samples/sec: 59.38\n",
      "2020-05-21 23:23:41,501 epoch 5 - iter 9/37 - loss 6.69195795 - samples/sec: 64.19\n",
      "2020-05-21 23:23:53,055 epoch 5 - iter 12/37 - loss 6.70653959 - samples/sec: 60.49\n",
      "2020-05-21 23:24:04,850 epoch 5 - iter 15/37 - loss 6.70460199 - samples/sec: 61.04\n",
      "2020-05-21 23:24:16,550 epoch 5 - iter 18/37 - loss 6.71767966 - samples/sec: 60.94\n",
      "2020-05-21 23:24:28,454 epoch 5 - iter 21/37 - loss 6.72249933 - samples/sec: 61.10\n",
      "2020-05-21 23:24:40,383 epoch 5 - iter 24/37 - loss 6.71670888 - samples/sec: 55.54\n",
      "2020-05-21 23:24:51,934 epoch 5 - iter 27/37 - loss 6.72217510 - samples/sec: 62.72\n",
      "2020-05-21 23:25:03,473 epoch 5 - iter 30/37 - loss 6.72259836 - samples/sec: 63.04\n",
      "2020-05-21 23:25:15,129 epoch 5 - iter 33/37 - loss 6.72243922 - samples/sec: 67.79\n",
      "2020-05-21 23:25:26,594 epoch 5 - iter 36/37 - loss 6.72408681 - samples/sec: 66.02\n",
      "2020-05-21 23:25:37,198 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:25:37,202 EPOCH 5 done: loss 6.7299 - lr 0.1000000\n",
      "2020-05-21 23:25:41,112 DEV : loss 7.160878658294678 - score 0.9992\n",
      "2020-05-21 23:25:41,267 BAD EPOCHS (no improvement): 3\n",
      "2020-05-21 23:25:41,272 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:25:43,194 epoch 6 - iter 3/37 - loss 6.53624932 - samples/sec: 62.91\n",
      "2020-05-21 23:25:55,000 epoch 6 - iter 6/37 - loss 6.55600365 - samples/sec: 55.11\n",
      "2020-05-21 23:26:06,677 epoch 6 - iter 9/37 - loss 6.49524487 - samples/sec: 61.41\n",
      "2020-05-21 23:26:18,287 epoch 6 - iter 12/37 - loss 6.51831718 - samples/sec: 64.97\n",
      "2020-05-21 23:26:29,866 epoch 6 - iter 15/37 - loss 6.51797174 - samples/sec: 61.32\n",
      "2020-05-21 23:26:41,482 epoch 6 - iter 18/37 - loss 6.51199839 - samples/sec: 64.28\n",
      "2020-05-21 23:26:53,308 epoch 6 - iter 21/37 - loss 6.51387825 - samples/sec: 59.46\n",
      "2020-05-21 23:27:04,962 epoch 6 - iter 24/37 - loss 6.51566366 - samples/sec: 59.31\n",
      "2020-05-21 23:27:16,593 epoch 6 - iter 27/37 - loss 6.51461983 - samples/sec: 67.90\n",
      "2020-05-21 23:27:30,628 epoch 6 - iter 30/37 - loss 6.52251325 - samples/sec: 63.34\n",
      "2020-05-21 23:27:42,753 epoch 6 - iter 33/37 - loss 6.53293189 - samples/sec: 55.95\n",
      "2020-05-21 23:27:54,468 epoch 6 - iter 36/37 - loss 6.53668843 - samples/sec: 63.71\n",
      "2020-05-21 23:28:05,015 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:28:05,021 EPOCH 6 done: loss 6.5344 - lr 0.1000000\n",
      "2020-05-21 23:28:07,819 DEV : loss 7.201419353485107 - score 0.9992\n",
      "Epoch     6: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-05-21 23:28:07,951 BAD EPOCHS (no improvement): 4\n",
      "2020-05-21 23:28:07,955 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:28:09,891 epoch 7 - iter 3/37 - loss 6.17453067 - samples/sec: 58.86\n",
      "2020-05-21 23:28:21,811 epoch 7 - iter 6/37 - loss 6.18373164 - samples/sec: 66.11\n",
      "2020-05-21 23:28:33,720 epoch 7 - iter 9/37 - loss 6.18343883 - samples/sec: 60.38\n",
      "2020-05-21 23:28:45,256 epoch 7 - iter 12/37 - loss 6.19466257 - samples/sec: 67.25\n",
      "2020-05-21 23:28:56,989 epoch 7 - iter 15/37 - loss 6.21465019 - samples/sec: 60.61\n",
      "2020-05-21 23:29:08,856 epoch 7 - iter 18/37 - loss 6.22720363 - samples/sec: 54.24\n",
      "2020-05-21 23:29:20,342 epoch 7 - iter 21/37 - loss 6.22328236 - samples/sec: 66.07\n",
      "2020-05-21 23:29:32,030 epoch 7 - iter 24/37 - loss 6.22349755 - samples/sec: 60.82\n",
      "2020-05-21 23:29:43,285 epoch 7 - iter 27/37 - loss 6.23554387 - samples/sec: 67.68\n",
      "2020-05-21 23:29:54,556 epoch 7 - iter 30/37 - loss 6.24380064 - samples/sec: 65.08\n",
      "2020-05-21 23:30:06,365 epoch 7 - iter 33/37 - loss 6.25439979 - samples/sec: 64.47\n",
      "2020-05-21 23:30:17,831 epoch 7 - iter 36/37 - loss 6.24988528 - samples/sec: 61.22\n",
      "2020-05-21 23:30:28,364 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:30:28,366 EPOCH 7 done: loss 6.2574 - lr 0.0500000\n",
      "2020-05-21 23:30:31,083 DEV : loss 7.249301910400391 - score 0.9992\n",
      "2020-05-21 23:30:31,211 BAD EPOCHS (no improvement): 1\n",
      "2020-05-21 23:30:31,216 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:30:33,309 epoch 8 - iter 3/37 - loss 6.13572947 - samples/sec: 67.12\n",
      "2020-05-21 23:30:44,919 epoch 8 - iter 6/37 - loss 6.08821424 - samples/sec: 59.96\n",
      "2020-05-21 23:30:56,973 epoch 8 - iter 9/37 - loss 6.08577092 - samples/sec: 62.31\n",
      "2020-05-21 23:31:08,515 epoch 8 - iter 12/37 - loss 6.07393634 - samples/sec: 58.77\n",
      "2020-05-21 23:31:20,407 epoch 8 - iter 15/37 - loss 6.07820772 - samples/sec: 61.16\n",
      "2020-05-21 23:31:32,084 epoch 8 - iter 18/37 - loss 6.08069820 - samples/sec: 60.74\n",
      "2020-05-21 23:31:43,896 epoch 8 - iter 21/37 - loss 6.08174683 - samples/sec: 57.22\n",
      "2020-05-21 23:31:56,435 epoch 8 - iter 24/37 - loss 6.09867827 - samples/sec: 62.61\n",
      "2020-05-21 23:32:09,903 epoch 8 - iter 27/37 - loss 6.09618455 - samples/sec: 62.73\n",
      "2020-05-21 23:32:21,644 epoch 8 - iter 30/37 - loss 6.09448209 - samples/sec: 67.37\n",
      "2020-05-21 23:32:34,270 epoch 8 - iter 33/37 - loss 6.09390448 - samples/sec: 63.83\n",
      "2020-05-21 23:32:47,159 epoch 8 - iter 36/37 - loss 6.09654939 - samples/sec: 65.59\n",
      "2020-05-21 23:32:57,709 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:32:57,711 EPOCH 8 done: loss 6.1022 - lr 0.0500000\n",
      "2020-05-21 23:33:00,743 DEV : loss 7.245715618133545 - score 0.9992\n",
      "2020-05-21 23:33:00,878 BAD EPOCHS (no improvement): 2\n",
      "2020-05-21 23:33:00,883 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:33:02,895 epoch 9 - iter 3/37 - loss 5.88956292 - samples/sec: 59.39\n",
      "2020-05-21 23:33:14,660 epoch 9 - iter 6/37 - loss 5.88908251 - samples/sec: 62.89\n",
      "2020-05-21 23:33:26,628 epoch 9 - iter 9/37 - loss 5.90335411 - samples/sec: 62.32\n",
      "2020-05-21 23:33:38,862 epoch 9 - iter 12/37 - loss 5.90608482 - samples/sec: 52.96\n",
      "2020-05-21 23:33:51,654 epoch 9 - iter 15/37 - loss 5.90775121 - samples/sec: 61.00\n",
      "2020-05-21 23:34:03,429 epoch 9 - iter 18/37 - loss 5.91565786 - samples/sec: 63.00\n",
      "2020-05-21 23:34:15,103 epoch 9 - iter 21/37 - loss 5.91305122 - samples/sec: 58.33\n",
      "2020-05-21 23:34:27,393 epoch 9 - iter 24/37 - loss 5.91936678 - samples/sec: 59.98\n",
      "2020-05-21 23:34:39,126 epoch 9 - iter 27/37 - loss 5.92742134 - samples/sec: 62.98\n",
      "2020-05-21 23:34:51,396 epoch 9 - iter 30/37 - loss 5.93548134 - samples/sec: 62.84\n",
      "2020-05-21 23:35:03,036 epoch 9 - iter 33/37 - loss 5.94098415 - samples/sec: 66.77\n",
      "2020-05-21 23:35:14,683 epoch 9 - iter 36/37 - loss 5.94645780 - samples/sec: 62.64\n",
      "2020-05-21 23:35:25,361 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:35:25,362 EPOCH 9 done: loss 5.9461 - lr 0.0500000\n",
      "2020-05-21 23:35:28,424 DEV : loss 7.2918877601623535 - score 0.9992\n",
      "2020-05-21 23:35:28,568 BAD EPOCHS (no improvement): 3\n",
      "2020-05-21 23:35:28,573 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:35:30,601 epoch 10 - iter 3/37 - loss 5.75315650 - samples/sec: 57.41\n",
      "2020-05-21 23:35:42,254 epoch 10 - iter 6/37 - loss 5.75142694 - samples/sec: 63.71\n",
      "2020-05-21 23:35:54,987 epoch 10 - iter 9/37 - loss 5.77735800 - samples/sec: 60.08\n",
      "2020-05-21 23:36:06,679 epoch 10 - iter 12/37 - loss 5.77123344 - samples/sec: 54.43\n",
      "2020-05-21 23:36:17,881 epoch 10 - iter 15/37 - loss 5.78044910 - samples/sec: 65.31\n",
      "2020-05-21 23:36:29,637 epoch 10 - iter 18/37 - loss 5.79316476 - samples/sec: 59.00\n",
      "2020-05-21 23:36:41,318 epoch 10 - iter 21/37 - loss 5.79286805 - samples/sec: 61.88\n",
      "2020-05-21 23:36:53,085 epoch 10 - iter 24/37 - loss 5.80839411 - samples/sec: 65.34\n",
      "2020-05-21 23:37:04,644 epoch 10 - iter 27/37 - loss 5.80546538 - samples/sec: 62.76\n",
      "2020-05-21 23:37:16,328 epoch 10 - iter 30/37 - loss 5.81066844 - samples/sec: 61.89\n",
      "2020-05-21 23:37:28,778 epoch 10 - iter 33/37 - loss 5.80741666 - samples/sec: 62.92\n",
      "2020-05-21 23:37:41,029 epoch 10 - iter 36/37 - loss 5.81114337 - samples/sec: 62.91\n",
      "2020-05-21 23:37:53,597 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:37:53,602 EPOCH 10 done: loss 5.8147 - lr 0.0500000\n",
      "2020-05-21 23:37:56,468 DEV : loss 7.355268955230713 - score 0.9992\n",
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2020-05-21 23:37:56,606 BAD EPOCHS (no improvement): 4\n",
      "2020-05-21 23:37:58,582 ----------------------------------------------------------------------------------------------------\n",
      "2020-05-21 23:37:58,587 Testing using best model ...\n",
      "2020-05-21 23:37:58,593 loading file /content/drive/My Drive/Capstone/data/best-model.pt\n",
      "2020-05-21 23:38:03,718 0.0\t0.0\t0.0\n",
      "2020-05-21 23:38:03,722 \n",
      "MICRO_AVG: acc 0.9984790874524715 - f1-score 0.0\n",
      "MACRO_AVG: acc 0.9984790874524718 - f1-score 0.0\n",
      "0          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "100        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1000       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1001       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1002       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1003       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1004       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1005       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1006       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1007       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1008       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1009       tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "101        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1010       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1011       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1012       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1013       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1014       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1015       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1016       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1017       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1018       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1019       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "102        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1020       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1021       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1022       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1023       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1024       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1025       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1026       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1027       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1028       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1029       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "103        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1030       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1031       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1032       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1033       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1034       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1035       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1036       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1037       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1038       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1039       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "104        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1041       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1042       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1043       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1044       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1045       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1046       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1047       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1049       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "105        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1050       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1051       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1052       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1053       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1054       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1055       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1056       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1057       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1059       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "106        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1060       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1061       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1062       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1063       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1064       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1066       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1067       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1069       tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "107        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1070       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1071       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1072       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1074       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1075       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1076       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1077       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1078       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1079       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "108        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1080       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1081       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1083       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1084       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1085       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1086       tp: 0 - fp: 2 - fn: 0 - tn: 145 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9864 - f1-score: 0.0000\n",
      "1087       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1088       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1089       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "109        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1091       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1092       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1093       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1094       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1095       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1096       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1097       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1098       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1099       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "11         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "110        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1100       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1102       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1103       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1104       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1105       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1106       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1107       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1108       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1109       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1110       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1111       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1112       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1113       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1114       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1115       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1117       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1118       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "112        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1120       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1121       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1122       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1123       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1124       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1125       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1126       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1127       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1128       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1129       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "113        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1130       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1131       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1132       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1133       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1134       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1135       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1136       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1137       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1138       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1139       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "114        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1140       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1141       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1142       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1143       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1145       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1146       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1147       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1149       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "115        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1150       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1151       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1152       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1154       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1155       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1156       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1157       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1159       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "116        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1160       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1161       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1162       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1163       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1165       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1166       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1167       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1168       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1169       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "117        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1170       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1171       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1172       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1173       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1174       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1175       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1176       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1178       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1179       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "118        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1180       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1181       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1182       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1183       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1184       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1185       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1186       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1187       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1188       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1189       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1190       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1191       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1194       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1195       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1196       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1198       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1199       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "12         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "120        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1200       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1201       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1202       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1203       tp: 0 - fp: 8 - fn: 0 - tn: 139 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9456 - f1-score: 0.0000\n",
      "1204       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1205       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1206       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1207       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1208       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1209       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "121        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1210       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1211       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1212       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1213       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1214       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1215       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1216       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1217       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1218       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1219       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "122        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1220       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1221       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1222       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1223       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1224       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1225       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1226       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1227       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1228       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1229       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "123        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1230       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1231       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1232       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1233       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1234       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1235       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1236       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1237       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1238       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1239       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1240       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1241       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1242       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1244       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1245       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1246       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1248       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1249       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "125        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1250       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1251       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1252       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1253       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1254       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1255       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1256       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1257       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1258       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1259       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "126        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1260       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1261       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1262       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1263       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1264       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1265       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1266       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1267       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1268       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1269       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "127        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1270       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1272       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1273       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1274       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1275       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1277       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1278       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1279       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "128        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1280       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1281       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1282       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1283       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1284       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1285       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1286       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1287       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1288       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "129        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1291       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1293       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1294       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1296       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1297       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1298       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1299       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "13         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "130        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1300       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1301       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1302       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1303       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1304       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1305       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1306       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1307       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1308       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1309       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "131        tp: 0 - fp: 7 - fn: 0 - tn: 140 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9524 - f1-score: 0.0000\n",
      "1310       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1311       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1312       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1313       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1314       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1315       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1316       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1317       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1319       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "132        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1320       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1321       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1322       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1323       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1324       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1325       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1326       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1327       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1328       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1329       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "133        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1330       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1331       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1332       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1334       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1335       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1336       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1337       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1338       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "134        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1340       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1341       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1342       tp: 0 - fp: 2 - fn: 0 - tn: 145 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9864 - f1-score: 0.0000\n",
      "1343       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1344       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1345       tp: 0 - fp: 3 - fn: 0 - tn: 144 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9796 - f1-score: 0.0000\n",
      "1346       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1347       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1348       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1349       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "135        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1351       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1352       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1353       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1354       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1355       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1356       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1357       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1358       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1359       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "136        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1360       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1361       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1362       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1363       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1365       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1366       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1368       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1369       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1370       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1371       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1372       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1373       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1374       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1376       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1377       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1378       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1379       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "138        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1380       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1381       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1382       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1383       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1384       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1385       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1387       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1388       tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1389       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "139        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1390       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1391       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1392       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1393       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1394       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1395       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1396       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1397       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1398       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1399       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "14         tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "140        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1400       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1401       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1402       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1403       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1404       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1405       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1406       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1407       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1408       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1409       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "141        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1410       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1411       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1412       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1413       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1414       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1415       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1416       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1417       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1418       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "142        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1420       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1423       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1424       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1425       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1426       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1429       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "143        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1430       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1431       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1432       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1433       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1434       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1435       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1437       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1438       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1439       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "144        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1440       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1441       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1442       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1443       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1444       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1445       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1446       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1447       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1448       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1449       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "145        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1451       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1452       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1453       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1454       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1456       tp: 0 - fp: 19 - fn: 0 - tn: 128 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.8707 - f1-score: 0.0000\n",
      "1457       tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "1459       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "146        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "1460       tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "147        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "148        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "149        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "15         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "150        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "151        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "153        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "154        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "155        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "156        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "157        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "158        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "159        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "16         tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "160        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "161        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "162        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "163        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "164        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "165        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "167        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "168        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "169        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "17         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "170        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "171        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "172        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "173        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "174        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "175        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "176        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "177        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "178        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "179        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "18         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "180        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "181        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "182        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "183        tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "184        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "185        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "186        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "188        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "189        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "19         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "190        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "192        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "193        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "194        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "195        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "196        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "197        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "198        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "199        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "2          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "20         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "200        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "201        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "202        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "203        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "204        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "205        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "206        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "207        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "208        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "209        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "21         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "210        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "212        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "213        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "214        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "215        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "216        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "217        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "218        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "219        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "22         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "220        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "221        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "222        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "223        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "224        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "225        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "226        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "228        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "229        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "23         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "230        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "231        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "232        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "233        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "235        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "236        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "237        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "238        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "24         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "240        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "242        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "243        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "244        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "245        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "247        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "249        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "25         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "250        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "251        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "252        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "253        tp: 0 - fp: 7 - fn: 0 - tn: 140 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9524 - f1-score: 0.0000\n",
      "254        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "255        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "256        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "257        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "259        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "26         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "261        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "262        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "263        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "264        tp: 0 - fp: 9 - fn: 0 - tn: 138 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9388 - f1-score: 0.0000\n",
      "265        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "266        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "267        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "268        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "269        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "27         tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "270        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "271        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "272        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "273        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "274        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "275        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "276        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "277        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "278        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "279        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "28         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "280        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "281        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "282        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "283        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "285        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "286        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "287        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "288        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "289        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "29         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "290        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "291        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "292        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "293        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "294        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "295        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "296        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "297        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "298        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "299        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "3          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "30         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "300        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "301        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "302        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "303        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "304        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "305        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "306        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "307        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "308        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "309        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "31         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "311        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "312        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "313        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "314        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "315        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "318        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "319        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "32         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "320        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "321        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "322        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "323        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "324        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "325        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "326        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "327        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "328        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "33         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "330        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "332        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "333        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "334        tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "335        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "336        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "337        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "339        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "34         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "340        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "341        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "342        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "343        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "344        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "345        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "346        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "347        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "349        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "35         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "350        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "351        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "352        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "353        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "354        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "356        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "357        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "358        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "359        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "36         tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "360        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "361        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "362        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "363        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "364        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "365        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "367        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "368        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "369        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "37         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "370        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "371        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "372        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "373        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "374        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "375        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "377        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "378        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "379        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "38         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "380        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "381        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "382        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "383        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "385        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "386        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "387        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "388        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "389        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "39         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "390        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "391        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "392        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "393        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "394        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "396        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "397        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "399        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "4          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "40         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "400        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "401        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "402        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "403        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "404        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "405        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "406        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "407        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "408        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "41         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "410        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "411        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "412        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "413        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "414        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "415        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "416        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "417        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "418        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "42         tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "422        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "423        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "424        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "425        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "426        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "427        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "428        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "429        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "43         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "430        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "431        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "433        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "436        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "437        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "438        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "439        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "440        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "441        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "442        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "444        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "445        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "446        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "447        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "448        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "449        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "45         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "451        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "452        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "453        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "454        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "455        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "456        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "457        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "458        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "459        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "46         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "460        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "461        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "462        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "463        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "464        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "465        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "466        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "467        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "469        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "47         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "471        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "472        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "473        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "474        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "475        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "476        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "477        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "478        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "479        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "48         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "480        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "481        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "482        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "483        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "484        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "485        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "487        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "488        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "489        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "49         tp: 0 - fp: 5 - fn: 0 - tn: 142 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9660 - f1-score: 0.0000\n",
      "490        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "491        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "492        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "493        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "494        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "496        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "497        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "498        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "499        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "5          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "50         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "500        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "501        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "502        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "503        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "504        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "505        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "506        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "507        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "508        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "509        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "51         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "510        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "511        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "512        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "513        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "514        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "515        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "516        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "517        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "518        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "519        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "52         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "520        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "521        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "522        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "523        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "524        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "525        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "526        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "527        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "528        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "530        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "531        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "532        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "533        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "534        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "535        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "537        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "538        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "539        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "54         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "540        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "541        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "542        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "543        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "544        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "545        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "547        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "548        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "549        tp: 0 - fp: 4 - fn: 0 - tn: 143 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9728 - f1-score: 0.0000\n",
      "55         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "550        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "551        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "552        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "553        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "554        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "555        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "556        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "557        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "558        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "559        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "56         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "560        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "561        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "562        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "563        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "564        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "565        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "567        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "568        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "569        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "57         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "570        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "571        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "572        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "573        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "574        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "576        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "577        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "578        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "579        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "58         tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "580        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "581        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "582        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "584        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "585        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "586        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "587        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "588        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "589        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "59         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "590        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "591        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "592        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "593        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "594        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "595        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "596        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "597        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "598        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "599        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "6          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "60         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "600        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "601        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "602        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "603        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "604        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "605        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "606        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "607        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "609        tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "61         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "611        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "612        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "613        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "614        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "615        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "616        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "617        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "618        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "619        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "62         tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "620        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "621        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "622        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "624        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "625        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "626        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "628        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "629        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "63         tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "630        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "631        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "632        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "633        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "634        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "635        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "636        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "637        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "638        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "639        tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "64         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "641        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "642        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "643        tp: 0 - fp: 8 - fn: 0 - tn: 139 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9456 - f1-score: 0.0000\n",
      "644        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "645        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "646        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "647        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "648        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "65         tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "650        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "652        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "653        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "654        tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "655        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "656        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "657        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "658        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "659        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "66         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "660        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "661        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "662        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "663        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "664        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "665        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "666        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "667        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "669        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "67         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "670        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "671        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "672        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "673        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "674        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "675        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "676        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "678        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "679        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "68         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "680        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "681        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "682        tp: 0 - fp: 2 - fn: 0 - tn: 145 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9864 - f1-score: 0.0000\n",
      "683        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "685        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "686        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "687        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "688        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "689        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "69         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "690        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "691        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "692        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "693        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "695        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "696        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "697        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "7          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "70         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "700        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "701        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "702        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "703        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "704        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "705        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "706        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "707        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "708        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "709        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "71         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "710        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "711        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "712        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "713        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "714        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "715        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "716        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "717        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "718        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "719        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "72         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "720        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "721        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "723        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "724        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "725        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "726        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "727        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "728        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "729        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "73         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "730        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "731        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "732        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "733        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "734        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "737        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "739        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "74         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "740        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "741        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "742        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "744        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "745        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "746        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "747        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "748        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "749        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "75         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "750        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "751        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "752        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "753        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "754        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "755        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "756        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "757        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "759        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "76         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "760        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "761        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "762        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "763        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "764        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "765        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "766        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "767        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "768        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "769        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "77         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "770        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "771        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "772        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "774        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "775        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "776        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "777        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "778        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "779        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "78         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "780        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "781        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "782        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "783        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "784        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "785        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "786        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "787        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "788        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "79         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "790        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "792        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "794        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "795        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "796        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "797        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "799        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "8          tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "80         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "800        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "802        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "803        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "804        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "805        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "806        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "807        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "808        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "809        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "81         tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "810        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "811        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "812        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "813        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "814        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "815        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "816        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "817        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "818        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "819        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "82         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "821        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "822        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "823        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "824        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "825        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "826        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "827        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "828        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "829        tp: 0 - fp: 1 - fn: 1 - tn: 145 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9864 - f1-score: 0.0000\n",
      "83         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "830        tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "831        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "832        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "833        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "834        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "835        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "836        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "837        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "838        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "839        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "84         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "840        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "841        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "842        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "843        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "844        tp: 0 - fp: 1 - fn: 0 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "845        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "846        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "848        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "849        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "85         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "850        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "851        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "852        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "855        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "856        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "857        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "858        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "859        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "86         tp: 0 - fp: 2 - fn: 0 - tn: 145 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9864 - f1-score: 0.0000\n",
      "861        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "862        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "863        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "864        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "865        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "866        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "867        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "868        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "869        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "870        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "871        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "872        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "873        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "874        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "875        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "876        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "877        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "878        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "879        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "88         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "880        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "881        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "882        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "883        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "884        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "885        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "886        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "887        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "888        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "89         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "890        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "891        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "892        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "893        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "894        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "895        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "896        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "897        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "898        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "899        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "900        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "901        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "902        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "903        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "904        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "906        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "907        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "908        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "909        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "91         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "911        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "912        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "913        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "914        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "915        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "916        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "917        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "918        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "919        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "92         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "920        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "921        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "922        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "923        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "924        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "925        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "927        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "928        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "93         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "930        tp: 0 - fp: 8 - fn: 0 - tn: 139 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9456 - f1-score: 0.0000\n",
      "931        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "932        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "933        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "934        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "935        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "937        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "938        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "939        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "94         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "940        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "941        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "942        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "943        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "944        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "945        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "946        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "947        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "948        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "95         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "950        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "951        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "953        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "954        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "956        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "957        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "958        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "959        tp: 0 - fp: 48 - fn: 0 - tn: 99 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.6735 - f1-score: 0.0000\n",
      "96         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "960        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "961        tp: 0 - fp: 0 - fn: 1 - tn: 146 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9932 - f1-score: 0.0000\n",
      "963        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "965        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "966        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "968        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "969        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "97         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "970        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "971        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "974        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "975        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "976        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "977        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "978        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "979        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "98         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "980        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "981        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "982        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "983        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "984        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "985        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "987        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "988        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "989        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "99         tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "990        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "991        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "992        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "993        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "994        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "995        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "996        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "997        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "998        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "999        tp: 0 - fp: 0 - fn: 0 - tn: 147 - precision: 0.0000 - recall: 0.0000 - accuracy: 1.0000 - f1-score: 0.0000\n",
      "2020-05-21 23:38:03,733 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [7.103290557861328,\n",
       "  7.0794572830200195,\n",
       "  7.157044887542725,\n",
       "  7.181530952453613,\n",
       "  7.160878658294678,\n",
       "  7.201419353485107,\n",
       "  7.249301910400391,\n",
       "  7.245715618133545,\n",
       "  7.2918877601623535,\n",
       "  7.355268955230713],\n",
       " 'dev_score_history': [0.9992395437262357,\n",
       "  0.9992395437262357,\n",
       "  0.9992395437262357,\n",
       "  0.9992395437262357,\n",
       "  0.9992395437262357,\n",
       "  0.9992395437262357,\n",
       "  0.9992395437262357,\n",
       "  0.9992395437262357,\n",
       "  0.9992395437262357,\n",
       "  0.9992395437262357],\n",
       " 'test_score': 0.9984790874524715,\n",
       " 'train_loss_history': [7.261060572959281,\n",
       "  7.166334848146181,\n",
       "  7.04078568638982,\n",
       "  6.900489781353925,\n",
       "  6.729859287674363,\n",
       "  6.5343695460139095,\n",
       "  6.257350702543516,\n",
       "  6.102229672509271,\n",
       "  5.946094177864693,\n",
       "  5.814655716354783]}"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "# classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "# trainer = ModelTrainer(classifier, corpus)\n",
    "# trainer.train(data_folder, max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kedLK7MXljbc"
   },
   "source": [
    "Second phase training with GDP-specific training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1E2JikhOEZFW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Initial_flair_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
