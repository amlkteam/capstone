{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "from datetime import datetime, timedelta, date\n",
    "import pytz\n",
    "import dateutil.parser\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataframe_by_month(dataframe, sample_size):\n",
    "    \"\"\"\n",
    "    create sample of dataframe based on publish date, sample size is the number of articles to be extracted from each month\n",
    "    \"\"\"\n",
    "    article_dictionary_by_month = defaultdict(list)\n",
    "    full_list = []\n",
    "    for column, row in dataframe.iterrows():\n",
    "        article_date = (dateutil.parser.parse(row['publishedAt']))\n",
    "        article_year = article_date.year\n",
    "        article_month = article_date.month\n",
    "        article_dictionary_by_month[str(article_year) + '-' + str(article_month)].append(row)\n",
    "\n",
    "    for month_number, list_of_articles in article_dictionary_by_month.items():\n",
    "        random.shuffle(list_of_articles)\n",
    "        subset_list = list_of_articles[:sample_size]\n",
    "        full_list.extend(subset_list)\n",
    "\n",
    "    sample_df = pd.DataFrame(full_list)\n",
    "    sample_df = sample_df.sort_values(by='publishedAt', ascending=False)\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fp_bloomberg_then_sample(bloomberg_json, fp_json, keyword, article_num_per_month):\n",
    "    '''\n",
    "    Combine financial post articles and bloomberg articles, and create a sample by publish date.\n",
    "    \n",
    "    input:\n",
    "    \n",
    "    bloomberg_json is the file of articles from bloomberg stored in json format\n",
    "    fp_json is the the file of articles from financial post stored in json format\n",
    "    keyword is the searching query used to collect these articles\n",
    "    \n",
    "    '''\n",
    "    bloomberg_df = pd.read_json(bloomberg_json)\n",
    "    bloomberg_df = bloomberg_df[(bloomberg_df.source != 'The Canadian Press') & (bloomberg_df.source != 'Reuters')]\n",
    "    bloomberg_df = bloomberg_df[bloomberg_df.source.notnull()]\n",
    "    bloomberg_df['publishedAt'] = bloomberg_df['publishedAt'].apply(lambda x: datetime.strptime(x, '%b %d, %Y').strftime('%Y-%m-%d'))\n",
    "    \n",
    "    fp_df = pd.read_json(fp_json)\n",
    "    fp_df['publishedAt'] = fp_df['publishedAt'].apply(lambda x: ' '.join(x.split()[:3]))\n",
    "    fp_df['publishedAt'] = fp_df['publishedAt'].apply(lambda x: datetime.strptime(x, '%B %d, %Y').strftime('%Y-%m-%d'))\n",
    "\n",
    "    concat_df = pd.concat([bloomberg_df, fp_df])\n",
    "    concat_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    concat_df['title_desc_sent_1'] = None\n",
    "    concat_df['sent_1_note'] = None\n",
    "    concat_df['title_desc_sent_2'] = None\n",
    "    concat_df['sent_2_note'] = None\n",
    "    concat_df = concat_df[['source', 'author', 'title', 'description', 'title_desc_sent_1', 'sent_1_note', 'title_desc_sent_2', 'sent_2_note', 'publishedAt', 'url', 'urlToImage', 'content']]\n",
    "    \n",
    "    sample_df = sample_dataframe_by_month(concat_df, article_num_per_month)\n",
    "    \n",
    "    \n",
    "    sample_df.to_csv(keyword + '_sample.csv')\n",
    "    #return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fp_bloomberg(bloomberg_json, fp_json):\n",
    "    '''\n",
    "    Combine financial post articles and bloomberg articles.\n",
    "    \n",
    "    input:\n",
    "    \n",
    "    bloomberg_json is the file of articles from bloomberg stored in json format\n",
    "    fp_json is the the file of articles from financial post stored in json format\n",
    "    \n",
    "    '''\n",
    "    bloomberg_df = pd.read_json(bloomberg_json)\n",
    "    bloomberg_df = bloomberg_df[(bloomberg_df.source != 'The Canadian Press') & (bloomberg_df.source != 'Reuters')]\n",
    "    bloomberg_df = bloomberg_df[bloomberg_df.source.notnull()]\n",
    "    bloomberg_df['publishedAt'] = bloomberg_df['publishedAt'].apply(lambda x: datetime.strptime(x, '%b %d, %Y').strftime('%Y-%m-%d'))\n",
    "    \n",
    "    fp_df = pd.read_json(fp_json)\n",
    "    fp_df['publishedAt'] = fp_df['publishedAt'].apply(lambda x: ' '.join(x.split()[:3]))\n",
    "    fp_df['publishedAt'] = fp_df['publishedAt'].apply(lambda x: datetime.strptime(x, '%B %d, %Y').strftime('%Y-%m-%d'))\n",
    "\n",
    "    concat_df = pd.concat([bloomberg_df, fp_df])\n",
    "    concat_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    concat_df['title_desc_sent_1'] = None\n",
    "    concat_df['sent_1_note'] = None\n",
    "    concat_df['title_desc_sent_2'] = None\n",
    "    concat_df['sent_2_note'] = None\n",
    "    concat_df = concat_df[['source', 'title', 'description', 'publishedAt']]\n",
    "    concat_df = concat_df.sort_values(by='publishedAt', ascending=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #sample_df.to_csv(keyword + '_sample.csv')\n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unannotated_data(bloomberg_json, fp_json, annotated_route, indicator):\n",
    "    '''get unannotated data (remove annotated data from all the articles that are collected)'''\n",
    "    total = combine_fp_bloomberg(bloomberg_json, fp_json)\n",
    "    annotated = pd.read_csv(annotated_route)\n",
    "    \n",
    "    if 'Unnamed: 0' in annotated.columns.values:\n",
    "        drop_list = annotated['Unnamed: 0'].to_list()\n",
    "    else:\n",
    "        drop_list = annotated['Column1'].to_list()\n",
    "    total = total.drop(drop_list)\n",
    "    total['title_desc'] = total['title'] + '. ' + total['description']\n",
    "    total = total[['source', 'title_desc', 'publishedAt']]\n",
    "    total = total.drop_duplicates(subset='title_desc', keep='first' )\n",
    "    #return total\n",
    "    total.to_csv('predictions_dataset_' + indicator + '_' + 'Bloomberg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_ua = get_unannotated_data('mortgage_rates_100_Bloomberg_article.json', 'mortgage_rate_fpbloomberg.json', '../Annotated_datasets_for_model_finetuning/Bloomberg_mortgagerate_annotated_agreed.csv', 'mortgagerates')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fp_bloomberg_then_sample('interest_rates_100_Bloomberg_article.json', 'interest_rate_fpbloomberg.json', 'interest_combined', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unannotated_data('interest_rates_100_Bloomberg_article.json', 'interest_rate_fpbloomberg.json', '../Annotated_datasets_for_model_finetuning/Bloomberg_interestrate_annotated_agreed.csv', 'interestrates')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fp_bloomberg_then_sample('housing_price_100_Bloomberg_article.json', 'housing_fpbloomberg.json', 'housing_combined', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unannotated_data('housing_price_100_Bloomberg_article.json', 'housing_fpbloomberg.json', '../Annotated_datasets_for_model_finetuning/Bloomberg_housing_annotated_agreed.csv', 'housing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fp_bloomberg_then_sample('GDP_100_Bloomberg_article.json', 'GDP_fpbloomberg.json', 'GDP_combined', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unannotated_data('GDP_100_Bloomberg_article.json', 'GDP_fpbloomberg.json', '../Annotated_datasets_for_model_finetuning/Bloomberg_GDP_annotated_agreed.csv', 'GDP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fp_bloomberg_then_sample('employment_95_Bloomberg_article.json', 'employment_fpbloomberg.json', 'employement_combined', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unannotated_data('employment_95_Bloomberg_article.json', 'employment_fpbloomberg.json', '../Annotated_datasets_for_model_finetuning/Bloomberg_employment_annotated_agreed.csv', 'employment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fp_bloomberg_then_sample('stock_market_100_Bloomberg_article.json', 'stock_market_fpbloomberg.json', 'stock_market_combined', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unannotated_data('stock_market_100_Bloomberg_article.json', 'stock_market_fpbloomberg.json', '../Annotated_datasets_for_model_finetuning/Bloomberg_TSX_annotated_agreed.csv', 'stockmarket')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interest_rate_b_df = pd.read_json('interest_rates_100_Bloomberg_article.json')\n",
    "# interest_rate_b_df['publishedAt'] = interest_rate_b_df['publishedAt'].apply(lambda x: datetime.strptime(x, '%b %d, %Y').strftime('%Y-%m-%d'))\n",
    "\n",
    "# interest_rate_fpb_article_df = pd.read_json('interest_rate_fpbloomberg.json')\n",
    "# interest_rate_fpb_article_df['publishedAt'] = interest_rate_fpb_article_df['publishedAt'].apply(lambda x: ' '.join(x.split()[:3]))\n",
    "# interest_rate_fpb_article_df['publishedAt'] = interest_rate_fpb_article_df['publishedAt'].apply(lambda x: datetime.strptime(x, '%B %d, %Y').strftime('%Y-%m-%d'))\n",
    "\n",
    "# housing_b_df = pd.read_json('housing_price_100_Bloomberg_article.json')\n",
    "# housing_b_df['publishedAt'] = housing_b_df['publishedAt'].apply(lambda x: datetime.strptime(x, '%b %d, %Y').date())\n",
    "\n",
    "# housing_fpb_df = pd.read_json('housing_fpbloomberg.json')\n",
    "# housing_fpb_df['publishedAt'] = housing_fpb_df['publishedAt'].apply(lambda x: ' '.join(x.split()[:3]))\n",
    "# housing_fpb_df['publishedAt'] = housing_fpb_df['publishedAt'].apply(lambda x: datetime.strptime(x, '%B %d, %Y').date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interest_concat = pd.concat([interest_rate_b_df, interest_rate_fpb_article_df])\n",
    "# interest_concat.reset_index(drop=True, inplace=True)\n",
    "# interest_concat.to_json('interst_concat.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing_concat = pd.concat([housing_b_df, housing_fpb_df])\n",
    "# housing_concat.reset_index(drop=True, inplace=True)\n",
    "# housing_concat.to_json('housing_concat.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interest_list = extract_subset_in_csv('interst_concat.json', 5)\n",
    "# len(interest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_dictionary_by_month = defaultdict(list)\n",
    "# full_list = []\n",
    "# for column, row in interest_concat.iterrows():\n",
    "#     article_date = (dateutil.parser.parse(row['publishedAt']))\n",
    "#     article_month = article_date.month\n",
    "#     article_dictionary_by_month[article_month].append(row)\n",
    "    \n",
    "# for month_number, list_of_articles in article_dictionary_by_month.items():\n",
    "#     random.shuffle(list_of_articles)\n",
    "#     subset_list = list_of_articles[:5]\n",
    "#     full_list.extend(subset_list)\n",
    "\n",
    "# sample_df = pd.DataFrame(full_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
