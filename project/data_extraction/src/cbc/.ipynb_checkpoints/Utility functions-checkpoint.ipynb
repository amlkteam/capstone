{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creator: Pandramishi Naga Sirisha\n",
    "# Created on: 27-05-2020\n",
    "# Utilities functions to be created into a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "from datetime import datetime, timedelta, date\n",
    "import pytz\n",
    "import dateutil.parser\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_df(project_path,path_to_json):\n",
    "    \"\"\"This function reads a json file and outputs a dataframe\n",
    "    Input:\n",
    "    ------\n",
    "    project_path: path to project\n",
    "    path_to_json: path to json file\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    Dataframe object\n",
    "    \"\"\"\n",
    "    if  check_dir_exists(project_path+path_to_json):\n",
    "        df = pd.read_json(project_path+path_to_json)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"From convert_json_to_df(): Could not convert the json file to dataframe\")\n",
    "        return None\n",
    "\n",
    "# project_path = \"/Users/nagasiri/Desktop/NagaSiri/MDS-CL/Capstone/better_dwelling_capstone/\"\n",
    "# file_path = \"project/data_extraction/data/unannotated_data/cbc/interestrates_CBC_article.json\"\n",
    "# converted_df = convert_json_to_df(project_path, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir_exists(directory_path):\n",
    "    \"\"\"This function check if a given directory exists\n",
    "    Input:\n",
    "    ------\n",
    "    directory_path - string : The path to the directory we want to check\n",
    "    \n",
    "    Output:\n",
    "    --------\n",
    "    Boolean: True if exists, False if does not exist\"\"\"\n",
    "    \n",
    "    if  os.path.exists(directory_path):\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The given path\", directory_path, \"does not exist\")\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(absolute_file_path):\n",
    "    \"\"\"This function check if a given file exists\n",
    "    Input:\n",
    "    ------\n",
    "    absolute_file_path - string : The absolute path to the file we want to check\n",
    "    \n",
    "    Output:\n",
    "    --------\n",
    "    Boolean: True if exists, False if does not exist\"\"\"\n",
    "    \n",
    "    if os.path.exists(absolute_file_path):\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The file:\", absolute_file_path, \"does not exist\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_object,column_name_list,remove_Nans = True):\n",
    "    \"\"\"\n",
    "    This function preprocesses the dataframe\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    df_object - object: The dataframe object to preprocess\n",
    "    column_name_list- list: list of required columns\n",
    "    remove_Nans - Boolean: To remove all rows which contain None or NaN\n",
    "    filter_query - string\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    object - The preprocessed dataframe object\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        subset_columns_df = df_object[column_name_list]\n",
    "    except:\n",
    "        print(\"From preprocess_df(): Check the dataframe object and column names\")\n",
    "        return None\n",
    "        \n",
    "    if remove_Nans:\n",
    "        subset_columns_df = subset_columns_df.dropna()\n",
    "\n",
    "    return  subset_columns_df\n",
    "    \n",
    "# k = preprocess_df(converted_df, ['title', 'description','publishedAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_df_to_csv(df,project_path,file_path,file_name):\n",
    "    \"\"\"Takes a dataframe and writes to a file\"\"\"\n",
    "    if os.path.isdir(project_path+file_path):\n",
    "        df.to_csv(project_path+file_path+file_name, encoding='utf-8', index=False)\n",
    "    else:\n",
    "        print(\"Path does not exist:\", project_path+file_path)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataframe_by_month(dataframe, sample_size):\n",
    "    \"\"\"\n",
    "    create sample of dataframe based on publish date, sample size is the number of articles to be extracted from each month\n",
    "    \"\"\"\n",
    "    article_dictionary_by_month = defaultdict(list)\n",
    "    full_list = []\n",
    "    \n",
    "    if  type(dataframe) is  not pd.DataFrame or not isinstance(sample_size, int):\n",
    "        print(\"not integer\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        for column, row in dataframe.iterrows():\n",
    "            article_date = (dateutil.parser.parse(row['publishedAt']))\n",
    "            article_year = article_date.year\n",
    "            article_month = article_date.month\n",
    "            article_dictionary_by_month[str(article_year) + '-' + str(article_month)].append(row)\n",
    "\n",
    "        for month_number, list_of_articles in article_dictionary_by_month.items():\n",
    "            random.shuffle(list_of_articles)\n",
    "            subset_list = list_of_articles[:sample_size]\n",
    "            full_list.extend(subset_list)\n",
    "\n",
    "        sample_df = pd.DataFrame(full_list)\n",
    "        sample_df = sample_df.sort_values(by='publishedAt', ascending=False)\n",
    "    \n",
    "    except:\n",
    "        print(\"From function sample_dataframe_by_month() : Could not sample\")\n",
    "        return None\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lambda(df, column, lambda_string):\n",
    "    \"\"\"Takes a dataframe, column and applies a lambda function to it\"\"\"\n",
    "    try:\n",
    "        df[column] = df[column].apply(eval(lambda_string))\n",
    "        return df    \n",
    "    except:\n",
    "        print(\"Cannot apply lambda function\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unannotated_data(combined_df, annotated_df, indicator, source, prediction_file_path = \"../../sentiment_analyzer/data/predictions_data/test/\"):\n",
    "    '''get unannotated data (remove annotated data from all the articles that are collected)'''\n",
    "    # Running this line will erase current data\n",
    "    #prediction_file_path = \"../../sentiment_analyzer/data/predictions_data/bloomberg/\" \n",
    "    # try this line for testing\n",
    "    \n",
    "    total = combined_df\n",
    "    drop_list = annotated_df.index.values.tolist()\n",
    "    total = total.drop(drop_list)\n",
    "    total['title_desc'] = total['title'] + '. ' + total['description']\n",
    "    total = total[['source', 'title_desc', 'publishedAt']]\n",
    "    total = total.drop_duplicates(subset='title_desc', keep='first' )\n",
    "    #return total\n",
    "    if check_dir_exists(prediction_file_path):\n",
    "        total.to_csv(prediction_file_path + 'predictions_dataset_' + indicator + \"_\" + source + '.csv')\n",
    "    if file_exists(prediction_file_path + 'predictions_dataset_' + indicator + \"_\" + source + '.csv'):\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Final directory is not created\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From convert_json_to_df(): Following path does not exist -   /non existentnon_existent_file.json\n",
      "From preprocess_df(): Check the dataframe object and column names\n"
     ]
    }
   ],
   "source": [
    "def unit_tests():\n",
    "    project_path = \"/non existent\"\n",
    "    file_path = \"non_existent_file.json\"\n",
    "    converted_df = convert_json_to_df(project_path, file_path)\n",
    "    assert converted_df is None, \"If path is not present return None\"\n",
    "    \n",
    "    project_path = \"/Users/nagasiri/Desktop/NagaSiri/MDS-CL/Capstone/better_dwelling_capstone/\"\n",
    "    file_path = \"project/data_extraction/data/unannotated_data/cbc/interestrates_CBC_article.json\"\n",
    "    converted_df = convert_json_to_df(project_path, file_path)\n",
    "    df = preprocess_df(converted_df, ['xyz'])\n",
    "    assert df is None\n",
    "    k = preprocess_df(converted_df, ['title', 'description','publishedAt'])\n",
    "    assert isinstance(k, pd.DataFrame)\n",
    "    \n",
    "unit_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
